
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII" />
  <title>client-internal.cc</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">// Licensed to the Apache Software Foundation (ASF) under one</a>
<a name="ln2">// or more contributor license agreements.  See the NOTICE file</a>
<a name="ln3">// distributed with this work for additional information</a>
<a name="ln4">// regarding copyright ownership.  The ASF licenses this file</a>
<a name="ln5">// to you under the Apache License, Version 2.0 (the</a>
<a name="ln6">// &quot;License&quot;); you may not use this file except in compliance</a>
<a name="ln7">// with the License.  You may obtain a copy of the License at</a>
<a name="ln8">//</a>
<a name="ln9">//   http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln10">//</a>
<a name="ln11">// Unless required by applicable law or agreed to in writing,</a>
<a name="ln12">// software distributed under the License is distributed on an</a>
<a name="ln13">// &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</a>
<a name="ln14">// KIND, either express or implied.  See the License for the</a>
<a name="ln15">// specific language governing permissions and limitations</a>
<a name="ln16">// under the License.</a>
<a name="ln17">//</a>
<a name="ln18">// The following only applies to changes made to this file as part of YugaByte development.</a>
<a name="ln19">//</a>
<a name="ln20">// Portions Copyright (c) YugaByte, Inc.</a>
<a name="ln21">//</a>
<a name="ln22">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except</a>
<a name="ln23">// in compliance with the License.  You may obtain a copy of the License at</a>
<a name="ln24">//</a>
<a name="ln25">// http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln26">//</a>
<a name="ln27">// Unless required by applicable law or agreed to in writing, software distributed under the License</a>
<a name="ln28">// is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express</a>
<a name="ln29">// or implied.  See the License for the specific language governing permissions and limitations</a>
<a name="ln30">// under the License.</a>
<a name="ln31">//</a>
<a name="ln32"> </a>
<a name="ln33">#include &quot;yb/client/client-internal.h&quot;</a>
<a name="ln34"> </a>
<a name="ln35">#include &lt;algorithm&gt;</a>
<a name="ln36">#include &lt;functional&gt;</a>
<a name="ln37">#include &lt;iostream&gt;</a>
<a name="ln38">#include &lt;limits&gt;</a>
<a name="ln39">#include &lt;mutex&gt;</a>
<a name="ln40">#include &lt;sstream&gt;</a>
<a name="ln41">#include &lt;string&gt;</a>
<a name="ln42">#include &lt;vector&gt;</a>
<a name="ln43">#include &lt;fstream&gt;</a>
<a name="ln44"> </a>
<a name="ln45">#include &lt;boost/algorithm/string/predicate.hpp&gt;</a>
<a name="ln46">#include &lt;boost/preprocessor/seq/for_each.hpp&gt;</a>
<a name="ln47"> </a>
<a name="ln48">#include &quot;yb/client/meta_cache.h&quot;</a>
<a name="ln49">#include &quot;yb/client/table.h&quot;</a>
<a name="ln50"> </a>
<a name="ln51">#include &quot;yb/common/index.h&quot;</a>
<a name="ln52">#include &quot;yb/common/schema.h&quot;</a>
<a name="ln53">#include &quot;yb/common/wire_protocol.h&quot;</a>
<a name="ln54">#include &quot;yb/gutil/map-util.h&quot;</a>
<a name="ln55">#include &quot;yb/gutil/strings/human_readable.h&quot;</a>
<a name="ln56">#include &quot;yb/gutil/strings/join.h&quot;</a>
<a name="ln57">#include &quot;yb/gutil/strings/substitute.h&quot;</a>
<a name="ln58">#include &quot;yb/gutil/strings/split.h&quot;</a>
<a name="ln59">#include &quot;yb/gutil/strings/util.h&quot;</a>
<a name="ln60">#include &quot;yb/gutil/sysinfo.h&quot;</a>
<a name="ln61">#include &quot;yb/master/master_defaults.h&quot;</a>
<a name="ln62">#include &quot;yb/master/master_rpc.h&quot;</a>
<a name="ln63">#include &quot;yb/master/master_util.h&quot;</a>
<a name="ln64">#include &quot;yb/master/master.pb.h&quot;</a>
<a name="ln65">#include &quot;yb/master/master.proxy.h&quot;</a>
<a name="ln66">#include &quot;yb/yql/redis/redisserver/redis_constants.h&quot;</a>
<a name="ln67">#include &quot;yb/rpc/rpc.h&quot;</a>
<a name="ln68">#include &quot;yb/rpc/rpc_controller.h&quot;</a>
<a name="ln69">#include &quot;yb/rpc/messenger.h&quot;</a>
<a name="ln70">#include &quot;yb/tserver/tserver_flags.h&quot;</a>
<a name="ln71">#include &quot;yb/util/net/dns_resolver.h&quot;</a>
<a name="ln72">#include &quot;yb/util/curl_util.h&quot;</a>
<a name="ln73">#include &quot;yb/util/flags.h&quot;</a>
<a name="ln74">#include &quot;yb/util/flag_tags.h&quot;</a>
<a name="ln75">#include &quot;yb/util/net/net_util.h&quot;</a>
<a name="ln76">#include &quot;yb/util/scope_exit.h&quot;</a>
<a name="ln77">#include &quot;yb/util/thread_restrictions.h&quot;</a>
<a name="ln78"> </a>
<a name="ln79">using namespace std::literals;</a>
<a name="ln80"> </a>
<a name="ln81">DEFINE_test_flag(bool, assert_local_tablet_server_selected, false, &quot;Verify that SelectTServer &quot;</a>
<a name="ln82">                 &quot;selected the local tablet server. Also verify that ReplicaSelection is equal &quot;</a>
<a name="ln83">                 &quot;to CLOSEST_REPLICA&quot;);</a>
<a name="ln84"> </a>
<a name="ln85">DEFINE_test_flag(string, assert_tablet_server_select_is_in_zone, &quot;&quot;,</a>
<a name="ln86">                 &quot;Verify that SelectTServer selected a talet server in the AZ specified by this &quot;</a>
<a name="ln87">                 &quot;flag.&quot;);</a>
<a name="ln88"> </a>
<a name="ln89">DECLARE_string(flagfile);</a>
<a name="ln90"> </a>
<a name="ln91">namespace yb {</a>
<a name="ln92"> </a>
<a name="ln93">using std::set;</a>
<a name="ln94">using std::shared_ptr;</a>
<a name="ln95">using std::string;</a>
<a name="ln96">using std::vector;</a>
<a name="ln97">using strings::Substitute;</a>
<a name="ln98"> </a>
<a name="ln99">using namespace std::placeholders;</a>
<a name="ln100"> </a>
<a name="ln101">using consensus::RaftPeerPB;</a>
<a name="ln102">using master::GetLeaderMasterRpc;</a>
<a name="ln103">using master::MasterServiceProxy;</a>
<a name="ln104">using master::MasterErrorPB;</a>
<a name="ln105">using rpc::Rpc;</a>
<a name="ln106">using rpc::RpcController;</a>
<a name="ln107"> </a>
<a name="ln108">namespace client {</a>
<a name="ln109"> </a>
<a name="ln110">using internal::GetTableSchemaRpc;</a>
<a name="ln111">using internal::RemoteTablet;</a>
<a name="ln112">using internal::RemoteTabletServer;</a>
<a name="ln113">using internal::UpdateLocalTsState;</a>
<a name="ln114"> </a>
<a name="ln115">Status RetryFunc(</a>
<a name="ln116">    CoarseTimePoint deadline, const string&amp; retry_msg, const string&amp; timeout_msg,</a>
<a name="ln117">    const std::function&lt;Status(CoarseTimePoint, bool*)&gt;&amp; func) {</a>
<a name="ln118">  DCHECK(deadline != CoarseTimePoint());</a>
<a name="ln119"> </a>
<a name="ln120">  if (deadline &lt; CoarseMonoClock::Now()) {</a>
<a name="ln121">    return STATUS(TimedOut, timeout_msg);</a>
<a name="ln122">  }</a>
<a name="ln123"> </a>
<a name="ln124">  MonoDelta wait_time = 1ms;</a>
<a name="ln125">  MonoDelta kMaxSleep = 2s;</a>
<a name="ln126">  for (;;) {</a>
<a name="ln127">    bool retry = true;</a>
<a name="ln128">    Status s = func(deadline, &amp;retry);</a>
<a name="ln129">    if (!retry) {</a>
<a name="ln130">      return s;</a>
<a name="ln131">    }</a>
<a name="ln132"> </a>
<a name="ln133">    VLOG(1) &lt;&lt; retry_msg &lt;&lt; &quot; status=&quot; &lt;&lt; s.ToString();</a>
<a name="ln134">    wait_time = std::min(wait_time * 5 / 4, kMaxSleep);</a>
<a name="ln135"> </a>
<a name="ln136">    // We assume that the function will take the same amount of time to run</a>
<a name="ln137">    // as it did in the previous attempt. If we don't have enough time left</a>
<a name="ln138">    // to sleep and run it again, we don't bother sleeping and retrying.</a>
<a name="ln139">    if (CoarseMonoClock::Now() + wait_time &gt; deadline) {</a>
<a name="ln140">      break;</a>
<a name="ln141">    }</a>
<a name="ln142"> </a>
<a name="ln143">    VLOG(1) &lt;&lt; &quot;Waiting for &quot; &lt;&lt; wait_time &lt;&lt; &quot; before retrying...&quot;;</a>
<a name="ln144">    SleepFor(wait_time);</a>
<a name="ln145">  }</a>
<a name="ln146"> </a>
<a name="ln147">  return STATUS(TimedOut, timeout_msg);</a>
<a name="ln148">}</a>
<a name="ln149"> </a>
<a name="ln150">template &lt;class ReqClass, class RespClass&gt;</a>
<a name="ln151">Status YBClient::Data::SyncLeaderMasterRpc(</a>
<a name="ln152">    CoarseTimePoint deadline, const ReqClass&amp; req, RespClass* resp,</a>
<a name="ln153">    int* num_attempts, const char* func_name,</a>
<a name="ln154">    const std::function&lt;Status(MasterServiceProxy*, const ReqClass&amp;, RespClass*, RpcController*)&gt;&amp;</a>
<a name="ln155">        func) {</a>
<a name="ln156">  running_sync_requests_.fetch_add(1, std::memory_order_acquire);</a>
<a name="ln157">  auto se = ScopeExit([this] {</a>
<a name="ln158">    running_sync_requests_.fetch_sub(1, std::memory_order_acquire);</a>
<a name="ln159">  });</a>
<a name="ln160"> </a>
<a name="ln161">  DSCHECK(deadline != CoarseTimePoint(), InvalidArgument, &quot;Deadline is not set&quot;);</a>
<a name="ln162">  CoarseTimePoint start_time;</a>
<a name="ln163"> </a>
<a name="ln164">  while (true) {</a>
<a name="ln165">    if (closing_.load(std::memory_order_acquire)) {</a>
<a name="ln166">      return STATUS(Aborted, &quot;Client is shutting down&quot;);</a>
<a name="ln167">    }</a>
<a name="ln168"> </a>
<a name="ln169">    RpcController rpc;</a>
<a name="ln170"> </a>
<a name="ln171">    // Have we already exceeded our deadline?</a>
<a name="ln172">    auto now = CoarseMonoClock::Now();</a>
<a name="ln173">    if (start_time == CoarseTimePoint()) {</a>
<a name="ln174">      start_time = now;</a>
<a name="ln175">    }</a>
<a name="ln176">    if (deadline &lt; now) {</a>
<a name="ln177">      return STATUS_FORMAT(TimedOut,</a>
<a name="ln178">          &quot;$0 timed out after deadline expired. Time elapsed: $1, allowed: $2&quot;,</a>
<a name="ln179">          func_name, now - start_time, deadline - start_time);</a>
<a name="ln180">    }</a>
<a name="ln181"> </a>
<a name="ln182">    // The RPC's deadline is intentionally earlier than the overall</a>
<a name="ln183">    // deadline so that we reserve some time with which to find a new</a>
<a name="ln184">    // leader master and retry before the overall deadline expires.</a>
<a name="ln185">    //</a>
<a name="ln186">    // TODO: KUDU-683 tracks cleanup for this.</a>
<a name="ln187">    auto rpc_deadline = now + default_rpc_timeout_;</a>
<a name="ln188">    rpc.set_deadline(std::min(rpc_deadline, deadline));</a>
<a name="ln189"> </a>
<a name="ln190">    if (num_attempts != nullptr) {</a>
<a name="ln191">      ++*num_attempts;</a>
<a name="ln192">    }</a>
<a name="ln193"> </a>
<a name="ln194">    std::shared_ptr&lt;MasterServiceProxy&gt; master_proxy;</a>
<a name="ln195">    {</a>
<a name="ln196">      std::lock_guard&lt;simple_spinlock&gt; l(leader_master_lock_);</a>
<a name="ln197">      master_proxy = master_proxy_;</a>
<a name="ln198">    }</a>
<a name="ln199">    Status s = func(master_proxy.get(), req, resp, &amp;rpc);</a>
<a name="ln200">    if (s.IsNetworkError() || s.IsServiceUnavailable()) {</a>
<a name="ln201">      YB_LOG_EVERY_N_SECS(WARNING, 1)</a>
<a name="ln202">          &lt;&lt; &quot;Unable to send the request &quot; &lt;&lt; req.GetTypeName() &lt;&lt; &quot; (&quot; &lt;&lt; req.ShortDebugString()</a>
<a name="ln203">          &lt;&lt; &quot;) to leader Master (&quot; &lt;&lt; leader_master_hostport().ToString()</a>
<a name="ln204">          &lt;&lt; &quot;): &quot; &lt;&lt; s;</a>
<a name="ln205">      if (IsMultiMaster()) {</a>
<a name="ln206">        YB_LOG_EVERY_N_SECS(INFO, 1) &lt;&lt; &quot;Determining the new leader Master and retrying...&quot;;</a>
<a name="ln207">        WARN_NOT_OK(SetMasterServerProxy(deadline),</a>
<a name="ln208">                    &quot;Unable to determine the new leader Master&quot;);</a>
<a name="ln209">      }</a>
<a name="ln210">      continue;</a>
<a name="ln211">    }</a>
<a name="ln212"> </a>
<a name="ln213">    if (s.IsTimedOut()) {</a>
<a name="ln214">      now = CoarseMonoClock::Now();</a>
<a name="ln215">      if (now &lt; deadline) {</a>
<a name="ln216">        YB_LOG_EVERY_N_SECS(WARNING, 1)</a>
<a name="ln217">            &lt;&lt; &quot;Unable to send the request (&quot; &lt;&lt; req.ShortDebugString()</a>
<a name="ln218">            &lt;&lt; &quot;) to leader Master (&quot; &lt;&lt; leader_master_hostport().ToString()</a>
<a name="ln219">            &lt;&lt; &quot;): &quot; &lt;&lt; s.ToString();</a>
<a name="ln220">        if (IsMultiMaster()) {</a>
<a name="ln221">          YB_LOG_EVERY_N_SECS(INFO, 1) &lt;&lt; &quot;Determining the new leader Master and retrying...&quot;;</a>
<a name="ln222">          WARN_NOT_OK(SetMasterServerProxy(deadline),</a>
<a name="ln223">                      &quot;Unable to determine the new leader Master&quot;);</a>
<a name="ln224">        }</a>
<a name="ln225">        continue;</a>
<a name="ln226">      } else {</a>
<a name="ln227">        // Operation deadline expired during this latest RPC.</a>
<a name="ln228">        s = s.CloneAndPrepend(Format(</a>
<a name="ln229">            &quot;$0 timed out after deadline expired. Time elapsed: $1, allowed: $2&quot;,</a>
<a name="ln230">            func_name, now - start_time, deadline - start_time));</a>
<a name="ln231">      }</a>
<a name="ln232">    }</a>
<a name="ln233"> </a>
<a name="ln234">    if (s.ok() &amp;&amp; resp-&gt;has_error()) {</a>
<a name="ln235">      if (resp-&gt;error().code() == MasterErrorPB::NOT_THE_LEADER ||</a>
<a name="ln236">          resp-&gt;error().code() == MasterErrorPB::CATALOG_MANAGER_NOT_INITIALIZED) {</a>
<a name="ln237">        if (IsMultiMaster()) {</a>
<a name="ln238">          YB_LOG_EVERY_N_SECS(INFO, 1) &lt;&lt; &quot;Determining the new leader Master and retrying...&quot;;</a>
<a name="ln239">          WARN_NOT_OK(SetMasterServerProxy(deadline),</a>
<a name="ln240">                      &quot;Unable to determine the new leader Master&quot;);</a>
<a name="ln241">        }</a>
<a name="ln242">        continue;</a>
<a name="ln243">      } else {</a>
<a name="ln244">        return StatusFromPB(resp-&gt;error().status());</a>
<a name="ln245">      }</a>
<a name="ln246">    }</a>
<a name="ln247">    return s;</a>
<a name="ln248">  }</a>
<a name="ln249">}</a>
<a name="ln250"> </a>
<a name="ln251">#define YB_CLIENT_SPECIALIZE(RequestTypePB, ResponseTypePB) \</a>
<a name="ln252">    using yb::master::RequestTypePB; \</a>
<a name="ln253">    using yb::master::ResponseTypePB; \</a>
<a name="ln254">    template Status YBClient::Data::SyncLeaderMasterRpc( \</a>
<a name="ln255">        CoarseTimePoint deadline, const RequestTypePB&amp; req, \</a>
<a name="ln256">        ResponseTypePB* resp, int* num_attempts, const char* func_name, \</a>
<a name="ln257">        const std::function&lt;Status( \</a>
<a name="ln258">            MasterServiceProxy*, const RequestTypePB&amp;, ResponseTypePB*, RpcController*)&gt;&amp; \</a>
<a name="ln259">            func);</a>
<a name="ln260"> </a>
<a name="ln261">#define YB_CLIENT_SPECIALIZE_SIMPLE(prefix) \</a>
<a name="ln262">    YB_CLIENT_SPECIALIZE(BOOST_PP_CAT(prefix, RequestPB), BOOST_PP_CAT(prefix, ResponsePB))</a>
<a name="ln263"> </a>
<a name="ln264">// Explicit specialization for callers outside this compilation unit.</a>
<a name="ln265">YB_CLIENT_SPECIALIZE_SIMPLE(ListTables);</a>
<a name="ln266">YB_CLIENT_SPECIALIZE_SIMPLE(ListTabletServers);</a>
<a name="ln267">YB_CLIENT_SPECIALIZE_SIMPLE(GetTableLocations);</a>
<a name="ln268">YB_CLIENT_SPECIALIZE_SIMPLE(GetTabletLocations);</a>
<a name="ln269">YB_CLIENT_SPECIALIZE_SIMPLE(ListMasters);</a>
<a name="ln270">YB_CLIENT_SPECIALIZE_SIMPLE(CreateNamespace);</a>
<a name="ln271">YB_CLIENT_SPECIALIZE_SIMPLE(DeleteNamespace);</a>
<a name="ln272">YB_CLIENT_SPECIALIZE_SIMPLE(AlterNamespace);</a>
<a name="ln273">YB_CLIENT_SPECIALIZE_SIMPLE(ListNamespaces);</a>
<a name="ln274">YB_CLIENT_SPECIALIZE_SIMPLE(GetNamespaceInfo);</a>
<a name="ln275">YB_CLIENT_SPECIALIZE_SIMPLE(ReservePgsqlOids);</a>
<a name="ln276">YB_CLIENT_SPECIALIZE_SIMPLE(GetYsqlCatalogConfig);</a>
<a name="ln277">YB_CLIENT_SPECIALIZE_SIMPLE(CreateUDType);</a>
<a name="ln278">YB_CLIENT_SPECIALIZE_SIMPLE(DeleteUDType);</a>
<a name="ln279">YB_CLIENT_SPECIALIZE_SIMPLE(ListUDTypes);</a>
<a name="ln280">YB_CLIENT_SPECIALIZE_SIMPLE(GetUDTypeInfo);</a>
<a name="ln281">YB_CLIENT_SPECIALIZE_SIMPLE(CreateRole);</a>
<a name="ln282">YB_CLIENT_SPECIALIZE_SIMPLE(AlterRole);</a>
<a name="ln283">YB_CLIENT_SPECIALIZE_SIMPLE(DeleteRole);</a>
<a name="ln284">YB_CLIENT_SPECIALIZE_SIMPLE(GrantRevokeRole);</a>
<a name="ln285">YB_CLIENT_SPECIALIZE_SIMPLE(GrantRevokePermission);</a>
<a name="ln286">YB_CLIENT_SPECIALIZE_SIMPLE(GetPermissions);</a>
<a name="ln287">YB_CLIENT_SPECIALIZE_SIMPLE(RedisConfigSet);</a>
<a name="ln288">YB_CLIENT_SPECIALIZE_SIMPLE(RedisConfigGet);</a>
<a name="ln289">YB_CLIENT_SPECIALIZE_SIMPLE(CreateCDCStream);</a>
<a name="ln290">YB_CLIENT_SPECIALIZE_SIMPLE(DeleteCDCStream);</a>
<a name="ln291">YB_CLIENT_SPECIALIZE_SIMPLE(ListCDCStreams);</a>
<a name="ln292">YB_CLIENT_SPECIALIZE_SIMPLE(GetCDCStream);</a>
<a name="ln293">YB_CLIENT_SPECIALIZE_SIMPLE(CreateTablegroup);</a>
<a name="ln294">YB_CLIENT_SPECIALIZE_SIMPLE(DeleteTablegroup);</a>
<a name="ln295">YB_CLIENT_SPECIALIZE_SIMPLE(ListTablegroups);</a>
<a name="ln296">// These are not actually exposed outside, but it's nice to auto-add using directive.</a>
<a name="ln297">YB_CLIENT_SPECIALIZE_SIMPLE(AlterTable);</a>
<a name="ln298">YB_CLIENT_SPECIALIZE_SIMPLE(FlushTables);</a>
<a name="ln299">YB_CLIENT_SPECIALIZE_SIMPLE(ChangeMasterClusterConfig);</a>
<a name="ln300">YB_CLIENT_SPECIALIZE_SIMPLE(TruncateTable);</a>
<a name="ln301">YB_CLIENT_SPECIALIZE_SIMPLE(CreateTable);</a>
<a name="ln302">YB_CLIENT_SPECIALIZE_SIMPLE(DeleteTable);</a>
<a name="ln303">YB_CLIENT_SPECIALIZE_SIMPLE(GetMasterClusterConfig);</a>
<a name="ln304">YB_CLIENT_SPECIALIZE_SIMPLE(GetTableSchema);</a>
<a name="ln305">YB_CLIENT_SPECIALIZE_SIMPLE(IsAlterTableDone);</a>
<a name="ln306">YB_CLIENT_SPECIALIZE_SIMPLE(IsFlushTablesDone);</a>
<a name="ln307">YB_CLIENT_SPECIALIZE_SIMPLE(IsCreateTableDone);</a>
<a name="ln308">YB_CLIENT_SPECIALIZE_SIMPLE(IsTruncateTableDone);</a>
<a name="ln309">YB_CLIENT_SPECIALIZE_SIMPLE(IsDeleteTableDone);</a>
<a name="ln310">YB_CLIENT_SPECIALIZE_SIMPLE(IsLoadBalanced);</a>
<a name="ln311">YB_CLIENT_SPECIALIZE_SIMPLE(IsLoadBalancerIdle);</a>
<a name="ln312">YB_CLIENT_SPECIALIZE_SIMPLE(IsCreateNamespaceDone);</a>
<a name="ln313">YB_CLIENT_SPECIALIZE_SIMPLE(IsDeleteNamespaceDone);</a>
<a name="ln314"> </a>
<a name="ln315">YBClient::Data::Data()</a>
<a name="ln316">    : leader_master_rpc_(rpcs_.InvalidHandle()),</a>
<a name="ln317">      latest_observed_hybrid_time_(YBClient::kNoHybridTime),</a>
<a name="ln318">      id_(ClientId::GenerateRandom()) {}</a>
<a name="ln319"> </a>
<a name="ln320">YBClient::Data::~Data() {</a>
<a name="ln321">  rpcs_.Shutdown();</a>
<a name="ln322">}</a>
<a name="ln323"> </a>
<a name="ln324">RemoteTabletServer* YBClient::Data::SelectTServer(RemoteTablet* rt,</a>
<a name="ln325">                                                  const ReplicaSelection selection,</a>
<a name="ln326">                                                  const set&lt;string&gt;&amp; blacklist,</a>
<a name="ln327">                                                  vector&lt;RemoteTabletServer*&gt;* candidates) {</a>
<a name="ln328">  RemoteTabletServer* ret = nullptr;</a>
<a name="ln329">  candidates-&gt;clear();</a>
<a name="ln330">  if (PREDICT_FALSE(FLAGS_TEST_assert_local_tablet_server_selected ||</a>
<a name="ln331">                    !FLAGS_TEST_assert_tablet_server_select_is_in_zone.empty()) &amp;&amp;</a>
<a name="ln332">      selection != CLOSEST_REPLICA) {</a>
<a name="ln333">    LOG(FATAL) &lt;&lt; &quot;Invalid ReplicaSelection &quot; &lt;&lt; selection;</a>
<a name="ln334">  }</a>
<a name="ln335"> </a>
<a name="ln336">  switch (selection) {</a>
<a name="ln337">    case LEADER_ONLY: {</a>
<a name="ln338">      ret = rt-&gt;LeaderTServer();</a>
<a name="ln339">      if (ret != nullptr) {</a>
<a name="ln340">        candidates-&gt;push_back(ret);</a>
<a name="ln341">        if (ContainsKey(blacklist, ret-&gt;permanent_uuid())) {</a>
<a name="ln342">          ret = nullptr;</a>
<a name="ln343">        }</a>
<a name="ln344">      }</a>
<a name="ln345">      break;</a>
<a name="ln346">    }</a>
<a name="ln347">    case CLOSEST_REPLICA:</a>
<a name="ln348">    case FIRST_REPLICA: {</a>
<a name="ln349">      if (PREDICT_TRUE(FLAGS_TEST_assert_tablet_server_select_is_in_zone.empty())) {</a>
<a name="ln350">        rt-&gt;GetRemoteTabletServers(candidates);</a>
<a name="ln351">      } else {</a>
<a name="ln352">        rt-&gt;GetRemoteTabletServers(candidates, internal::IncludeFailedReplicas::kTrue);</a>
<a name="ln353">      }</a>
<a name="ln354"> </a>
<a name="ln355">      // Filter out all the blacklisted candidates.</a>
<a name="ln356">      vector&lt;RemoteTabletServer*&gt; filtered;</a>
<a name="ln357">      for (RemoteTabletServer* rts : *candidates) {</a>
<a name="ln358">        if (!ContainsKey(blacklist, rts-&gt;permanent_uuid())) {</a>
<a name="ln359">          filtered.push_back(rts);</a>
<a name="ln360">        } else {</a>
<a name="ln361">          VLOG(1) &lt;&lt; &quot;Excluding blacklisted tserver &quot; &lt;&lt; rts-&gt;permanent_uuid();</a>
<a name="ln362">        }</a>
<a name="ln363">      }</a>
<a name="ln364">      if (selection == FIRST_REPLICA) {</a>
<a name="ln365">        if (!filtered.empty()) {</a>
<a name="ln366">          ret = filtered[0];</a>
<a name="ln367">        }</a>
<a name="ln368">      } else if (selection == CLOSEST_REPLICA) {</a>
<a name="ln369">        // Choose the closest replica.</a>
<a name="ln370">        bool local_zone_ts = false;</a>
<a name="ln371">        for (RemoteTabletServer* rts : filtered) {</a>
<a name="ln372">          if (IsTabletServerLocal(*rts)) {</a>
<a name="ln373">            ret = rts;</a>
<a name="ln374">            // If the tserver is local, we are done here.</a>
<a name="ln375">            break;</a>
<a name="ln376">          } else if (cloud_info_pb_.has_placement_region() &amp;&amp;</a>
<a name="ln377">                     rts-&gt;cloud_info().has_placement_region() &amp;&amp;</a>
<a name="ln378">                     cloud_info_pb_.placement_region() == rts-&gt;cloud_info().placement_region()) {</a>
<a name="ln379">            if (cloud_info_pb_.has_placement_zone() &amp;&amp; rts-&gt;cloud_info().has_placement_zone() &amp;&amp;</a>
<a name="ln380">                cloud_info_pb_.placement_zone() == rts-&gt;cloud_info().placement_zone()) {</a>
<a name="ln381">              // Note down that we have found a zone local tserver and continue looking for node</a>
<a name="ln382">              // local tserver.</a>
<a name="ln383">              ret = rts;</a>
<a name="ln384">              local_zone_ts = true;</a>
<a name="ln385">            } else if (!local_zone_ts) {</a>
<a name="ln386">              // Look for a region local tserver only if we haven't found a zone local tserver yet.</a>
<a name="ln387">              ret = rts;</a>
<a name="ln388">            }</a>
<a name="ln389">          }</a>
<a name="ln390">        }</a>
<a name="ln391"> </a>
<a name="ln392">        // If ret is not null here, it should point to the closest replica from the client.</a>
<a name="ln393"> </a>
<a name="ln394">        // Fallback to a random replica if none are local.</a>
<a name="ln395">        if (ret == nullptr &amp;&amp; !filtered.empty()) {</a>
<a name="ln396">          ret = filtered[rand() % filtered.size()];</a>
<a name="ln397">        }</a>
<a name="ln398">      }</a>
<a name="ln399">      break;</a>
<a name="ln400">    }</a>
<a name="ln401">    default:</a>
<a name="ln402">      FATAL_INVALID_ENUM_VALUE(ReplicaSelection, selection);</a>
<a name="ln403">  }</a>
<a name="ln404">  if (PREDICT_FALSE(FLAGS_TEST_assert_local_tablet_server_selected) &amp;&amp; !IsTabletServerLocal(*ret)) {</a>
<a name="ln405">    LOG(FATAL) &lt;&lt; &quot;Selected replica is not the local tablet server&quot;;</a>
<a name="ln406">  }</a>
<a name="ln407">  if (PREDICT_FALSE(!FLAGS_TEST_assert_tablet_server_select_is_in_zone.empty())) {</a>
<a name="ln408">    if (ret-&gt;cloud_info().placement_zone() != FLAGS_TEST_assert_tablet_server_select_is_in_zone) {</a>
<a name="ln409">      string msg = Substitute(&quot;\nZone placement:\nNumber of candidates: $0\n&quot;, candidates-&gt;size());</a>
<a name="ln410">      for (RemoteTabletServer* rts : *candidates) {</a>
<a name="ln411">        msg += Substitute(&quot;Replica: $0 in zone $1\n&quot;,</a>
<a name="ln412">                          rts-&gt;ToString(), rts-&gt;cloud_info().placement_zone());</a>
<a name="ln413">      }</a>
<a name="ln414">      LOG(FATAL) &lt;&lt; &quot;Selected replica &quot; &lt;&lt; ret-&gt;ToString()</a>
<a name="ln415">                 &lt;&lt; &quot; is in zone &quot; &lt;&lt; ret-&gt;cloud_info().placement_zone()</a>
<a name="ln416">                 &lt;&lt; &quot; instead of the expected zone &quot;</a>
<a name="ln417">                 &lt;&lt; FLAGS_TEST_assert_tablet_server_select_is_in_zone</a>
<a name="ln418">                 &lt;&lt; &quot; Cloud info: &quot; &lt;&lt; cloud_info_pb_.ShortDebugString()</a>
<a name="ln419">                 &lt;&lt; &quot; for selection policy &quot; &lt;&lt; selection</a>
<a name="ln420">                 &lt;&lt; msg;</a>
<a name="ln421">    }</a>
<a name="ln422">  }</a>
<a name="ln423"> </a>
<a name="ln424">  return ret;</a>
<a name="ln425">}</a>
<a name="ln426"> </a>
<a name="ln427">Status YBClient::Data::GetTabletServer(YBClient* client,</a>
<a name="ln428">                                       const scoped_refptr&lt;RemoteTablet&gt;&amp; rt,</a>
<a name="ln429">                                       ReplicaSelection selection,</a>
<a name="ln430">                                       const set&lt;string&gt;&amp; blacklist,</a>
<a name="ln431">                                       vector&lt;RemoteTabletServer*&gt;* candidates,</a>
<a name="ln432">                                       RemoteTabletServer** ts) {</a>
<a name="ln433">  // TODO: write a proper async version of this for async client.</a>
<a name="ln434">  RemoteTabletServer* ret = SelectTServer(rt.get(), selection, blacklist, candidates);</a>
<a name="ln435">  if (PREDICT_FALSE(ret == nullptr)) {</a>
<a name="ln436">    // Construct a blacklist string if applicable.</a>
<a name="ln437">    string blacklist_string = &quot;&quot;;</a>
<a name="ln438">    if (!blacklist.empty()) {</a>
<a name="ln439">      blacklist_string = Substitute(&quot;(blacklist replicas $0)&quot;, JoinStrings(blacklist, &quot;, &quot;));</a>
<a name="ln440">    }</a>
<a name="ln441">    return STATUS(ServiceUnavailable,</a>
<a name="ln442">        Substitute(&quot;No $0 for tablet $1 $2&quot;,</a>
<a name="ln443">                   selection == LEADER_ONLY ? &quot;LEADER&quot; : &quot;replicas&quot;,</a>
<a name="ln444">                   rt-&gt;tablet_id(),</a>
<a name="ln445">                   blacklist_string));</a>
<a name="ln446">  }</a>
<a name="ln447">  RETURN_NOT_OK(ret-&gt;InitProxy(client));</a>
<a name="ln448"> </a>
<a name="ln449">  *ts = ret;</a>
<a name="ln450">  return Status::OK();</a>
<a name="ln451">}</a>
<a name="ln452"> </a>
<a name="ln453">Status YBClient::Data::CreateTable(YBClient* client,</a>
<a name="ln454">                                   const CreateTableRequestPB&amp; req,</a>
<a name="ln455">                                   const YBSchema&amp; schema,</a>
<a name="ln456">                                   CoarseTimePoint deadline,</a>
<a name="ln457">                                   string* table_id) {</a>
<a name="ln458">  CreateTableResponsePB resp;</a>
<a name="ln459"> </a>
<a name="ln460">  int attempts = 0;</a>
<a name="ln461">  Status s = SyncLeaderMasterRpc&lt;CreateTableRequestPB, CreateTableResponsePB&gt;(</a>
<a name="ln462">      deadline, req, &amp;resp, &amp;attempts, &quot;CreateTable&quot;, &amp;MasterServiceProxy::CreateTable);</a>
<a name="ln463">  // Set the table id even if there was an error. This is useful when the error is IsAlreadyPresent</a>
<a name="ln464">  // so that we can wait for the existing table to be available to receive requests.</a>
<a name="ln465">  *table_id = resp.table_id();</a>
<a name="ln466"> </a>
<a name="ln467">  // Handle special cases based on resp.error().</a>
<a name="ln468">  if (resp.has_error()) {</a>
<a name="ln469">    LOG_IF(DFATAL, s.ok()) &lt;&lt; &quot;Expecting error status if response has error: &quot; &lt;&lt;</a>
<a name="ln470">        resp.error().code() &lt;&lt; &quot; Status: &quot; &lt;&lt; resp.error().status().ShortDebugString();</a>
<a name="ln471"> </a>
<a name="ln472">    if (resp.error().code() == MasterErrorPB::OBJECT_ALREADY_PRESENT &amp;&amp; attempts &gt; 1) {</a>
<a name="ln473">      // If the table already exists and the number of attempts is &gt;</a>
<a name="ln474">      // 1, then it means we may have succeeded in creating the</a>
<a name="ln475">      // table, but client didn't receive the successful</a>
<a name="ln476">      // response (e.g., due to failure before the successful</a>
<a name="ln477">      // response could be sent back, or due to a I/O pause or a</a>
<a name="ln478">      // network blip leading to a timeout, etc...)</a>
<a name="ln479">      YBTableInfo info;</a>
<a name="ln480">      const string keyspace = req.has_namespace_()</a>
<a name="ln481">          ? req.namespace_().name()</a>
<a name="ln482">          : (req.name() == common::kRedisTableName ? common::kRedisKeyspaceName : &quot;&quot;);</a>
<a name="ln483">      const YQLDatabase db_type = req.has_namespace_() &amp;&amp; req.namespace_().has_database_type()</a>
<a name="ln484">          ? req.namespace_().database_type()</a>
<a name="ln485">          : (keyspace.empty() ? YQL_DATABASE_CQL : master::GetDefaultDatabaseType(keyspace));</a>
<a name="ln486"> </a>
<a name="ln487">      // Identify the table by name.</a>
<a name="ln488">      LOG_IF(DFATAL, keyspace.empty()) &lt;&lt; &quot;No keyspace. Request:\n&quot; &lt;&lt; req.DebugString();</a>
<a name="ln489">      const YBTableName table_name(db_type, keyspace, req.name());</a>
<a name="ln490"> </a>
<a name="ln491">      // A fix for https://yugabyte.atlassian.net/browse/ENG-529:</a>
<a name="ln492">      // If we've been retrying table creation, and the table is now in the process is being</a>
<a name="ln493">      // created, we can sometimes see an empty schema. Wait until the table is fully created</a>
<a name="ln494">      // before we compare the schema.</a>
<a name="ln495">      RETURN_NOT_OK_PREPEND(</a>
<a name="ln496">          WaitForCreateTableToFinish(client, table_name, resp.table_id(), deadline),</a>
<a name="ln497">          Substitute(&quot;Failed waiting for table $0 to finish being created&quot;, table_name.ToString()));</a>
<a name="ln498"> </a>
<a name="ln499">      RETURN_NOT_OK_PREPEND(</a>
<a name="ln500">          GetTableSchema(client, table_name, deadline, &amp;info),</a>
<a name="ln501">          Substitute(&quot;Unable to check the schema of table $0&quot;, table_name.ToString()));</a>
<a name="ln502">      if (!schema.Equals(info.schema)) {</a>
<a name="ln503">         string msg = Format(&quot;Table $0 already exists with a different &quot;</a>
<a name="ln504">                             &quot;schema. Requested schema was: $1, actual schema is: $2&quot;,</a>
<a name="ln505">                             table_name,</a>
<a name="ln506">                             internal::GetSchema(schema),</a>
<a name="ln507">                             internal::GetSchema(info.schema));</a>
<a name="ln508">        LOG(ERROR) &lt;&lt; msg;</a>
<a name="ln509">        return STATUS(AlreadyPresent, msg);</a>
<a name="ln510">      }</a>
<a name="ln511"> </a>
<a name="ln512">      // The partition schema in the request can be empty.</a>
<a name="ln513">      // If there are user partition schema in the request - compare it with the received one.</a>
<a name="ln514">      if (req.partition_schema().hash_bucket_schemas_size() &gt; 0) {</a>
<a name="ln515">        PartitionSchema partition_schema;</a>
<a name="ln516">        // We need to use the schema received from the server, because the user-constructed</a>
<a name="ln517">        // schema might not have column ids.</a>
<a name="ln518">        RETURN_NOT_OK(PartitionSchema::FromPB(req.partition_schema(),</a>
<a name="ln519">                                              internal::GetSchema(info.schema),</a>
<a name="ln520">                                              &amp;partition_schema));</a>
<a name="ln521">        if (!partition_schema.Equals(info.partition_schema)) {</a>
<a name="ln522">          string msg = Substitute(&quot;Table $0 already exists with a different partition schema. &quot;</a>
<a name="ln523">              &quot;Requested partition schema was: $1, actual partition schema is: $2&quot;,</a>
<a name="ln524">              table_name.ToString(),</a>
<a name="ln525">              partition_schema.DebugString(internal::GetSchema(schema)),</a>
<a name="ln526">              info.partition_schema.DebugString(internal::GetSchema(info.schema)));</a>
<a name="ln527">          LOG(ERROR) &lt;&lt; msg;</a>
<a name="ln528">          return STATUS(AlreadyPresent, msg);</a>
<a name="ln529">        }</a>
<a name="ln530">      }</a>
<a name="ln531"> </a>
<a name="ln532">      return Status::OK();</a>
<a name="ln533">    }</a>
<a name="ln534"> </a>
<a name="ln535">    return StatusFromPB(resp.error().status());</a>
<a name="ln536">  }</a>
<a name="ln537"> </a>
<a name="ln538">  // Use the status only if the response has no error.</a>
<a name="ln539">  return s;</a>
<a name="ln540">}</a>
<a name="ln541"> </a>
<a name="ln542">Status YBClient::Data::IsCreateTableInProgress(YBClient* client,</a>
<a name="ln543">                                               const YBTableName&amp; table_name,</a>
<a name="ln544">                                               const string&amp; table_id,</a>
<a name="ln545">                                               CoarseTimePoint deadline,</a>
<a name="ln546">                                               bool* create_in_progress) {</a>
<a name="ln547">  DCHECK_ONLY_NOTNULL(create_in_progress);</a>
<a name="ln548">  IsCreateTableDoneRequestPB req;</a>
<a name="ln549">  IsCreateTableDoneResponsePB resp;</a>
<a name="ln550">  if (table_name.has_table()) {</a>
<a name="ln551">    table_name.SetIntoTableIdentifierPB(req.mutable_table());</a>
<a name="ln552">  }</a>
<a name="ln553">  if (!table_id.empty()) {</a>
<a name="ln554">    req.mutable_table()-&gt;set_table_id(table_id);</a>
<a name="ln555">  }</a>
<a name="ln556"> </a>
<a name="ln557">  const Status s =</a>
<a name="ln558">      SyncLeaderMasterRpc&lt;IsCreateTableDoneRequestPB, IsCreateTableDoneResponsePB&gt;(</a>
<a name="ln559">          deadline,</a>
<a name="ln560">          req,</a>
<a name="ln561">          &amp;resp,</a>
<a name="ln562">          nullptr /* num_attempts */,</a>
<a name="ln563">          &quot;IsCreateTableDone&quot;,</a>
<a name="ln564">          &amp;MasterServiceProxy::IsCreateTableDone);</a>
<a name="ln565">  // RETURN_NOT_OK macro can't take templated function call as param,</a>
<a name="ln566">  // and SyncLeaderMasterRpc must be explicitly instantiated, else the</a>
<a name="ln567">  // compiler complains.</a>
<a name="ln568">  RETURN_NOT_OK(s);</a>
<a name="ln569">  if (resp.has_error()) {</a>
<a name="ln570">    return StatusFromPB(resp.error().status());</a>
<a name="ln571">  }</a>
<a name="ln572"> </a>
<a name="ln573">  *create_in_progress = !resp.done();</a>
<a name="ln574">  return Status::OK();</a>
<a name="ln575">}</a>
<a name="ln576"> </a>
<a name="ln577">Status YBClient::Data::WaitForCreateTableToFinish(YBClient* client,</a>
<a name="ln578">                                                  const YBTableName&amp; table_name,</a>
<a name="ln579">                                                  const string&amp; table_id,</a>
<a name="ln580">                                                  CoarseTimePoint deadline) {</a>
<a name="ln581">  return RetryFunc(</a>
<a name="ln582">      deadline, &quot;Waiting on Create Table to be completed&quot;, &quot;Timed out waiting for Table Creation&quot;,</a>
<a name="ln583">      std::bind(&amp;YBClient::Data::IsCreateTableInProgress, this, client,</a>
<a name="ln584">                table_name, table_id, _1, _2));</a>
<a name="ln585">}</a>
<a name="ln586"> </a>
<a name="ln587">Status YBClient::Data::DeleteTable(YBClient* client,</a>
<a name="ln588">                                   const YBTableName&amp; table_name,</a>
<a name="ln589">                                   const string&amp; table_id,</a>
<a name="ln590">                                   const bool is_index_table,</a>
<a name="ln591">                                   CoarseTimePoint deadline,</a>
<a name="ln592">                                   YBTableName* indexed_table_name,</a>
<a name="ln593">                                   bool wait) {</a>
<a name="ln594">  DeleteTableRequestPB req;</a>
<a name="ln595">  DeleteTableResponsePB resp;</a>
<a name="ln596">  int attempts = 0;</a>
<a name="ln597"> </a>
<a name="ln598">  if (table_name.has_table()) {</a>
<a name="ln599">    table_name.SetIntoTableIdentifierPB(req.mutable_table());</a>
<a name="ln600">  }</a>
<a name="ln601">  if (!table_id.empty()) {</a>
<a name="ln602">    req.mutable_table()-&gt;set_table_id(table_id);</a>
<a name="ln603">  }</a>
<a name="ln604">  req.set_is_index_table(is_index_table);</a>
<a name="ln605">  const Status s = SyncLeaderMasterRpc&lt;DeleteTableRequestPB, DeleteTableResponsePB&gt;(</a>
<a name="ln606">      deadline, req, &amp;resp, &amp;attempts, &quot;DeleteTable&quot;, &amp;MasterServiceProxy::DeleteTable);</a>
<a name="ln607"> </a>
<a name="ln608">  // Handle special cases based on resp.error().</a>
<a name="ln609">  if (resp.has_error()) {</a>
<a name="ln610">    LOG_IF(DFATAL, s.ok()) &lt;&lt; &quot;Expecting error status if response has error: &quot; &lt;&lt;</a>
<a name="ln611">        resp.error().code() &lt;&lt; &quot; Status: &quot; &lt;&lt; resp.error().status().ShortDebugString();</a>
<a name="ln612"> </a>
<a name="ln613">    if (resp.error().code() == MasterErrorPB::OBJECT_NOT_FOUND &amp;&amp; attempts &gt; 1) {</a>
<a name="ln614">      // A prior attempt to delete the table has succeeded, but</a>
<a name="ln615">      // appeared as a failure to the client due to, e.g., an I/O or</a>
<a name="ln616">      // network issue.</a>
<a name="ln617">      // Good case - go through - to 'return Status::OK()'</a>
<a name="ln618">    } else {</a>
<a name="ln619">      return StatusFromPB(resp.error().status());</a>
<a name="ln620">    }</a>
<a name="ln621">  } else {</a>
<a name="ln622">    // Check the status only if the response has no error.</a>
<a name="ln623">    RETURN_NOT_OK(s);</a>
<a name="ln624">  }</a>
<a name="ln625"> </a>
<a name="ln626">  // Spin until the table is fully deleted, if requested.</a>
<a name="ln627">  if (wait &amp;&amp; resp.has_table_id()) {</a>
<a name="ln628">    RETURN_NOT_OK(WaitForDeleteTableToFinish(client, resp.table_id(), deadline));</a>
<a name="ln629">  }</a>
<a name="ln630">  if (wait &amp;&amp; resp.has_indexed_table()) {</a>
<a name="ln631">    auto res = WaitUntilIndexPermissionsAtLeast(</a>
<a name="ln632">        client, resp.indexed_table().table_id(), resp.table_id(), deadline,</a>
<a name="ln633">        IndexPermissions::INDEX_PERM_NOT_USED);</a>
<a name="ln634">    if (!res &amp;&amp; !res.status().IsNotFound()) {</a>
<a name="ln635">      LOG(WARNING) &lt;&lt; &quot;Waiting for the index to be deleted from the indexed table, got &quot; &lt;&lt; res;</a>
<a name="ln636">      return res.status();</a>
<a name="ln637">    }</a>
<a name="ln638">  }</a>
<a name="ln639"> </a>
<a name="ln640">  // Return indexed table name if requested.</a>
<a name="ln641">  if (resp.has_indexed_table() &amp;&amp; indexed_table_name != nullptr) {</a>
<a name="ln642">    indexed_table_name-&gt;GetFromTableIdentifierPB(resp.indexed_table());</a>
<a name="ln643">  }</a>
<a name="ln644"> </a>
<a name="ln645">  LOG(INFO) &lt;&lt; &quot;Deleted table &quot; &lt;&lt; (!table_id.empty() ? table_id : table_name.ToString());</a>
<a name="ln646">  return Status::OK();</a>
<a name="ln647">}</a>
<a name="ln648"> </a>
<a name="ln649">Status YBClient::Data::IsDeleteTableInProgress(YBClient* client,</a>
<a name="ln650">                                               const std::string&amp; table_id,</a>
<a name="ln651">                                               CoarseTimePoint deadline,</a>
<a name="ln652">                                               bool* delete_in_progress) {</a>
<a name="ln653">  DCHECK_ONLY_NOTNULL(delete_in_progress);</a>
<a name="ln654">  IsDeleteTableDoneRequestPB req;</a>
<a name="ln655">  IsDeleteTableDoneResponsePB resp;</a>
<a name="ln656">  req.set_table_id(table_id);</a>
<a name="ln657"> </a>
<a name="ln658">  const Status s =</a>
<a name="ln659">      SyncLeaderMasterRpc&lt;IsDeleteTableDoneRequestPB, IsDeleteTableDoneResponsePB&gt;(</a>
<a name="ln660">          deadline,</a>
<a name="ln661">          req,</a>
<a name="ln662">          &amp;resp,</a>
<a name="ln663">          nullptr /* num_attempts */,</a>
<a name="ln664">          &quot;IsDeleteTableDone&quot;,</a>
<a name="ln665">          &amp;MasterServiceProxy::IsDeleteTableDone);</a>
<a name="ln666">  // RETURN_NOT_OK macro can't take templated function call as param,</a>
<a name="ln667">  // and SyncLeaderMasterRpc must be explicitly instantiated, else the</a>
<a name="ln668">  // compiler complains.</a>
<a name="ln669">  RETURN_NOT_OK(s);</a>
<a name="ln670">  if (resp.has_error()) {</a>
<a name="ln671">    if (resp.error().code() == MasterErrorPB::OBJECT_NOT_FOUND) {</a>
<a name="ln672">      *delete_in_progress = false;</a>
<a name="ln673">      return Status::OK();</a>
<a name="ln674">    }</a>
<a name="ln675">    return StatusFromPB(resp.error().status());</a>
<a name="ln676">  }</a>
<a name="ln677"> </a>
<a name="ln678">  *delete_in_progress = !resp.done();</a>
<a name="ln679">  return Status::OK();</a>
<a name="ln680">}</a>
<a name="ln681"> </a>
<a name="ln682">Status YBClient::Data::WaitForDeleteTableToFinish(YBClient* client,</a>
<a name="ln683">                                                  const std::string&amp; table_id,</a>
<a name="ln684">                                                  CoarseTimePoint deadline) {</a>
<a name="ln685">  return RetryFunc(</a>
<a name="ln686">      deadline, &quot;Waiting on Delete Table to be completed&quot;, &quot;Timed out waiting for Table Deletion&quot;,</a>
<a name="ln687">      std::bind(&amp;YBClient::Data::IsDeleteTableInProgress, this, client, table_id, _1, _2));</a>
<a name="ln688">}</a>
<a name="ln689"> </a>
<a name="ln690">Status YBClient::Data::TruncateTables(YBClient* client,</a>
<a name="ln691">                                     const vector&lt;string&gt;&amp; table_ids,</a>
<a name="ln692">                                     CoarseTimePoint deadline,</a>
<a name="ln693">                                     bool wait) {</a>
<a name="ln694">  TruncateTableRequestPB req;</a>
<a name="ln695">  TruncateTableResponsePB resp;</a>
<a name="ln696"> </a>
<a name="ln697">  for (const auto&amp; table_id : table_ids) {</a>
<a name="ln698">    req.add_table_ids(table_id);</a>
<a name="ln699">  }</a>
<a name="ln700">  RETURN_NOT_OK((SyncLeaderMasterRpc&lt;TruncateTableRequestPB, TruncateTableResponsePB&gt;(</a>
<a name="ln701">      deadline, req, &amp;resp, nullptr /* num_attempts */, &quot;TruncateTable&quot;,</a>
<a name="ln702">      &amp;MasterServiceProxy::TruncateTable)));</a>
<a name="ln703">  if (resp.has_error()) {</a>
<a name="ln704">    return StatusFromPB(resp.error().status());</a>
<a name="ln705">  }</a>
<a name="ln706"> </a>
<a name="ln707">  // Spin until the table is fully truncated, if requested.</a>
<a name="ln708">  if (wait) {</a>
<a name="ln709">    for (const auto&amp; table_id : table_ids) {</a>
<a name="ln710">      RETURN_NOT_OK(WaitForTruncateTableToFinish(client, table_id, deadline));</a>
<a name="ln711">    }</a>
<a name="ln712">  }</a>
<a name="ln713"> </a>
<a name="ln714">  LOG(INFO) &lt;&lt; &quot;Truncated table(s) &quot; &lt;&lt; JoinStrings(table_ids, &quot;,&quot;);</a>
<a name="ln715">  return Status::OK();</a>
<a name="ln716">}</a>
<a name="ln717"> </a>
<a name="ln718">Status YBClient::Data::IsTruncateTableInProgress(YBClient* client,</a>
<a name="ln719">                                                 const std::string&amp; table_id,</a>
<a name="ln720">                                                 CoarseTimePoint deadline,</a>
<a name="ln721">                                                 bool* truncate_in_progress) {</a>
<a name="ln722">  DCHECK_ONLY_NOTNULL(truncate_in_progress);</a>
<a name="ln723">  IsTruncateTableDoneRequestPB req;</a>
<a name="ln724">  IsTruncateTableDoneResponsePB resp;</a>
<a name="ln725"> </a>
<a name="ln726">  req.set_table_id(table_id);</a>
<a name="ln727">  RETURN_NOT_OK((SyncLeaderMasterRpc&lt;IsTruncateTableDoneRequestPB, IsTruncateTableDoneResponsePB&gt;(</a>
<a name="ln728">      deadline, req, &amp;resp, nullptr /* num_attempts */, &quot;IsTruncateTableDone&quot;,</a>
<a name="ln729">      &amp;MasterServiceProxy::IsTruncateTableDone)));</a>
<a name="ln730">  if (resp.has_error()) {</a>
<a name="ln731">    return StatusFromPB(resp.error().status());</a>
<a name="ln732">  }</a>
<a name="ln733"> </a>
<a name="ln734">  *truncate_in_progress = !resp.done();</a>
<a name="ln735">  return Status::OK();</a>
<a name="ln736">}</a>
<a name="ln737"> </a>
<a name="ln738">Status YBClient::Data::WaitForTruncateTableToFinish(YBClient* client,</a>
<a name="ln739">                                                    const std::string&amp; table_id,</a>
<a name="ln740">                                                    CoarseTimePoint deadline) {</a>
<a name="ln741">  return RetryFunc(</a>
<a name="ln742">      deadline, &quot;Waiting on Truncate Table to be completed&quot;,</a>
<a name="ln743">      &quot;Timed out waiting for Table Truncation&quot;,</a>
<a name="ln744">      std::bind(&amp;YBClient::Data::IsTruncateTableInProgress, this, client, table_id, _1, _2));</a>
<a name="ln745">}</a>
<a name="ln746"> </a>
<a name="ln747">Status YBClient::Data::AlterNamespace(YBClient* client,</a>
<a name="ln748">                                      const AlterNamespaceRequestPB&amp; req,</a>
<a name="ln749">                                      CoarseTimePoint deadline) {</a>
<a name="ln750">  AlterNamespaceResponsePB resp;</a>
<a name="ln751">  Status s =</a>
<a name="ln752">      SyncLeaderMasterRpc&lt;AlterNamespaceRequestPB, AlterNamespaceResponsePB&gt;(</a>
<a name="ln753">          deadline,</a>
<a name="ln754">          req,</a>
<a name="ln755">          &amp;resp,</a>
<a name="ln756">          nullptr /* num_attempts */,</a>
<a name="ln757">          &quot;AlterNamespace&quot;,</a>
<a name="ln758">          &amp;MasterServiceProxy::AlterNamespace);</a>
<a name="ln759">  RETURN_NOT_OK(s);</a>
<a name="ln760">  if (resp.has_error()) {</a>
<a name="ln761">    return StatusFromPB(resp.error().status());</a>
<a name="ln762">  }</a>
<a name="ln763">  return Status::OK();</a>
<a name="ln764">}</a>
<a name="ln765"> </a>
<a name="ln766">Status YBClient::Data::IsCreateNamespaceInProgress(</a>
<a name="ln767">    YBClient* client,</a>
<a name="ln768">    const std::string&amp; namespace_name,</a>
<a name="ln769">    const boost::optional&lt;YQLDatabase&gt;&amp; database_type,</a>
<a name="ln770">    const std::string&amp; namespace_id,</a>
<a name="ln771">    CoarseTimePoint deadline,</a>
<a name="ln772">    bool *create_in_progress) {</a>
<a name="ln773">  DCHECK_ONLY_NOTNULL(create_in_progress);</a>
<a name="ln774">  IsCreateNamespaceDoneRequestPB req;</a>
<a name="ln775">  IsCreateNamespaceDoneResponsePB resp;</a>
<a name="ln776"> </a>
<a name="ln777">  req.mutable_namespace_()-&gt;set_name(namespace_name);</a>
<a name="ln778">  if (database_type) {</a>
<a name="ln779">    req.mutable_namespace_()-&gt;set_database_type(*database_type);</a>
<a name="ln780">  }</a>
<a name="ln781">  if (!namespace_id.empty()) {</a>
<a name="ln782">    req.mutable_namespace_()-&gt;set_id(namespace_id);</a>
<a name="ln783">  }</a>
<a name="ln784"> </a>
<a name="ln785">  // RETURN_NOT_OK macro can't take templated function call as param,</a>
<a name="ln786">  // and SyncLeaderMasterRpc must be explicitly instantiated, else the</a>
<a name="ln787">  // compiler complains.</a>
<a name="ln788">  const Status s =</a>
<a name="ln789">      SyncLeaderMasterRpc&lt;IsCreateNamespaceDoneRequestPB, IsCreateNamespaceDoneResponsePB&gt;(</a>
<a name="ln790">          deadline,</a>
<a name="ln791">          req,</a>
<a name="ln792">          &amp;resp,</a>
<a name="ln793">          nullptr /* num_attempts */,</a>
<a name="ln794">          &quot;IsCreateNamespaceDone&quot;,</a>
<a name="ln795">          &amp;MasterServiceProxy::IsCreateNamespaceDone);</a>
<a name="ln796"> </a>
<a name="ln797">  // IsCreate could return a terminal/done state as FAILED. This would result in an error'd Status.</a>
<a name="ln798">  if (resp.has_done()) {</a>
<a name="ln799">    *create_in_progress = !resp.done();</a>
<a name="ln800">  }</a>
<a name="ln801"> </a>
<a name="ln802">  RETURN_NOT_OK(s);</a>
<a name="ln803">  if (resp.has_error()) {</a>
<a name="ln804">    return StatusFromPB(resp.error().status());</a>
<a name="ln805">  }</a>
<a name="ln806"> </a>
<a name="ln807">  return Status::OK();</a>
<a name="ln808">}</a>
<a name="ln809"> </a>
<a name="ln810">Status YBClient::Data::WaitForCreateNamespaceToFinish(</a>
<a name="ln811">    YBClient* client,</a>
<a name="ln812">    const std::string&amp; namespace_name,</a>
<a name="ln813">    const boost::optional&lt;YQLDatabase&gt;&amp; database_type,</a>
<a name="ln814">    const std::string&amp; namespace_id,</a>
<a name="ln815">    CoarseTimePoint deadline) {</a>
<a name="ln816">  return RetryFunc(</a>
<a name="ln817">      deadline,</a>
<a name="ln818">      &quot;Waiting on Create Namespace to be completed&quot;,</a>
<a name="ln819">      &quot;Timed out waiting for Namespace Creation&quot;,</a>
<a name="ln820">      std::bind(&amp;YBClient::Data::IsCreateNamespaceInProgress, this, client,</a>
<a name="ln821">          namespace_name, database_type, namespace_id, _1, _2));</a>
<a name="ln822">}</a>
<a name="ln823"> </a>
<a name="ln824">Status YBClient::Data::IsDeleteNamespaceInProgress(YBClient* client,</a>
<a name="ln825">    const std::string&amp; namespace_name,</a>
<a name="ln826">    const boost::optional&lt;YQLDatabase&gt;&amp; database_type,</a>
<a name="ln827">    const std::string&amp; namespace_id,</a>
<a name="ln828">    CoarseTimePoint deadline,</a>
<a name="ln829">    bool* delete_in_progress) {</a>
<a name="ln830">  DCHECK_ONLY_NOTNULL(delete_in_progress);</a>
<a name="ln831">  IsDeleteNamespaceDoneRequestPB req;</a>
<a name="ln832">  IsDeleteNamespaceDoneResponsePB resp;</a>
<a name="ln833"> </a>
<a name="ln834">  req.mutable_namespace_()-&gt;set_name(namespace_name);</a>
<a name="ln835">  if (database_type) {</a>
<a name="ln836">    req.mutable_namespace_()-&gt;set_database_type(*database_type);</a>
<a name="ln837">  }</a>
<a name="ln838">  if (!namespace_id.empty()) {</a>
<a name="ln839">    req.mutable_namespace_()-&gt;set_id(namespace_id);</a>
<a name="ln840">  }</a>
<a name="ln841"> </a>
<a name="ln842">  const Status s =</a>
<a name="ln843">      SyncLeaderMasterRpc&lt;IsDeleteNamespaceDoneRequestPB, IsDeleteNamespaceDoneResponsePB&gt;(</a>
<a name="ln844">          deadline,</a>
<a name="ln845">          req,</a>
<a name="ln846">          &amp;resp,</a>
<a name="ln847">          nullptr, // num_attempts</a>
<a name="ln848">          &quot;IsDeleteNamespaceDone&quot;,</a>
<a name="ln849">          &amp;MasterServiceProxy::IsDeleteNamespaceDone);</a>
<a name="ln850">  // RETURN_NOT_OK macro can't take templated function call as param,</a>
<a name="ln851">  // and SyncLeaderMasterRpc must be explicitly instantiated, else the</a>
<a name="ln852">  // compiler complains.</a>
<a name="ln853">  RETURN_NOT_OK(s);</a>
<a name="ln854">  if (resp.has_error()) {</a>
<a name="ln855">    if (resp.error().code() == MasterErrorPB::OBJECT_NOT_FOUND) {</a>
<a name="ln856">      *delete_in_progress = false;</a>
<a name="ln857">      return Status::OK();</a>
<a name="ln858">    }</a>
<a name="ln859">    return StatusFromPB(resp.error().status());</a>
<a name="ln860">  }</a>
<a name="ln861"> </a>
<a name="ln862">  *delete_in_progress = !resp.done();</a>
<a name="ln863">  return Status::OK();</a>
<a name="ln864">}</a>
<a name="ln865"> </a>
<a name="ln866">Status YBClient::Data::WaitForDeleteNamespaceToFinish(YBClient* client,</a>
<a name="ln867">    const std::string&amp; namespace_name,</a>
<a name="ln868">    const boost::optional&lt;YQLDatabase&gt;&amp; database_type,</a>
<a name="ln869">    const std::string&amp; namespace_id,</a>
<a name="ln870">    CoarseTimePoint deadline) {</a>
<a name="ln871">  return RetryFunc(</a>
<a name="ln872">      deadline,</a>
<a name="ln873">      &quot;Waiting on Delete Namespace to be completed&quot;,</a>
<a name="ln874">      &quot;Timed out waiting for Namespace Deletion&quot;,</a>
<a name="ln875">      std::bind(&amp;YBClient::Data::IsDeleteNamespaceInProgress, this,</a>
<a name="ln876">          client, namespace_name, database_type, namespace_id, _1, _2));</a>
<a name="ln877">}</a>
<a name="ln878"> </a>
<a name="ln879">Status YBClient::Data::AlterTable(YBClient* client,</a>
<a name="ln880">                                  const AlterTableRequestPB&amp; req,</a>
<a name="ln881">                                  CoarseTimePoint deadline) {</a>
<a name="ln882">  AlterTableResponsePB resp;</a>
<a name="ln883">  Status s =</a>
<a name="ln884">      SyncLeaderMasterRpc&lt;AlterTableRequestPB, AlterTableResponsePB&gt;(</a>
<a name="ln885">          deadline,</a>
<a name="ln886">          req,</a>
<a name="ln887">          &amp;resp,</a>
<a name="ln888">          nullptr /* num_attempts */,</a>
<a name="ln889">          &quot;AlterTable&quot;,</a>
<a name="ln890">          &amp;MasterServiceProxy::AlterTable);</a>
<a name="ln891">  RETURN_NOT_OK(s);</a>
<a name="ln892">  // TODO: Consider the situation where the request is sent to the</a>
<a name="ln893">  // server, gets executed on the server and written to the server,</a>
<a name="ln894">  // but is seen as failed by the client, and is then retried (in which</a>
<a name="ln895">  // case the retry will fail due to original table being removed, a</a>
<a name="ln896">  // column being already added, etc...)</a>
<a name="ln897">  if (resp.has_error()) {</a>
<a name="ln898">    return StatusFromPB(resp.error().status());</a>
<a name="ln899">  }</a>
<a name="ln900">  return Status::OK();</a>
<a name="ln901">}</a>
<a name="ln902"> </a>
<a name="ln903">Status YBClient::Data::IsAlterTableInProgress(YBClient* client,</a>
<a name="ln904">                                              const YBTableName&amp; table_name,</a>
<a name="ln905">                                              string table_id,</a>
<a name="ln906">                                              CoarseTimePoint deadline,</a>
<a name="ln907">                                              bool *alter_in_progress) {</a>
<a name="ln908">  IsAlterTableDoneRequestPB req;</a>
<a name="ln909">  IsAlterTableDoneResponsePB resp;</a>
<a name="ln910"> </a>
<a name="ln911">  if (table_name.has_table()) {</a>
<a name="ln912">    table_name.SetIntoTableIdentifierPB(req.mutable_table());</a>
<a name="ln913">  }</a>
<a name="ln914"> </a>
<a name="ln915">  if (!table_id.empty()) {</a>
<a name="ln916">    (req.mutable_table())-&gt;set_table_id(table_id);</a>
<a name="ln917">  }</a>
<a name="ln918"> </a>
<a name="ln919">  Status s =</a>
<a name="ln920">      SyncLeaderMasterRpc&lt;IsAlterTableDoneRequestPB, IsAlterTableDoneResponsePB&gt;(</a>
<a name="ln921">          deadline,</a>
<a name="ln922">          req,</a>
<a name="ln923">          &amp;resp,</a>
<a name="ln924">          nullptr /* num_attempts */,</a>
<a name="ln925">          &quot;IsAlterTableDone&quot;,</a>
<a name="ln926">          &amp;MasterServiceProxy::IsAlterTableDone);</a>
<a name="ln927">  RETURN_NOT_OK(s);</a>
<a name="ln928">  if (resp.has_error()) {</a>
<a name="ln929">    return StatusFromPB(resp.error().status());</a>
<a name="ln930">  }</a>
<a name="ln931"> </a>
<a name="ln932">  *alter_in_progress = !resp.done();</a>
<a name="ln933">  return Status::OK();</a>
<a name="ln934">}</a>
<a name="ln935"> </a>
<a name="ln936">Status YBClient::Data::WaitForAlterTableToFinish(YBClient* client,</a>
<a name="ln937">                                                 const YBTableName&amp; alter_name,</a>
<a name="ln938">                                                 const string table_id,</a>
<a name="ln939">                                                 CoarseTimePoint deadline) {</a>
<a name="ln940">  return RetryFunc(</a>
<a name="ln941">      deadline, &quot;Waiting on Alter Table to be completed&quot;, &quot;Timed out waiting for AlterTable&quot;,</a>
<a name="ln942">      std::bind(&amp;YBClient::Data::IsAlterTableInProgress, this, client,</a>
<a name="ln943">              alter_name, table_id, _1, _2));</a>
<a name="ln944">}</a>
<a name="ln945"> </a>
<a name="ln946">CHECKED_STATUS YBClient::Data::FlushTablesHelper(YBClient* client,</a>
<a name="ln947">                                                const CoarseTimePoint deadline,</a>
<a name="ln948">                                                const FlushTablesRequestPB req) {</a>
<a name="ln949">  int attempts = 0;</a>
<a name="ln950">  FlushTablesResponsePB resp;</a>
<a name="ln951"> </a>
<a name="ln952">  RETURN_NOT_OK((SyncLeaderMasterRpc&lt;FlushTablesRequestPB, FlushTablesResponsePB&gt;(</a>
<a name="ln953">      deadline, req, &amp;resp, &amp;attempts, &quot;FlushTables&quot;, &amp;MasterServiceProxy::FlushTables)));</a>
<a name="ln954">  if (resp.has_error()) {</a>
<a name="ln955">    return StatusFromPB(resp.error().status());</a>
<a name="ln956">  }</a>
<a name="ln957"> </a>
<a name="ln958">  // Spin until the table is flushed.</a>
<a name="ln959">  if (!resp.flush_request_id().empty()) {</a>
<a name="ln960">    RETURN_NOT_OK(WaitForFlushTableToFinish(client, resp.flush_request_id(), deadline));</a>
<a name="ln961">  }</a>
<a name="ln962"> </a>
<a name="ln963">  LOG(INFO) &lt;&lt; (req.is_compaction() ? &quot;Compacted&quot; : &quot;Flushed&quot;)</a>
<a name="ln964">            &lt;&lt; &quot; table &quot;</a>
<a name="ln965">            &lt;&lt; req.tables(0).ShortDebugString()</a>
<a name="ln966">            &lt;&lt; (req.add_indexes() ? &quot; and indexes&quot; : &quot;&quot;);</a>
<a name="ln967">  return Status::OK();</a>
<a name="ln968">}</a>
<a name="ln969"> </a>
<a name="ln970">CHECKED_STATUS YBClient::Data::FlushTables(YBClient* client,</a>
<a name="ln971">                                           const vector&lt;YBTableName&gt;&amp; table_names,</a>
<a name="ln972">                                           bool add_indexes,</a>
<a name="ln973">                                           const CoarseTimePoint deadline,</a>
<a name="ln974">                                           const bool is_compaction) {</a>
<a name="ln975">  FlushTablesRequestPB req;</a>
<a name="ln976">  req.set_add_indexes(add_indexes);</a>
<a name="ln977">  req.set_is_compaction(is_compaction);</a>
<a name="ln978">  for (const auto&amp; table : table_names) {</a>
<a name="ln979">    table.SetIntoTableIdentifierPB(req.add_tables());</a>
<a name="ln980">  }</a>
<a name="ln981"> </a>
<a name="ln982">  return FlushTablesHelper(client, deadline, req);</a>
<a name="ln983">}</a>
<a name="ln984"> </a>
<a name="ln985">CHECKED_STATUS YBClient::Data::FlushTables(YBClient* client,</a>
<a name="ln986">                                           const vector&lt;TableId&gt;&amp; table_ids,</a>
<a name="ln987">                                           bool add_indexes,</a>
<a name="ln988">                                           const CoarseTimePoint deadline,</a>
<a name="ln989">                                           const bool is_compaction) {</a>
<a name="ln990">  FlushTablesRequestPB req;</a>
<a name="ln991">  req.set_add_indexes(add_indexes);</a>
<a name="ln992">  req.set_is_compaction(is_compaction);</a>
<a name="ln993">  for (const auto&amp; table : table_ids) {</a>
<a name="ln994">    req.add_tables()-&gt;set_table_id(table);</a>
<a name="ln995">  }</a>
<a name="ln996"> </a>
<a name="ln997">  return FlushTablesHelper(client, deadline, req);</a>
<a name="ln998">}</a>
<a name="ln999"> </a>
<a name="ln1000">Status YBClient::Data::IsFlushTableInProgress(YBClient* client,</a>
<a name="ln1001">                                              const FlushRequestId&amp; flush_id,</a>
<a name="ln1002">                                              const CoarseTimePoint deadline,</a>
<a name="ln1003">                                              bool *flush_in_progress) {</a>
<a name="ln1004">  DCHECK_ONLY_NOTNULL(flush_in_progress);</a>
<a name="ln1005">  IsFlushTablesDoneRequestPB req;</a>
<a name="ln1006">  IsFlushTablesDoneResponsePB resp;</a>
<a name="ln1007"> </a>
<a name="ln1008">  req.set_flush_request_id(flush_id);</a>
<a name="ln1009">  RETURN_NOT_OK((SyncLeaderMasterRpc&lt;IsFlushTablesDoneRequestPB, IsFlushTablesDoneResponsePB&gt;(</a>
<a name="ln1010">      deadline, req, &amp;resp, nullptr /* num_attempts */, &quot;IsFlushTableDone&quot;,</a>
<a name="ln1011">      &amp;MasterServiceProxy::IsFlushTablesDone)));</a>
<a name="ln1012">  if (resp.has_error()) {</a>
<a name="ln1013">    return StatusFromPB(resp.error().status());</a>
<a name="ln1014">  }</a>
<a name="ln1015"> </a>
<a name="ln1016">  *flush_in_progress = !resp.done();</a>
<a name="ln1017">  return Status::OK();</a>
<a name="ln1018">}</a>
<a name="ln1019"> </a>
<a name="ln1020">Status YBClient::Data::WaitForFlushTableToFinish(YBClient* client,</a>
<a name="ln1021">                                                 const FlushRequestId&amp; flush_id,</a>
<a name="ln1022">                                                 const CoarseTimePoint deadline) {</a>
<a name="ln1023">  return RetryFunc(</a>
<a name="ln1024">      deadline, &quot;Waiting for FlushTables to be completed&quot;, &quot;Timed out waiting for FlushTables&quot;,</a>
<a name="ln1025">      std::bind(&amp;YBClient::Data::IsFlushTableInProgress, this, client, flush_id, _1, _2));</a>
<a name="ln1026">}</a>
<a name="ln1027"> </a>
<a name="ln1028">Status YBClient::Data::InitLocalHostNames() {</a>
<a name="ln1029">  std::vector&lt;IpAddress&gt; addresses;</a>
<a name="ln1030">  auto status = GetLocalAddresses(&amp;addresses, AddressFilter::EXTERNAL);</a>
<a name="ln1031">  if (!status.ok()) {</a>
<a name="ln1032">    LOG(WARNING) &lt;&lt; &quot;Failed to enumerate network interfaces&quot; &lt;&lt; status.ToString();</a>
<a name="ln1033">  }</a>
<a name="ln1034"> </a>
<a name="ln1035">  string hostname;</a>
<a name="ln1036">  status = GetFQDN(&amp;hostname);</a>
<a name="ln1037"> </a>
<a name="ln1038">  if (status.ok()) {</a>
<a name="ln1039">    // We don't want to consider 'localhost' to be local - otherwise if a misconfigured</a>
<a name="ln1040">    // server reports its own name as localhost, all clients will hammer it.</a>
<a name="ln1041">    if (hostname != &quot;localhost&quot; &amp;&amp; hostname != &quot;localhost.localdomain&quot;) {</a>
<a name="ln1042">      local_host_names_.insert(hostname);</a>
<a name="ln1043">      VLOG(1) &lt;&lt; &quot;Considering host &quot; &lt;&lt; hostname &lt;&lt; &quot; local&quot;;</a>
<a name="ln1044">    }</a>
<a name="ln1045"> </a>
<a name="ln1046">    std::vector&lt;Endpoint&gt; endpoints;</a>
<a name="ln1047">    status = HostPort(hostname, 0).ResolveAddresses(&amp;endpoints);</a>
<a name="ln1048">    if (!status.ok()) {</a>
<a name="ln1049">      const auto message = Substitute(&quot;Could not resolve local host name '$0'&quot;, hostname);</a>
<a name="ln1050">      LOG(WARNING) &lt;&lt; message;</a>
<a name="ln1051">      if (addresses.empty()) {</a>
<a name="ln1052">        return status.CloneAndPrepend(message);</a>
<a name="ln1053">      }</a>
<a name="ln1054">    } else {</a>
<a name="ln1055">      addresses.reserve(addresses.size() + endpoints.size());</a>
<a name="ln1056">      for (const auto&amp; endpoint : endpoints) {</a>
<a name="ln1057">        addresses.push_back(endpoint.address());</a>
<a name="ln1058">      }</a>
<a name="ln1059">    }</a>
<a name="ln1060">  } else {</a>
<a name="ln1061">    LOG(WARNING) &lt;&lt; &quot;Failed to get hostname: &quot; &lt;&lt; status.ToString();</a>
<a name="ln1062">    if (addresses.empty()) {</a>
<a name="ln1063">      return status;</a>
<a name="ln1064">    }</a>
<a name="ln1065">  }</a>
<a name="ln1066"> </a>
<a name="ln1067">  for (const auto&amp; addr : addresses) {</a>
<a name="ln1068">    // Similar to above, ignore local or wildcard addresses.</a>
<a name="ln1069">    if (addr.is_unspecified() || addr.is_loopback()) continue;</a>
<a name="ln1070"> </a>
<a name="ln1071">    VLOG(1) &lt;&lt; &quot;Considering host &quot; &lt;&lt; addr &lt;&lt; &quot; local&quot;;</a>
<a name="ln1072">    local_host_names_.insert(addr.to_string());</a>
<a name="ln1073">  }</a>
<a name="ln1074"> </a>
<a name="ln1075">  return Status::OK();</a>
<a name="ln1076">}</a>
<a name="ln1077"> </a>
<a name="ln1078">bool YBClient::Data::IsLocalHostPort(const HostPort&amp; hp) const {</a>
<a name="ln1079">  return ContainsKey(local_host_names_, hp.host());</a>
<a name="ln1080">}</a>
<a name="ln1081"> </a>
<a name="ln1082">bool YBClient::Data::IsTabletServerLocal(const RemoteTabletServer&amp; rts) const {</a>
<a name="ln1083">  // If the uuid's are same, we are sure the tablet server is local, since if this client is used</a>
<a name="ln1084">  // via the CQL proxy, the tablet server's uuid is set in the client.</a>
<a name="ln1085">  if (uuid_ == rts.permanent_uuid()) {</a>
<a name="ln1086">    return true;</a>
<a name="ln1087">  }</a>
<a name="ln1088"> </a>
<a name="ln1089">  return rts.HasHostFrom(local_host_names_);</a>
<a name="ln1090">}</a>
<a name="ln1091"> </a>
<a name="ln1092">namespace internal {</a>
<a name="ln1093"> </a>
<a name="ln1094">// Gets data from the leader master. If the leader master</a>
<a name="ln1095">// is down, waits for a new master to become the leader, and then gets</a>
<a name="ln1096">// the data from the new leader master.</a>
<a name="ln1097">class ClientMasterRpc : public Rpc {</a>
<a name="ln1098"> public:</a>
<a name="ln1099">  ClientMasterRpc(YBClient* client,</a>
<a name="ln1100">                  CoarseTimePoint deadline,</a>
<a name="ln1101">                  rpc::Messenger* messenger,</a>
<a name="ln1102">                  rpc::ProxyCache* proxy_cache);</a>
<a name="ln1103"> </a>
<a name="ln1104">  virtual ~ClientMasterRpc();</a>
<a name="ln1105"> </a>
<a name="ln1106">  void ResetLeaderMasterAndRetry();</a>
<a name="ln1107"> </a>
<a name="ln1108">  void NewLeaderMasterDeterminedCb(const Status&amp; status);</a>
<a name="ln1109"> </a>
<a name="ln1110">  template&lt;class Response&gt;</a>
<a name="ln1111">  Status HandleFinished(const Status&amp; status, const Response&amp; resp, bool* finished);</a>
<a name="ln1112"> </a>
<a name="ln1113"> private:</a>
<a name="ln1114">  YBClient* const client_;</a>
<a name="ln1115"> </a>
<a name="ln1116">};</a>
<a name="ln1117"> </a>
<a name="ln1118">// Gets a table's schema from the leader master. If the leader master</a>
<a name="ln1119">// is down, waits for a new master to become the leader, and then gets</a>
<a name="ln1120">// the table schema from the new leader master.</a>
<a name="ln1121">//</a>
<a name="ln1122">// TODO: When we implement the next fault tolerant client-master RPC</a>
<a name="ln1123">// call (e.g., CreateTable/AlterTable), we should generalize this</a>
<a name="ln1124">// method as to enable code sharing.</a>
<a name="ln1125">class GetTableSchemaRpc : public ClientMasterRpc {</a>
<a name="ln1126"> public:</a>
<a name="ln1127">  GetTableSchemaRpc(YBClient* client,</a>
<a name="ln1128">                    StatusCallback user_cb,</a>
<a name="ln1129">                    const YBTableName&amp; table_name,</a>
<a name="ln1130">                    YBTableInfo* info,</a>
<a name="ln1131">                    CoarseTimePoint deadline,</a>
<a name="ln1132">                    rpc::Messenger* messenger,</a>
<a name="ln1133">                    rpc::ProxyCache* proxy_cache);</a>
<a name="ln1134">  GetTableSchemaRpc(YBClient* client,</a>
<a name="ln1135">                    StatusCallback user_cb,</a>
<a name="ln1136">                    const TableId&amp; table_id,</a>
<a name="ln1137">                    YBTableInfo* info,</a>
<a name="ln1138">                    CoarseTimePoint deadline,</a>
<a name="ln1139">                    rpc::Messenger* messenger,</a>
<a name="ln1140">                    rpc::ProxyCache* proxy_cache);</a>
<a name="ln1141"> </a>
<a name="ln1142">  void SendRpc() override;</a>
<a name="ln1143"> </a>
<a name="ln1144">  string ToString() const override;</a>
<a name="ln1145"> </a>
<a name="ln1146">  virtual ~GetTableSchemaRpc();</a>
<a name="ln1147"> </a>
<a name="ln1148"> private:</a>
<a name="ln1149">  void Finished(const Status&amp; status) override;</a>
<a name="ln1150"> </a>
<a name="ln1151">  YBClient* const client_;</a>
<a name="ln1152">  StatusCallback user_cb_;</a>
<a name="ln1153">  master::TableIdentifierPB table_identifier_;</a>
<a name="ln1154">  YBTableInfo* info_;</a>
<a name="ln1155">  GetTableSchemaRequestPB req_;</a>
<a name="ln1156">  GetTableSchemaResponsePB resp_;</a>
<a name="ln1157">  rpc::Rpcs::Handle retained_self_;</a>
<a name="ln1158">};</a>
<a name="ln1159"> </a>
<a name="ln1160">namespace {</a>
<a name="ln1161"> </a>
<a name="ln1162">master::TableIdentifierPB ToTableIdentifierPB(const YBTableName&amp; table_name) {</a>
<a name="ln1163">  master::TableIdentifierPB id;</a>
<a name="ln1164">  table_name.SetIntoTableIdentifierPB(&amp;id);</a>
<a name="ln1165">  return id;</a>
<a name="ln1166">}</a>
<a name="ln1167"> </a>
<a name="ln1168">master::TableIdentifierPB ToTableIdentifierPB(const TableId&amp; table_id) {</a>
<a name="ln1169">  master::TableIdentifierPB id;</a>
<a name="ln1170">  id.set_table_id(table_id);</a>
<a name="ln1171">  return id;</a>
<a name="ln1172">}</a>
<a name="ln1173"> </a>
<a name="ln1174">} // namespace</a>
<a name="ln1175"> </a>
<a name="ln1176">ClientMasterRpc::ClientMasterRpc(YBClient* client,</a>
<a name="ln1177">                                 CoarseTimePoint deadline,</a>
<a name="ln1178">                                 rpc::Messenger* messenger,</a>
<a name="ln1179">                                 rpc::ProxyCache* proxy_cache)</a>
<a name="ln1180">    : Rpc(deadline, messenger, proxy_cache),</a>
<a name="ln1181">      client_(DCHECK_NOTNULL(client)) {</a>
<a name="ln1182">}</a>
<a name="ln1183"> </a>
<a name="ln1184">ClientMasterRpc::~ClientMasterRpc() {</a>
<a name="ln1185">}</a>
<a name="ln1186"> </a>
<a name="ln1187">void ClientMasterRpc::ResetLeaderMasterAndRetry() {</a>
<a name="ln1188">  client_-&gt;data_-&gt;SetMasterServerProxyAsync(</a>
<a name="ln1189">      retrier().deadline(),</a>
<a name="ln1190">      false /* skip_resolution */,</a>
<a name="ln1191">      true, /* wait for leader election */</a>
<a name="ln1192">      Bind(&amp;ClientMasterRpc::NewLeaderMasterDeterminedCb,</a>
<a name="ln1193">           Unretained(this)));</a>
<a name="ln1194">}</a>
<a name="ln1195"> </a>
<a name="ln1196">void ClientMasterRpc::NewLeaderMasterDeterminedCb(const Status&amp; status) {</a>
<a name="ln1197">  if (status.ok()) {</a>
<a name="ln1198">    mutable_retrier()-&gt;mutable_controller()-&gt;Reset();</a>
<a name="ln1199">    SendRpc();</a>
<a name="ln1200">  } else {</a>
<a name="ln1201">    LOG(WARNING) &lt;&lt; &quot;Failed to determine new Master: &quot; &lt;&lt; status.ToString();</a>
<a name="ln1202">    ScheduleRetry(status);</a>
<a name="ln1203">  }</a>
<a name="ln1204">}</a>
<a name="ln1205"> </a>
<a name="ln1206">template&lt;class Response&gt;</a>
<a name="ln1207">Status ClientMasterRpc::HandleFinished(const Status&amp; status, const Response&amp; resp,</a>
<a name="ln1208">                                       bool* finished) {</a>
<a name="ln1209">  *finished = false;</a>
<a name="ln1210">  Status new_status = status;</a>
<a name="ln1211">  if (new_status.ok() &amp;&amp; mutable_retrier()-&gt;HandleResponse(this, &amp;new_status)) {</a>
<a name="ln1212">    return new_status;</a>
<a name="ln1213">  }</a>
<a name="ln1214"> </a>
<a name="ln1215">  if (new_status.ok() &amp;&amp; resp.has_error()) {</a>
<a name="ln1216">    if (resp.error().code() == MasterErrorPB::NOT_THE_LEADER ||</a>
<a name="ln1217">        resp.error().code() == MasterErrorPB::CATALOG_MANAGER_NOT_INITIALIZED) {</a>
<a name="ln1218">      LOG(WARNING) &lt;&lt; &quot;Leader Master has changed (&quot;</a>
<a name="ln1219">                   &lt;&lt; client_-&gt;data_-&gt;leader_master_hostport().ToString()</a>
<a name="ln1220">                   &lt;&lt; &quot; is no longer the leader), re-trying...&quot;;</a>
<a name="ln1221">      ResetLeaderMasterAndRetry();</a>
<a name="ln1222">      return new_status;</a>
<a name="ln1223">    }</a>
<a name="ln1224"> </a>
<a name="ln1225">    if (resp.error().status().code() == AppStatusPB::LEADER_NOT_READY_TO_SERVE ||</a>
<a name="ln1226">        resp.error().status().code() == AppStatusPB::LEADER_HAS_NO_LEASE) {</a>
<a name="ln1227">      LOG(WARNING) &lt;&lt; &quot;Leader Master &quot; &lt;&lt; client_-&gt;data_-&gt;leader_master_hostport().ToString()</a>
<a name="ln1228">                   &lt;&lt; &quot; does not have a valid exclusive lease: &quot;</a>
<a name="ln1229">                   &lt;&lt; resp.error().status().ShortDebugString() &lt;&lt; &quot;, re-trying...&quot;;</a>
<a name="ln1230">      ResetLeaderMasterAndRetry();</a>
<a name="ln1231">      return new_status;</a>
<a name="ln1232">    }</a>
<a name="ln1233">    VLOG(2) &lt;&lt; &quot;resp.error().status()=&quot; &lt;&lt; resp.error().status().DebugString();</a>
<a name="ln1234">    new_status = StatusFromPB(resp.error().status());</a>
<a name="ln1235">  }</a>
<a name="ln1236"> </a>
<a name="ln1237">  if (new_status.IsTimedOut()) {</a>
<a name="ln1238">    if (CoarseMonoClock::Now() &lt; retrier().deadline()) {</a>
<a name="ln1239">      LOG(WARNING) &lt;&lt; &quot;Leader Master (&quot;</a>
<a name="ln1240">          &lt;&lt; client_-&gt;data_-&gt;leader_master_hostport().ToString()</a>
<a name="ln1241">          &lt;&lt; &quot;) timed out, re-trying...&quot;;</a>
<a name="ln1242">      ResetLeaderMasterAndRetry();</a>
<a name="ln1243">      return new_status;</a>
<a name="ln1244">    } else {</a>
<a name="ln1245">      // Operation deadline expired during this latest RPC.</a>
<a name="ln1246">      new_status = new_status.CloneAndPrepend(</a>
<a name="ln1247">          &quot;RPC timed out after deadline expired&quot;);</a>
<a name="ln1248">    }</a>
<a name="ln1249">  }</a>
<a name="ln1250"> </a>
<a name="ln1251">  if (new_status.IsNetworkError()) {</a>
<a name="ln1252">    LOG(WARNING) &lt;&lt; &quot;Encountered a network error from the Master(&quot;</a>
<a name="ln1253">                 &lt;&lt; client_-&gt;data_-&gt;leader_master_hostport().ToString() &lt;&lt; &quot;): &quot;</a>
<a name="ln1254">                 &lt;&lt; new_status.ToString() &lt;&lt; &quot;, retrying...&quot;;</a>
<a name="ln1255">    ResetLeaderMasterAndRetry();</a>
<a name="ln1256">    return new_status;</a>
<a name="ln1257">  }</a>
<a name="ln1258"> </a>
<a name="ln1259">  *finished = true;</a>
<a name="ln1260">  return new_status;</a>
<a name="ln1261">}</a>
<a name="ln1262"> </a>
<a name="ln1263">GetTableSchemaRpc::GetTableSchemaRpc(YBClient* client,</a>
<a name="ln1264">                                     StatusCallback user_cb,</a>
<a name="ln1265">                                     const YBTableName&amp; table_name,</a>
<a name="ln1266">                                     YBTableInfo* info,</a>
<a name="ln1267">                                     CoarseTimePoint deadline,</a>
<a name="ln1268">                                     rpc::Messenger* messenger,</a>
<a name="ln1269">                                     rpc::ProxyCache* proxy_cache)</a>
<a name="ln1270">    : ClientMasterRpc(client, deadline, messenger, proxy_cache),</a>
<a name="ln1271">      client_(DCHECK_NOTNULL(client)),</a>
<a name="ln1272">      user_cb_(std::move(user_cb)),</a>
<a name="ln1273">      table_identifier_(ToTableIdentifierPB(table_name)),</a>
<a name="ln1274">      info_(DCHECK_NOTNULL(info)),</a>
<a name="ln1275">      retained_self_(client-&gt;data_-&gt;rpcs_.InvalidHandle()) {</a>
<a name="ln1276">}</a>
<a name="ln1277"> </a>
<a name="ln1278">GetTableSchemaRpc::GetTableSchemaRpc(YBClient* client,</a>
<a name="ln1279">                                     StatusCallback user_cb,</a>
<a name="ln1280">                                     const TableId&amp; table_id,</a>
<a name="ln1281">                                     YBTableInfo* info,</a>
<a name="ln1282">                                     CoarseTimePoint deadline,</a>
<a name="ln1283">                                     rpc::Messenger* messenger,</a>
<a name="ln1284">                                     rpc::ProxyCache* proxy_cache)</a>
<a name="ln1285">    : ClientMasterRpc(client, deadline, messenger, proxy_cache),</a>
<a name="ln1286">      client_(DCHECK_NOTNULL(client)),</a>
<a name="ln1287">      user_cb_(std::move(user_cb)),</a>
<a name="ln1288">      table_identifier_(ToTableIdentifierPB(table_id)),</a>
<a name="ln1289">      info_(DCHECK_NOTNULL(info)),</a>
<a name="ln1290">      retained_self_(client-&gt;data_-&gt;rpcs_.InvalidHandle()) {</a>
<a name="ln1291">}</a>
<a name="ln1292"> </a>
<a name="ln1293">GetTableSchemaRpc::~GetTableSchemaRpc() {</a>
<a name="ln1294">}</a>
<a name="ln1295"> </a>
<a name="ln1296">void GetTableSchemaRpc::SendRpc() {</a>
<a name="ln1297">  client_-&gt;data_-&gt;rpcs_.Register(shared_from_this(), &amp;retained_self_);</a>
<a name="ln1298"> </a>
<a name="ln1299">  auto now = CoarseMonoClock::Now();</a>
<a name="ln1300">  if (retrier().deadline() &lt; now) {</a>
<a name="ln1301">    Finished(STATUS(TimedOut, &quot;GetTableSchema timed out after deadline expired&quot;));</a>
<a name="ln1302">    return;</a>
<a name="ln1303">  }</a>
<a name="ln1304"> </a>
<a name="ln1305">  // See YBClient::Data::SyncLeaderMasterRpc().</a>
<a name="ln1306">  auto rpc_deadline = now + client_-&gt;default_rpc_timeout();</a>
<a name="ln1307">  mutable_retrier()-&gt;mutable_controller()-&gt;set_deadline(</a>
<a name="ln1308">      std::min(rpc_deadline, retrier().deadline()));</a>
<a name="ln1309"> </a>
<a name="ln1310">  req_.mutable_table()-&gt;CopyFrom(table_identifier_);</a>
<a name="ln1311">  client_-&gt;data_-&gt;master_proxy()-&gt;GetTableSchemaAsync(</a>
<a name="ln1312">      req_, &amp;resp_, mutable_retrier()-&gt;mutable_controller(),</a>
<a name="ln1313">      std::bind(&amp;GetTableSchemaRpc::Finished, this, Status::OK()));</a>
<a name="ln1314">}</a>
<a name="ln1315"> </a>
<a name="ln1316">string GetTableSchemaRpc::ToString() const {</a>
<a name="ln1317">  return Substitute(&quot;GetTableSchemaRpc(table_identifier: $0, num_attempts: $1)&quot;,</a>
<a name="ln1318">                    table_identifier_.ShortDebugString(), num_attempts());</a>
<a name="ln1319">}</a>
<a name="ln1320"> </a>
<a name="ln1321">void GetTableSchemaRpc::Finished(const Status&amp; status) {</a>
<a name="ln1322">  bool finished;</a>
<a name="ln1323">  Status new_status = HandleFinished(status, resp_, &amp;finished);</a>
<a name="ln1324">  if (!finished) {</a>
<a name="ln1325">    return;</a>
<a name="ln1326">  }</a>
<a name="ln1327"> </a>
<a name="ln1328">  auto retained_self = client_-&gt;data_-&gt;rpcs_.Unregister(&amp;retained_self_);</a>
<a name="ln1329"> </a>
<a name="ln1330">  if (new_status.ok()) {</a>
<a name="ln1331">    std::unique_ptr&lt;Schema&gt; schema(new Schema());</a>
<a name="ln1332">    new_status = SchemaFromPB(resp_.schema(), schema.get());</a>
<a name="ln1333">    if (new_status.ok()) {</a>
<a name="ln1334">      info_-&gt;schema.Reset(std::move(schema));</a>
<a name="ln1335">      info_-&gt;schema.set_version(resp_.version());</a>
<a name="ln1336">      new_status = PartitionSchema::FromPB(resp_.partition_schema(),</a>
<a name="ln1337">                                           GetSchema(&amp;info_-&gt;schema),</a>
<a name="ln1338">                                           &amp;info_-&gt;partition_schema);</a>
<a name="ln1339"> </a>
<a name="ln1340">      info_-&gt;table_name.GetFromTableIdentifierPB(resp_.identifier());</a>
<a name="ln1341">      info_-&gt;table_id = resp_.identifier().table_id();</a>
<a name="ln1342">      CHECK_OK(YBTable::PBToClientTableType(resp_.table_type(), &amp;info_-&gt;table_type));</a>
<a name="ln1343">      info_-&gt;index_map.FromPB(resp_.indexes());</a>
<a name="ln1344">      if (resp_.has_index_info()) {</a>
<a name="ln1345">        info_-&gt;index_info.emplace(resp_.index_info());</a>
<a name="ln1346">      }</a>
<a name="ln1347">      CHECK_GT(info_-&gt;table_id.size(), 0) &lt;&lt; &quot;Running against a too-old master&quot;;</a>
<a name="ln1348">      info_-&gt;colocated = resp_.colocated();</a>
<a name="ln1349">    }</a>
<a name="ln1350">  }</a>
<a name="ln1351">  if (!new_status.ok()) {</a>
<a name="ln1352">    LOG(WARNING) &lt;&lt; ToString() &lt;&lt; &quot; failed: &quot; &lt;&lt; new_status.ToString();</a>
<a name="ln1353">  }</a>
<a name="ln1354">  user_cb_.Run(new_status);</a>
<a name="ln1355">}</a>
<a name="ln1356"> </a>
<a name="ln1357">class CreateCDCStreamRpc : public ClientMasterRpc {</a>
<a name="ln1358"> public:</a>
<a name="ln1359">  CreateCDCStreamRpc(YBClient* client,</a>
<a name="ln1360">                     CreateCDCStreamCallback user_cb,</a>
<a name="ln1361">                     const TableId&amp; table_id,</a>
<a name="ln1362">                     const std::unordered_map&lt;std::string, std::string&gt;&amp; options,</a>
<a name="ln1363">                     CoarseTimePoint deadline,</a>
<a name="ln1364">                     rpc::Messenger* messenger,</a>
<a name="ln1365">                     rpc::ProxyCache* proxy_cache);</a>
<a name="ln1366"> </a>
<a name="ln1367">  void SendRpc() override;</a>
<a name="ln1368"> </a>
<a name="ln1369">  string ToString() const override;</a>
<a name="ln1370"> </a>
<a name="ln1371">  virtual ~CreateCDCStreamRpc();</a>
<a name="ln1372"> </a>
<a name="ln1373"> private:</a>
<a name="ln1374">  void Finished(const Status&amp; status) override;</a>
<a name="ln1375"> </a>
<a name="ln1376">  YBClient* const client_;</a>
<a name="ln1377">  CreateCDCStreamCallback user_cb_;</a>
<a name="ln1378">  std::string table_id_;</a>
<a name="ln1379">  std::unordered_map&lt;std::string, std::string&gt; options_;</a>
<a name="ln1380">  CreateCDCStreamRequestPB req_;</a>
<a name="ln1381">  CreateCDCStreamResponsePB resp_;</a>
<a name="ln1382">  rpc::Rpcs::Handle retained_self_;</a>
<a name="ln1383">};</a>
<a name="ln1384"> </a>
<a name="ln1385">CreateCDCStreamRpc::CreateCDCStreamRpc(YBClient* client,</a>
<a name="ln1386">                                       CreateCDCStreamCallback user_cb,</a>
<a name="ln1387">                                       const TableId&amp; table_id,</a>
<a name="ln1388">                                       const std::unordered_map&lt;std::string, std::string&gt;&amp; options,</a>
<a name="ln1389">                                       CoarseTimePoint deadline,</a>
<a name="ln1390">                                       rpc::Messenger* messenger,</a>
<a name="ln1391">                                       rpc::ProxyCache* proxy_cache)</a>
<a name="ln1392">    : ClientMasterRpc(client, deadline, messenger, proxy_cache),</a>
<a name="ln1393">      client_(DCHECK_NOTNULL(client)),</a>
<a name="ln1394">      user_cb_(std::move(user_cb)),</a>
<a name="ln1395">      table_id_(table_id),</a>
<a name="ln1396">      options_(options),</a>
<a name="ln1397">      retained_self_(client-&gt;data_-&gt;rpcs_.InvalidHandle()) {</a>
<a name="ln1398">}</a>
<a name="ln1399"> </a>
<a name="ln1400">CreateCDCStreamRpc::~CreateCDCStreamRpc() {</a>
<a name="ln1401">}</a>
<a name="ln1402"> </a>
<a name="ln1403">void CreateCDCStreamRpc::SendRpc() {</a>
<a name="ln1404">  client_-&gt;data_-&gt;rpcs_.Register(shared_from_this(), &amp;retained_self_);</a>
<a name="ln1405"> </a>
<a name="ln1406">  auto now = CoarseMonoClock::Now();</a>
<a name="ln1407">  if (retrier().deadline() &lt; now) {</a>
<a name="ln1408">    Finished(STATUS(TimedOut, &quot;CreateCDCStream timed out after deadline expired&quot;));</a>
<a name="ln1409">    return;</a>
<a name="ln1410">  }</a>
<a name="ln1411"> </a>
<a name="ln1412">  // See YBClient::Data::SyncLeaderMasterRpc().</a>
<a name="ln1413">  auto rpc_deadline = now + client_-&gt;default_rpc_timeout();</a>
<a name="ln1414">  mutable_retrier()-&gt;mutable_controller()-&gt;set_deadline(</a>
<a name="ln1415">      std::min(rpc_deadline, retrier().deadline()));</a>
<a name="ln1416"> </a>
<a name="ln1417">  req_.set_table_id(table_id_);</a>
<a name="ln1418">  req_.mutable_options()-&gt;Reserve(options_.size());</a>
<a name="ln1419">  for (const auto&amp; option : options_) {</a>
<a name="ln1420">    auto* op = req_.add_options();</a>
<a name="ln1421">    op-&gt;set_key(option.first);</a>
<a name="ln1422">    op-&gt;set_value(option.second);</a>
<a name="ln1423">  }</a>
<a name="ln1424"> </a>
<a name="ln1425">  client_-&gt;data_-&gt;master_proxy()-&gt;CreateCDCStreamAsync(</a>
<a name="ln1426">      req_, &amp;resp_, mutable_retrier()-&gt;mutable_controller(),</a>
<a name="ln1427">      std::bind(&amp;CreateCDCStreamRpc::Finished, this, Status::OK()));</a>
<a name="ln1428">}</a>
<a name="ln1429"> </a>
<a name="ln1430">string CreateCDCStreamRpc::ToString() const {</a>
<a name="ln1431">  return Substitute(&quot;CreateCDCStream(table_id: $0, num_attempts: $1)&quot;, table_id_, num_attempts());</a>
<a name="ln1432">}</a>
<a name="ln1433"> </a>
<a name="ln1434">void CreateCDCStreamRpc::Finished(const Status&amp; status) {</a>
<a name="ln1435">  bool finished;</a>
<a name="ln1436">  Status new_status = HandleFinished(status, resp_, &amp;finished);</a>
<a name="ln1437">  if (!finished) {</a>
<a name="ln1438">    return;</a>
<a name="ln1439">  }</a>
<a name="ln1440"> </a>
<a name="ln1441">  auto retained_self = client_-&gt;data_-&gt;rpcs_.Unregister(&amp;retained_self_);</a>
<a name="ln1442"> </a>
<a name="ln1443">  if (new_status.ok()) {</a>
<a name="ln1444">    user_cb_(resp_.stream_id());</a>
<a name="ln1445">  } else {</a>
<a name="ln1446">    LOG(WARNING) &lt;&lt; ToString() &lt;&lt; &quot; failed: &quot; &lt;&lt; new_status.ToString();</a>
<a name="ln1447">    user_cb_(new_status);</a>
<a name="ln1448">  }</a>
<a name="ln1449">}</a>
<a name="ln1450"> </a>
<a name="ln1451">class DeleteCDCStreamRpc : public ClientMasterRpc {</a>
<a name="ln1452"> public:</a>
<a name="ln1453">  DeleteCDCStreamRpc(YBClient* client,</a>
<a name="ln1454">                     StatusCallback user_cb,</a>
<a name="ln1455">                     const CDCStreamId&amp; stream_id,</a>
<a name="ln1456">                     CoarseTimePoint deadline,</a>
<a name="ln1457">                     rpc::Messenger* messenger,</a>
<a name="ln1458">                     rpc::ProxyCache* proxy_cache);</a>
<a name="ln1459"> </a>
<a name="ln1460">  void SendRpc() override;</a>
<a name="ln1461"> </a>
<a name="ln1462">  string ToString() const override;</a>
<a name="ln1463"> </a>
<a name="ln1464">  virtual ~DeleteCDCStreamRpc();</a>
<a name="ln1465"> </a>
<a name="ln1466"> private:</a>
<a name="ln1467">  void Finished(const Status&amp; status) override;</a>
<a name="ln1468"> </a>
<a name="ln1469">  YBClient* const client_;</a>
<a name="ln1470">  StatusCallback user_cb_;</a>
<a name="ln1471">  std::string stream_id_;</a>
<a name="ln1472">  DeleteCDCStreamRequestPB req_;</a>
<a name="ln1473">  DeleteCDCStreamResponsePB resp_;</a>
<a name="ln1474">  rpc::Rpcs::Handle retained_self_;</a>
<a name="ln1475">};</a>
<a name="ln1476"> </a>
<a name="ln1477">DeleteCDCStreamRpc::DeleteCDCStreamRpc(YBClient* client,</a>
<a name="ln1478">                                       StatusCallback user_cb,</a>
<a name="ln1479">                                       const CDCStreamId&amp; stream_id,</a>
<a name="ln1480">                                       CoarseTimePoint deadline,</a>
<a name="ln1481">                                       rpc::Messenger* messenger,</a>
<a name="ln1482">                                       rpc::ProxyCache* proxy_cache)</a>
<a name="ln1483">    : ClientMasterRpc(client, deadline, messenger, proxy_cache),</a>
<a name="ln1484">      client_(DCHECK_NOTNULL(client)),</a>
<a name="ln1485">      user_cb_(std::move(user_cb)),</a>
<a name="ln1486">      stream_id_(stream_id),</a>
<a name="ln1487">      retained_self_(client-&gt;data_-&gt;rpcs_.InvalidHandle()) {</a>
<a name="ln1488">}</a>
<a name="ln1489"> </a>
<a name="ln1490">DeleteCDCStreamRpc::~DeleteCDCStreamRpc() {</a>
<a name="ln1491">}</a>
<a name="ln1492"> </a>
<a name="ln1493">void DeleteCDCStreamRpc::SendRpc() {</a>
<a name="ln1494">  client_-&gt;data_-&gt;rpcs_.Register(shared_from_this(), &amp;retained_self_);</a>
<a name="ln1495"> </a>
<a name="ln1496">  auto now = CoarseMonoClock::Now();</a>
<a name="ln1497">  if (retrier().deadline() &lt; now) {</a>
<a name="ln1498">    Finished(STATUS(TimedOut, &quot;DeleteCDCStream timed out after deadline expired&quot;));</a>
<a name="ln1499">    return;</a>
<a name="ln1500">  }</a>
<a name="ln1501"> </a>
<a name="ln1502">  // See YBClient::Data::SyncLeaderMasterRpc().</a>
<a name="ln1503">  auto rpc_deadline = now + client_-&gt;default_rpc_timeout();</a>
<a name="ln1504">  mutable_retrier()-&gt;mutable_controller()-&gt;set_deadline(</a>
<a name="ln1505">      std::min(rpc_deadline, retrier().deadline()));</a>
<a name="ln1506"> </a>
<a name="ln1507">  req_.add_stream_id(stream_id_);</a>
<a name="ln1508">  client_-&gt;data_-&gt;master_proxy()-&gt;DeleteCDCStreamAsync(</a>
<a name="ln1509">      req_, &amp;resp_, mutable_retrier()-&gt;mutable_controller(),</a>
<a name="ln1510">      std::bind(&amp;DeleteCDCStreamRpc::Finished, this, Status::OK()));</a>
<a name="ln1511">}</a>
<a name="ln1512"> </a>
<a name="ln1513">string DeleteCDCStreamRpc::ToString() const {</a>
<a name="ln1514">  return Substitute(&quot;DeleteCDCStream(stream_id: $0, num_attempts: $1)&quot;,</a>
<a name="ln1515">                    stream_id_, num_attempts());</a>
<a name="ln1516">}</a>
<a name="ln1517"> </a>
<a name="ln1518">void DeleteCDCStreamRpc::Finished(const Status&amp; status) {</a>
<a name="ln1519">  bool finished;</a>
<a name="ln1520">  Status new_status = HandleFinished(status, resp_, &amp;finished);</a>
<a name="ln1521">  if (!finished) {</a>
<a name="ln1522">    return;</a>
<a name="ln1523">  }</a>
<a name="ln1524"> </a>
<a name="ln1525">  auto retained_self = client_-&gt;data_-&gt;rpcs_.Unregister(&amp;retained_self_);</a>
<a name="ln1526"> </a>
<a name="ln1527">  if (!new_status.ok()) {</a>
<a name="ln1528">    LOG(WARNING) &lt;&lt; ToString() &lt;&lt; &quot; failed: &quot; &lt;&lt; new_status.ToString();</a>
<a name="ln1529">  }</a>
<a name="ln1530">  user_cb_.Run(new_status);</a>
<a name="ln1531">}</a>
<a name="ln1532"> </a>
<a name="ln1533">class GetCDCStreamRpc : public ClientMasterRpc {</a>
<a name="ln1534"> public:</a>
<a name="ln1535">  GetCDCStreamRpc(YBClient* client,</a>
<a name="ln1536">                  StdStatusCallback user_cb,</a>
<a name="ln1537">                  const CDCStreamId&amp; stream_id,</a>
<a name="ln1538">                  TableId* table_id,</a>
<a name="ln1539">                  std::unordered_map&lt;std::string, std::string&gt;* options,</a>
<a name="ln1540">                  CoarseTimePoint deadline,</a>
<a name="ln1541">                  rpc::Messenger* messenger,</a>
<a name="ln1542">                  rpc::ProxyCache* proxy_cache);</a>
<a name="ln1543"> </a>
<a name="ln1544">  void SendRpc() override;</a>
<a name="ln1545"> </a>
<a name="ln1546">  string ToString() const override;</a>
<a name="ln1547"> </a>
<a name="ln1548">  virtual ~GetCDCStreamRpc();</a>
<a name="ln1549"> </a>
<a name="ln1550"> private:</a>
<a name="ln1551">  void Finished(const Status&amp; status) override;</a>
<a name="ln1552"> </a>
<a name="ln1553">  YBClient* const client_;</a>
<a name="ln1554">  StdStatusCallback user_cb_;</a>
<a name="ln1555">  std::string stream_id_;</a>
<a name="ln1556">  TableId* table_id_;</a>
<a name="ln1557">  std::unordered_map&lt;std::string, std::string&gt;* options_;</a>
<a name="ln1558">  GetCDCStreamRequestPB req_;</a>
<a name="ln1559">  GetCDCStreamResponsePB resp_;</a>
<a name="ln1560">  rpc::Rpcs::Handle retained_self_;</a>
<a name="ln1561">};</a>
<a name="ln1562"> </a>
<a name="ln1563">GetCDCStreamRpc::GetCDCStreamRpc(YBClient* client,</a>
<a name="ln1564">                                 StdStatusCallback user_cb,</a>
<a name="ln1565">                                 const CDCStreamId&amp; stream_id,</a>
<a name="ln1566">                                 TableId* table_id,</a>
<a name="ln1567">                                 std::unordered_map&lt;std::string, std::string&gt;* options,</a>
<a name="ln1568">                                 CoarseTimePoint deadline,</a>
<a name="ln1569">                                 rpc::Messenger* messenger,</a>
<a name="ln1570">                                 rpc::ProxyCache* proxy_cache)</a>
<a name="ln1571">    : ClientMasterRpc(client, deadline, messenger, proxy_cache),</a>
<a name="ln1572">      client_(DCHECK_NOTNULL(client)),</a>
<a name="ln1573">      user_cb_(std::move(user_cb)),</a>
<a name="ln1574">      stream_id_(stream_id),</a>
<a name="ln1575">      table_id_(DCHECK_NOTNULL(table_id)),</a>
<a name="ln1576">      options_(DCHECK_NOTNULL(options)),</a>
<a name="ln1577">      retained_self_(client-&gt;data_-&gt;rpcs_.InvalidHandle()) {</a>
<a name="ln1578">}</a>
<a name="ln1579"> </a>
<a name="ln1580">GetCDCStreamRpc::~GetCDCStreamRpc() {</a>
<a name="ln1581">}</a>
<a name="ln1582"> </a>
<a name="ln1583">void GetCDCStreamRpc::SendRpc() {</a>
<a name="ln1584">  client_-&gt;data_-&gt;rpcs_.Register(shared_from_this(), &amp;retained_self_);</a>
<a name="ln1585"> </a>
<a name="ln1586">  auto now = CoarseMonoClock::Now();</a>
<a name="ln1587">  if (retrier().deadline() &lt; now) {</a>
<a name="ln1588">    Finished(STATUS(TimedOut, &quot;GetCDCStream timed out after deadline expired&quot;));</a>
<a name="ln1589">    return;</a>
<a name="ln1590">  }</a>
<a name="ln1591"> </a>
<a name="ln1592">  // See YBClient::Data::SyncLeaderMasterRpc().</a>
<a name="ln1593">  auto rpc_deadline = now + client_-&gt;default_rpc_timeout();</a>
<a name="ln1594">  mutable_retrier()-&gt;mutable_controller()-&gt;set_deadline(</a>
<a name="ln1595">      std::min(rpc_deadline, retrier().deadline()));</a>
<a name="ln1596"> </a>
<a name="ln1597">  req_.set_stream_id(stream_id_);</a>
<a name="ln1598">  client_-&gt;data_-&gt;master_proxy()-&gt;GetCDCStreamAsync(</a>
<a name="ln1599">      req_, &amp;resp_, mutable_retrier()-&gt;mutable_controller(),</a>
<a name="ln1600">      std::bind(&amp;GetCDCStreamRpc::Finished, this, Status::OK()));</a>
<a name="ln1601">}</a>
<a name="ln1602"> </a>
<a name="ln1603">string GetCDCStreamRpc::ToString() const {</a>
<a name="ln1604">  return Substitute(&quot;GetCDCStream(stream_id: $0, num_attempts: $1)&quot;,</a>
<a name="ln1605">                    stream_id_, num_attempts());</a>
<a name="ln1606">}</a>
<a name="ln1607"> </a>
<a name="ln1608">void GetCDCStreamRpc::Finished(const Status&amp; status) {</a>
<a name="ln1609">  bool finished;</a>
<a name="ln1610">  Status new_status = HandleFinished(status, resp_, &amp;finished);</a>
<a name="ln1611">  if (!finished) {</a>
<a name="ln1612">    return;</a>
<a name="ln1613">  }</a>
<a name="ln1614"> </a>
<a name="ln1615">  auto retained_self = client_-&gt;data_-&gt;rpcs_.Unregister(&amp;retained_self_);</a>
<a name="ln1616"> </a>
<a name="ln1617">  if (!new_status.ok()) {</a>
<a name="ln1618">    LOG(WARNING) &lt;&lt; ToString() &lt;&lt; &quot; failed: &quot; &lt;&lt; new_status.ToString();</a>
<a name="ln1619">  } else {</a>
<a name="ln1620">    *table_id_ = resp_.stream().table_id();</a>
<a name="ln1621"> </a>
<a name="ln1622">    options_-&gt;clear();</a>
<a name="ln1623">    options_-&gt;reserve(resp_.stream().options_size());</a>
<a name="ln1624">    for (const auto&amp; option : resp_.stream().options()) {</a>
<a name="ln1625">      options_-&gt;emplace(option.key(), option.value());</a>
<a name="ln1626">    }</a>
<a name="ln1627">  }</a>
<a name="ln1628">  user_cb_(new_status);</a>
<a name="ln1629">}</a>
<a name="ln1630"> </a>
<a name="ln1631">} // namespace internal</a>
<a name="ln1632"> </a>
<a name="ln1633">Status YBClient::Data::GetTableSchema(YBClient* client,</a>
<a name="ln1634">                                      const YBTableName&amp; table_name,</a>
<a name="ln1635">                                      CoarseTimePoint deadline,</a>
<a name="ln1636">                                      YBTableInfo* info) {</a>
<a name="ln1637">  Synchronizer sync;</a>
<a name="ln1638">  auto rpc = rpc::StartRpc&lt;GetTableSchemaRpc&gt;(</a>
<a name="ln1639">      client,</a>
<a name="ln1640">      sync.AsStatusCallback(),</a>
<a name="ln1641">      table_name,</a>
<a name="ln1642">      info,</a>
<a name="ln1643">      deadline,</a>
<a name="ln1644">      messenger_,</a>
<a name="ln1645">      proxy_cache_.get());</a>
<a name="ln1646">  return sync.Wait();</a>
<a name="ln1647">}</a>
<a name="ln1648"> </a>
<a name="ln1649">Status YBClient::Data::GetTableSchema(YBClient* client,</a>
<a name="ln1650">                                      const TableId&amp; table_id,</a>
<a name="ln1651">                                      CoarseTimePoint deadline,</a>
<a name="ln1652">                                      YBTableInfo* info) {</a>
<a name="ln1653">  Synchronizer sync;</a>
<a name="ln1654">  auto rpc = rpc::StartRpc&lt;GetTableSchemaRpc&gt;(</a>
<a name="ln1655">      client,</a>
<a name="ln1656">      sync.AsStatusCallback(),</a>
<a name="ln1657">      table_id,</a>
<a name="ln1658">      info,</a>
<a name="ln1659">      deadline,</a>
<a name="ln1660">      messenger_,</a>
<a name="ln1661">      proxy_cache_.get());</a>
<a name="ln1662">  return sync.Wait();</a>
<a name="ln1663">}</a>
<a name="ln1664"> </a>
<a name="ln1665">Status YBClient::Data::GetTableSchemaById(YBClient* client,</a>
<a name="ln1666">                                          const TableId&amp; table_id,</a>
<a name="ln1667">                                          CoarseTimePoint deadline,</a>
<a name="ln1668">                                          std::shared_ptr&lt;YBTableInfo&gt; info,</a>
<a name="ln1669">                                          StatusCallback callback) {</a>
<a name="ln1670">  auto rpc = rpc::StartRpc&lt;GetTableSchemaRpc&gt;(</a>
<a name="ln1671">      client,</a>
<a name="ln1672">      callback,</a>
<a name="ln1673">      table_id,</a>
<a name="ln1674">      info.get(),</a>
<a name="ln1675">      deadline,</a>
<a name="ln1676">      messenger_,</a>
<a name="ln1677">      proxy_cache_.get());</a>
<a name="ln1678">  return Status::OK();</a>
<a name="ln1679">}</a>
<a name="ln1680"> </a>
<a name="ln1681">Result&lt;IndexPermissions&gt; YBClient::Data::GetIndexPermissions(</a>
<a name="ln1682">    YBClient* client,</a>
<a name="ln1683">    const TableId&amp; table_id,</a>
<a name="ln1684">    const TableId&amp; index_id,</a>
<a name="ln1685">    const CoarseTimePoint deadline) {</a>
<a name="ln1686">  std::shared_ptr&lt;YBTableInfo&gt; yb_table_info = std::make_shared&lt;YBTableInfo&gt;();</a>
<a name="ln1687">  Synchronizer sync;</a>
<a name="ln1688"> </a>
<a name="ln1689">  RETURN_NOT_OK(GetTableSchemaById(client,</a>
<a name="ln1690">                                   table_id,</a>
<a name="ln1691">                                   deadline,</a>
<a name="ln1692">                                   yb_table_info,</a>
<a name="ln1693">                                   sync.AsStatusCallback()));</a>
<a name="ln1694">  Status s = sync.Wait();</a>
<a name="ln1695">  if (!s.ok()) {</a>
<a name="ln1696">    return s;</a>
<a name="ln1697">  }</a>
<a name="ln1698"> </a>
<a name="ln1699">  const IndexInfo* index_info =</a>
<a name="ln1700">      VERIFY_RESULT(yb_table_info-&gt;index_map.FindIndex(index_id));</a>
<a name="ln1701">  return index_info-&gt;index_permissions();</a>
<a name="ln1702">}</a>
<a name="ln1703"> </a>
<a name="ln1704">Result&lt;IndexPermissions&gt; YBClient::Data::WaitUntilIndexPermissionsAtLeast(</a>
<a name="ln1705">    YBClient* client,</a>
<a name="ln1706">    const TableId&amp; table_id,</a>
<a name="ln1707">    const TableId&amp; index_id,</a>
<a name="ln1708">    const CoarseTimePoint deadline,</a>
<a name="ln1709">    const IndexPermissions&amp; target_index_permissions) {</a>
<a name="ln1710">  RETURN_NOT_OK(RetryFunc(</a>
<a name="ln1711">      deadline,</a>
<a name="ln1712">      &quot;Waiting for index to have desired permissions&quot;,</a>
<a name="ln1713">      &quot;Timed out waiting for proper index permissions&quot;,</a>
<a name="ln1714">      [&amp;](CoarseTimePoint deadline, bool* retry) -&gt; Status {</a>
<a name="ln1715">        Result&lt;IndexPermissions&gt; result = GetIndexPermissions(client, table_id, index_id, deadline);</a>
<a name="ln1716">        // Treat NotFound as success if we are looking for INDEX_PERM_NOT_USED.</a>
<a name="ln1717">        if (!result &amp;&amp; result.status().IsNotFound() &amp;&amp;</a>
<a name="ln1718">            target_index_permissions == IndexPermissions::INDEX_PERM_NOT_USED) {</a>
<a name="ln1719">          LOG(INFO) &lt;&lt; &quot;Waiting to delete index &quot; &lt;&lt; index_id &lt;&lt; &quot; from table &quot; &lt;&lt; table_id</a>
<a name="ln1720">                    &lt;&lt; &quot; got &quot; &lt;&lt; result.ToString();</a>
<a name="ln1721">          *retry = false;</a>
<a name="ln1722">          return Status::OK();</a>
<a name="ln1723">        }</a>
<a name="ln1724">        IndexPermissions actual_index_permissions = VERIFY_RESULT(result);</a>
<a name="ln1725">        *retry = actual_index_permissions &lt; target_index_permissions;</a>
<a name="ln1726">        return Status::OK();</a>
<a name="ln1727">      }));</a>
<a name="ln1728">  // Now, the index permissions are guaranteed to be at (or beyond) the target.  Query again to</a>
<a name="ln1729">  // return it.</a>
<a name="ln1730">  return GetIndexPermissions(</a>
<a name="ln1731">      client,</a>
<a name="ln1732">      table_id,</a>
<a name="ln1733">      index_id,</a>
<a name="ln1734">      deadline);</a>
<a name="ln1735">}</a>
<a name="ln1736"> </a>
<a name="ln1737">void YBClient::Data::CreateCDCStream(YBClient* client,</a>
<a name="ln1738">                                     const TableId&amp; table_id,</a>
<a name="ln1739">                                     const std::unordered_map&lt;std::string, std::string&gt;&amp; options,</a>
<a name="ln1740">                                     CoarseTimePoint deadline,</a>
<a name="ln1741">                                     CreateCDCStreamCallback callback) {</a>
<a name="ln1742">  auto rpc = rpc::StartRpc&lt;internal::CreateCDCStreamRpc&gt;(</a>
<a name="ln1743">      client,</a>
<a name="ln1744">      callback,</a>
<a name="ln1745">      table_id,</a>
<a name="ln1746">      options,</a>
<a name="ln1747">      deadline,</a>
<a name="ln1748">      messenger_,</a>
<a name="ln1749">      proxy_cache_.get());</a>
<a name="ln1750">}</a>
<a name="ln1751"> </a>
<a name="ln1752">void YBClient::Data::DeleteCDCStream(YBClient* client,</a>
<a name="ln1753">                                     const CDCStreamId&amp; stream_id,</a>
<a name="ln1754">                                     CoarseTimePoint deadline,</a>
<a name="ln1755">                                     StatusCallback callback) {</a>
<a name="ln1756">  auto rpc = rpc::StartRpc&lt;internal::DeleteCDCStreamRpc&gt;(</a>
<a name="ln1757">      client,</a>
<a name="ln1758">      callback,</a>
<a name="ln1759">      stream_id,</a>
<a name="ln1760">      deadline,</a>
<a name="ln1761">      messenger_,</a>
<a name="ln1762">      proxy_cache_.get());</a>
<a name="ln1763">}</a>
<a name="ln1764"> </a>
<a name="ln1765">void YBClient::Data::GetCDCStream(</a>
<a name="ln1766">    YBClient* client,</a>
<a name="ln1767">    const CDCStreamId&amp; stream_id,</a>
<a name="ln1768">    std::shared_ptr&lt;TableId&gt; table_id,</a>
<a name="ln1769">    std::shared_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt; options,</a>
<a name="ln1770">    CoarseTimePoint deadline,</a>
<a name="ln1771">    StdStatusCallback callback) {</a>
<a name="ln1772">  auto rpc = rpc::StartRpc&lt;internal::GetCDCStreamRpc&gt;(</a>
<a name="ln1773">      client,</a>
<a name="ln1774">      callback,</a>
<a name="ln1775">      stream_id,</a>
<a name="ln1776">      table_id.get(),</a>
<a name="ln1777">      options.get(),</a>
<a name="ln1778">      deadline,</a>
<a name="ln1779">      messenger_,</a>
<a name="ln1780">      proxy_cache_.get());</a>
<a name="ln1781">}</a>
<a name="ln1782"> </a>
<a name="ln1783">void YBClient::Data::LeaderMasterDetermined(const Status&amp; status,</a>
<a name="ln1784">                                            const HostPort&amp; host_port) {</a>
<a name="ln1785">  Status new_status = status;</a>
<a name="ln1786">  VLOG(4) &lt;&lt; &quot;YBClient: Leader master determined: status=&quot;</a>
<a name="ln1787">          &lt;&lt; status.ToString() &lt;&lt; &quot;, host port =&quot;</a>
<a name="ln1788">          &lt;&lt; host_port.ToString();</a>
<a name="ln1789">  std::vector&lt;StatusCallback&gt; cbs;</a>
<a name="ln1790">  {</a>
<a name="ln1791">    std::lock_guard&lt;simple_spinlock&gt; l(leader_master_lock_);</a>
<a name="ln1792">    cbs.swap(leader_master_callbacks_);</a>
<a name="ln1793"> </a>
<a name="ln1794">    if (new_status.ok()) {</a>
<a name="ln1795">      leader_master_hostport_ = host_port;</a>
<a name="ln1796">      master_proxy_.reset(new MasterServiceProxy(proxy_cache_.get(), host_port));</a>
<a name="ln1797">    }</a>
<a name="ln1798"> </a>
<a name="ln1799">    rpcs_.Unregister(&amp;leader_master_rpc_);</a>
<a name="ln1800">  }</a>
<a name="ln1801"> </a>
<a name="ln1802">  for (const StatusCallback&amp; cb : cbs) {</a>
<a name="ln1803">    cb.Run(new_status);</a>
<a name="ln1804">  }</a>
<a name="ln1805">}</a>
<a name="ln1806"> </a>
<a name="ln1807">Status YBClient::Data::SetMasterServerProxy(CoarseTimePoint deadline,</a>
<a name="ln1808">                                            bool skip_resolution,</a>
<a name="ln1809">                                            bool wait_for_leader_election) {</a>
<a name="ln1810"> </a>
<a name="ln1811">  Synchronizer sync;</a>
<a name="ln1812">  SetMasterServerProxyAsync(deadline, skip_resolution,</a>
<a name="ln1813">      wait_for_leader_election, sync.AsStatusCallback());</a>
<a name="ln1814">  return sync.Wait();</a>
<a name="ln1815">}</a>
<a name="ln1816"> </a>
<a name="ln1817">void YBClient::Data::SetMasterServerProxyAsync(CoarseTimePoint deadline,</a>
<a name="ln1818">                                               bool skip_resolution,</a>
<a name="ln1819">                                               bool wait_for_leader_election,</a>
<a name="ln1820">                                               const StatusCallback&amp; cb) {</a>
<a name="ln1821">  DCHECK(deadline != CoarseTimePoint::max());</a>
<a name="ln1822"> </a>
<a name="ln1823">  server::MasterAddresses master_addrs;</a>
<a name="ln1824">  // Refresh the value of 'master_server_addrs_' if needed.</a>
<a name="ln1825">  Status s = ReinitializeMasterAddresses();</a>
<a name="ln1826">  {</a>
<a name="ln1827">    std::lock_guard&lt;simple_spinlock&gt; l(master_server_addrs_lock_);</a>
<a name="ln1828">    if (!s.ok() &amp;&amp; full_master_server_addrs_.empty()) {</a>
<a name="ln1829">      cb.Run(s);</a>
<a name="ln1830">      return;</a>
<a name="ln1831">    }</a>
<a name="ln1832">    for (const string &amp;master_server_addr : full_master_server_addrs_) {</a>
<a name="ln1833">      std::vector&lt;HostPort&gt; addrs;</a>
<a name="ln1834">      // TODO: Do address resolution asynchronously as well.</a>
<a name="ln1835">      s = HostPort::ParseStrings(master_server_addr, master::kMasterDefaultPort, &amp;addrs);</a>
<a name="ln1836">      if (!s.ok()) {</a>
<a name="ln1837">        cb.Run(s);</a>
<a name="ln1838">        return;</a>
<a name="ln1839">      }</a>
<a name="ln1840">      if (addrs.empty()) {</a>
<a name="ln1841">        cb.Run(STATUS_FORMAT(</a>
<a name="ln1842">            InvalidArgument,</a>
<a name="ln1843">            &quot;No master address specified by '$0' (all master server addresses: $1)&quot;,</a>
<a name="ln1844">            master_server_addr, full_master_server_addrs_));</a>
<a name="ln1845">        return;</a>
<a name="ln1846">      }</a>
<a name="ln1847"> </a>
<a name="ln1848">      master_addrs.push_back(std::move(addrs));</a>
<a name="ln1849">    }</a>
<a name="ln1850">  }</a>
<a name="ln1851"> </a>
<a name="ln1852">  // Finding a new master involves a fan-out RPC to each master. A single</a>
<a name="ln1853">  // RPC timeout's worth of time should be sufficient, though we'll use</a>
<a name="ln1854">  // the provided deadline if it's sooner.</a>
<a name="ln1855">  auto leader_master_deadline = CoarseMonoClock::Now() + default_rpc_timeout_;</a>
<a name="ln1856">  auto actual_deadline = std::min(deadline, leader_master_deadline);</a>
<a name="ln1857"> </a>
<a name="ln1858">  // This ensures that no more than one GetLeaderMasterRpc is in</a>
<a name="ln1859">  // flight at a time -- there isn't much sense in requesting this information</a>
<a name="ln1860">  // in parallel, since the requests should end up with the same result.</a>
<a name="ln1861">  // Instead, we simply piggy-back onto the existing request by adding our own</a>
<a name="ln1862">  // callback to leader_master_callbacks_.</a>
<a name="ln1863">  std::unique_lock&lt;simple_spinlock&gt; l(leader_master_lock_);</a>
<a name="ln1864">  leader_master_callbacks_.push_back(cb);</a>
<a name="ln1865">  if (skip_resolution &amp;&amp; !master_addrs.empty() &amp;&amp; !master_addrs.front().empty()) {</a>
<a name="ln1866">    l.unlock();</a>
<a name="ln1867">    LeaderMasterDetermined(Status::OK(), master_addrs.front().front());</a>
<a name="ln1868">    return;</a>
<a name="ln1869">  }</a>
<a name="ln1870">  if (leader_master_rpc_ == rpcs_.InvalidHandle()) {</a>
<a name="ln1871">    // No one is sending a request yet - we need to be the one to do it.</a>
<a name="ln1872">    rpcs_.Register(</a>
<a name="ln1873">        std::make_shared&lt;GetLeaderMasterRpc&gt;(</a>
<a name="ln1874">            Bind(&amp;YBClient::Data::LeaderMasterDetermined, Unretained(this)),</a>
<a name="ln1875">            master_addrs,</a>
<a name="ln1876">            actual_deadline,</a>
<a name="ln1877">            messenger_,</a>
<a name="ln1878">            proxy_cache_.get(),</a>
<a name="ln1879">            &amp;rpcs_,</a>
<a name="ln1880">            false /*should timeout to follower*/,</a>
<a name="ln1881">            wait_for_leader_election),</a>
<a name="ln1882">        &amp;leader_master_rpc_);</a>
<a name="ln1883">    l.unlock();</a>
<a name="ln1884">    (**leader_master_rpc_).SendRpc();</a>
<a name="ln1885">  }</a>
<a name="ln1886">}</a>
<a name="ln1887"> </a>
<a name="ln1888">// API to clear and reset master addresses, used during master config change.</a>
<a name="ln1889">Status YBClient::Data::SetMasterAddresses(const string&amp; addrs) {</a>
<a name="ln1890">  std::lock_guard&lt;simple_spinlock&gt; l(master_server_addrs_lock_);</a>
<a name="ln1891">  if (addrs.empty()) {</a>
<a name="ln1892">    std::ostringstream out;</a>
<a name="ln1893">    out.str(&quot;Invalid empty master address cannot be set. Current list is: &quot;);</a>
<a name="ln1894">    for (const string&amp; master_server_addr : master_server_addrs_) {</a>
<a name="ln1895">      out.str(master_server_addr);</a>
<a name="ln1896">      out.str(&quot; &quot;);</a>
<a name="ln1897">    }</a>
<a name="ln1898">    LOG(ERROR) &lt;&lt; out.str();</a>
<a name="ln1899">    return STATUS(InvalidArgument, &quot;master addresses cannot be empty&quot;);</a>
<a name="ln1900">  }</a>
<a name="ln1901"> </a>
<a name="ln1902">  master_server_addrs_.clear();</a>
<a name="ln1903">  master_server_addrs_.push_back(addrs);</a>
<a name="ln1904"> </a>
<a name="ln1905">  return Status::OK();</a>
<a name="ln1906">}</a>
<a name="ln1907"> </a>
<a name="ln1908">// Add a given master to the master address list.</a>
<a name="ln1909">Status YBClient::Data::AddMasterAddress(const HostPort&amp; addr) {</a>
<a name="ln1910">  std::lock_guard&lt;simple_spinlock&gt; l(master_server_addrs_lock_);</a>
<a name="ln1911">  master_server_addrs_.push_back(addr.ToString());</a>
<a name="ln1912">  return Status::OK();</a>
<a name="ln1913">}</a>
<a name="ln1914"> </a>
<a name="ln1915">namespace {</a>
<a name="ln1916"> </a>
<a name="ln1917">Result&lt;std::string&gt; ReadMasterAddressesFromFlagFile(</a>
<a name="ln1918">    const std::string&amp; flag_file_path, const std::string&amp; flag_name) {</a>
<a name="ln1919">  std::ifstream input_file(flag_file_path);</a>
<a name="ln1920">  if (!input_file) {</a>
<a name="ln1921">    return STATUS_FORMAT(IOError, &quot;Unable to open flag file '$0': $1&quot;,</a>
<a name="ln1922">        flag_file_path, strerror(errno));</a>
<a name="ln1923">  }</a>
<a name="ln1924">  std::string line;</a>
<a name="ln1925"> </a>
<a name="ln1926">  std::string master_addrs;</a>
<a name="ln1927">  while (input_file.good() &amp;&amp; std::getline(input_file, line)) {</a>
<a name="ln1928">    const std::string flag_prefix = &quot;--&quot; + flag_name + &quot;=&quot;;</a>
<a name="ln1929">    if (boost::starts_with(line, flag_prefix)) {</a>
<a name="ln1930">      master_addrs = line.c_str() + flag_prefix.size();</a>
<a name="ln1931">    }</a>
<a name="ln1932">  }</a>
<a name="ln1933"> </a>
<a name="ln1934">  if (input_file.bad()) {</a>
<a name="ln1935">    // Do not check input_file.fail() here, reaching EOF may set that.</a>
<a name="ln1936">    return STATUS_FORMAT(IOError, &quot;Failed reading flag file '$0': $1&quot;,</a>
<a name="ln1937">        flag_file_path, strerror(errno));</a>
<a name="ln1938">  }</a>
<a name="ln1939">  return master_addrs;</a>
<a name="ln1940">}</a>
<a name="ln1941"> </a>
<a name="ln1942">} // anonymous namespace</a>
<a name="ln1943"> </a>
<a name="ln1944">// Read the master addresses (from a remote endpoint or a file depending on which is specified), and</a>
<a name="ln1945">// re-initialize the 'master_server_addrs_' variable.</a>
<a name="ln1946">Status YBClient::Data::ReinitializeMasterAddresses() {</a>
<a name="ln1947">  Status result;</a>
<a name="ln1948">  std::lock_guard&lt;simple_spinlock&gt; l(master_server_addrs_lock_);</a>
<a name="ln1949">  if (!master_server_endpoint_.empty()) {</a>
<a name="ln1950">    faststring buf;</a>
<a name="ln1951">    result = EasyCurl().FetchURL(master_server_endpoint_, &amp;buf);</a>
<a name="ln1952">    if (result.ok()) {</a>
<a name="ln1953">      // The JSON serialization adds a &quot; character to the beginning and end of the response, remove</a>
<a name="ln1954">      // those if they are present.</a>
<a name="ln1955">      std::string master_addrs = buf.ToString();</a>
<a name="ln1956">      if (master_addrs.at(0) == '&quot;') {</a>
<a name="ln1957">        master_addrs = master_addrs.erase(0, 1);</a>
<a name="ln1958">      }</a>
<a name="ln1959">      if (master_addrs.at(master_addrs.size() - 1) == '&quot;') {</a>
<a name="ln1960">        master_addrs = master_addrs.erase(master_addrs.size() - 1, 1);</a>
<a name="ln1961">      }</a>
<a name="ln1962">      master_server_addrs_.clear();</a>
<a name="ln1963">      master_server_addrs_.push_back(master_addrs);</a>
<a name="ln1964">      LOG(INFO) &lt;&lt; &quot;Got master addresses = &quot; &lt;&lt; master_addrs</a>
<a name="ln1965">                &lt;&lt; &quot; from REST endpoint: &quot; &lt;&lt; master_server_endpoint_;</a>
<a name="ln1966">    }</a>
<a name="ln1967">  } else if (!FLAGS_flagfile.empty() &amp;&amp; !skip_master_flagfile_) {</a>
<a name="ln1968">    LOG(INFO) &lt;&lt; &quot;Reinitialize master addresses from file: &quot; &lt;&lt; FLAGS_flagfile;</a>
<a name="ln1969">    auto master_addrs = ReadMasterAddressesFromFlagFile(</a>
<a name="ln1970">        FLAGS_flagfile, master_address_flag_name_);</a>
<a name="ln1971"> </a>
<a name="ln1972">    if (!master_addrs.ok()) {</a>
<a name="ln1973">      LOG(WARNING) &lt;&lt; &quot;Failure reading flagfile &quot; &lt;&lt; FLAGS_flagfile &lt;&lt; &quot;: &quot;</a>
<a name="ln1974">                   &lt;&lt; master_addrs.status();</a>
<a name="ln1975">      result = master_addrs.status();</a>
<a name="ln1976">    } else if (master_addrs-&gt;empty()) {</a>
<a name="ln1977">      LOG(WARNING) &lt;&lt; &quot;Couldn't find flag &quot; &lt;&lt; master_address_flag_name_ &lt;&lt; &quot; in flagfile &quot;</a>
<a name="ln1978">                   &lt;&lt; FLAGS_flagfile;</a>
<a name="ln1979">    } else {</a>
<a name="ln1980">      master_server_addrs_.clear();</a>
<a name="ln1981">      master_server_addrs_.push_back(*master_addrs);</a>
<a name="ln1982">    }</a>
<a name="ln1983">  } else {</a>
<a name="ln1984">    VLOG(1) &lt;&lt; &quot;Skipping reinitialize of master addresses, no REST endpoint or file specified&quot;;</a>
<a name="ln1985">  }</a>
<a name="ln1986">  full_master_server_addrs_.clear();</a>
<a name="ln1987">  for (const auto&amp; address : master_server_addrs_) {</a>
<a name="ln1988">    if (!address.empty()) {</a>
<a name="ln1989">      full_master_server_addrs_.push_back(address);</a>
<a name="ln1990">    }</a>
<a name="ln1991">  }</a>
<a name="ln1992">  for (const auto&amp; source : master_address_sources_) {</a>
<a name="ln1993">    auto current = source();</a>
<a name="ln1994">    full_master_server_addrs_.insert(</a>
<a name="ln1995">        full_master_server_addrs_.end(), current.begin(), current.end());</a>
<a name="ln1996">  }</a>
<a name="ln1997">  LOG(INFO) &lt;&lt; &quot;New master addresses: &quot; &lt;&lt; AsString(full_master_server_addrs_);</a>
<a name="ln1998"> </a>
<a name="ln1999">  if (full_master_server_addrs_.empty()) {</a>
<a name="ln2000">    return result.ok() ? STATUS(IllegalState, &quot;Unable to determine master addresses&quot;) : result;</a>
<a name="ln2001">  }</a>
<a name="ln2002">  return Status::OK();</a>
<a name="ln2003">}</a>
<a name="ln2004"> </a>
<a name="ln2005">// Remove a given master from the list of master_server_addrs_.</a>
<a name="ln2006">Status YBClient::Data::RemoveMasterAddress(const HostPort&amp; addr) {</a>
<a name="ln2007"> </a>
<a name="ln2008">  {</a>
<a name="ln2009">    auto str = addr.ToString();</a>
<a name="ln2010">    std::lock_guard&lt;simple_spinlock&gt; l(master_server_addrs_lock_);</a>
<a name="ln2011">    auto it = std::find(master_server_addrs_.begin(), master_server_addrs_.end(), str);</a>
<a name="ln2012">    if (it != master_server_addrs_.end()) {</a>
<a name="ln2013">      master_server_addrs_.erase(it, it + str.size());</a>
<a name="ln2014">    }</a>
<a name="ln2015">  }</a>
<a name="ln2016"> </a>
<a name="ln2017">  return Status::OK();</a>
<a name="ln2018">}</a>
<a name="ln2019"> </a>
<a name="ln2020">Status YBClient::Data::SetReplicationInfo(</a>
<a name="ln2021">    YBClient* client, const master::ReplicationInfoPB&amp; replication_info, CoarseTimePoint deadline,</a>
<a name="ln2022">    bool* retry) {</a>
<a name="ln2023">  // If retry was not set, we'll wrap around in a retryable function.</a>
<a name="ln2024">  if (!retry) {</a>
<a name="ln2025">    return RetryFunc(</a>
<a name="ln2026">        deadline, &quot;Other clients changed the config. Retrying.&quot;,</a>
<a name="ln2027">        &quot;Timed out retrying the config change. Probably too many concurrent attempts.&quot;,</a>
<a name="ln2028">        std::bind(&amp;YBClient::Data::SetReplicationInfo, this, client, replication_info, _1, _2));</a>
<a name="ln2029">  }</a>
<a name="ln2030"> </a>
<a name="ln2031">  // Get the current config.</a>
<a name="ln2032">  GetMasterClusterConfigRequestPB get_req;</a>
<a name="ln2033">  GetMasterClusterConfigResponsePB get_resp;</a>
<a name="ln2034">  Status s = SyncLeaderMasterRpc&lt;GetMasterClusterConfigRequestPB, GetMasterClusterConfigResponsePB&gt;(</a>
<a name="ln2035">      deadline, get_req, &amp;get_resp, nullptr /* num_attempts */, &quot;GetMasterClusterConfig&quot;,</a>
<a name="ln2036">      &amp;MasterServiceProxy::GetMasterClusterConfig);</a>
<a name="ln2037">  RETURN_NOT_OK(s);</a>
<a name="ln2038">  if (get_resp.has_error()) {</a>
<a name="ln2039">    return StatusFromPB(get_resp.error().status());</a>
<a name="ln2040">  }</a>
<a name="ln2041"> </a>
<a name="ln2042">  ChangeMasterClusterConfigRequestPB change_req;</a>
<a name="ln2043">  ChangeMasterClusterConfigResponsePB change_resp;</a>
<a name="ln2044"> </a>
<a name="ln2045">  // Update the list with the new replication info.</a>
<a name="ln2046">  change_req.mutable_cluster_config()-&gt;CopyFrom(get_resp.cluster_config());</a>
<a name="ln2047">  auto new_ri = change_req.mutable_cluster_config()-&gt;mutable_replication_info();</a>
<a name="ln2048">  new_ri-&gt;CopyFrom(replication_info);</a>
<a name="ln2049"> </a>
<a name="ln2050">  // Try to update it on the live cluster.</a>
<a name="ln2051">  s = SyncLeaderMasterRpc&lt;ChangeMasterClusterConfigRequestPB, ChangeMasterClusterConfigResponsePB&gt;(</a>
<a name="ln2052">      deadline, change_req, &amp;change_resp, nullptr /* num_attempts */,</a>
<a name="ln2053">      &quot;ChangeMasterClusterConfig&quot;, &amp;MasterServiceProxy::ChangeMasterClusterConfig);</a>
<a name="ln2054">  RETURN_NOT_OK(s);</a>
<a name="ln2055">  if (change_resp.has_error()) {</a>
<a name="ln2056">    // Retry on config mismatch.</a>
<a name="ln2057">    *retry = change_resp.error().code() == MasterErrorPB::CONFIG_VERSION_MISMATCH;</a>
<a name="ln2058">    return StatusFromPB(change_resp.error().status());</a>
<a name="ln2059">  }</a>
<a name="ln2060">  *retry = false;</a>
<a name="ln2061">  return Status::OK();</a>
<a name="ln2062">}</a>
<a name="ln2063"> </a>
<a name="ln2064">HostPort YBClient::Data::leader_master_hostport() const {</a>
<a name="ln2065">  std::lock_guard&lt;simple_spinlock&gt; l(leader_master_lock_);</a>
<a name="ln2066">  return leader_master_hostport_;</a>
<a name="ln2067">}</a>
<a name="ln2068"> </a>
<a name="ln2069">shared_ptr&lt;master::MasterServiceProxy&gt; YBClient::Data::master_proxy() const {</a>
<a name="ln2070">  std::lock_guard&lt;simple_spinlock&gt; l(leader_master_lock_);</a>
<a name="ln2071">  return master_proxy_;</a>
<a name="ln2072">}</a>
<a name="ln2073"> </a>
<a name="ln2074">uint64_t YBClient::Data::GetLatestObservedHybridTime() const {</a>
<a name="ln2075">  return latest_observed_hybrid_time_.Load();</a>
<a name="ln2076">}</a>
<a name="ln2077"> </a>
<a name="ln2078">void YBClient::Data::UpdateLatestObservedHybridTime(uint64_t hybrid_time) {</a>
<a name="ln2079">  latest_observed_hybrid_time_.StoreMax(hybrid_time);</a>
<a name="ln2080">}</a>
<a name="ln2081"> </a>
<a name="ln2082">void YBClient::Data::StartShutdown() {</a>
<a name="ln2083">  closing_.store(true, std::memory_order_release);</a>
<a name="ln2084">}</a>
<a name="ln2085"> </a>
<a name="ln2086">bool YBClient::Data::IsMultiMaster() {</a>
<a name="ln2087">  std::lock_guard&lt;simple_spinlock&gt; l(master_server_addrs_lock_);</a>
<a name="ln2088">  if (full_master_server_addrs_.size() &gt; 1) {</a>
<a name="ln2089">    return true;</a>
<a name="ln2090">  }</a>
<a name="ln2091">  // For single entry case, check if it is a list of host/ports.</a>
<a name="ln2092">  std::vector&lt;Endpoint&gt; addrs;</a>
<a name="ln2093">  const auto status = ParseAddressList(full_master_server_addrs_[0],</a>
<a name="ln2094">                                       yb::master::kMasterDefaultPort,</a>
<a name="ln2095">                                       &amp;addrs);</a>
<a name="ln2096">  return status.ok() &amp;&amp; (addrs.size() &gt; 1);</a>
<a name="ln2097">}</a>
<a name="ln2098"> </a>
<a name="ln2099">void YBClient::Data::CompleteShutdown() {</a>
<a name="ln2100">  while (running_sync_requests_.load(std::memory_order_acquire)) {</a>
<a name="ln2101">    YB_LOG_EVERY_N_SECS(INFO, 5) &lt;&lt; &quot;Waiting sync requests to finish&quot;;</a>
<a name="ln2102">    std::this_thread::sleep_for(100ms);</a>
<a name="ln2103">  }</a>
<a name="ln2104">}</a>
<a name="ln2105"> </a>
<a name="ln2106">} // namespace client</a>
<a name="ln2107">} // namespace yb</a>

</code></pre>
<div class="balloon" rel="118"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="133"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="143"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="361"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="469"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="488"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="610"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1043"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1071"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1233"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1342"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1786"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1821"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1984"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
