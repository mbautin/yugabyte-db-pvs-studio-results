
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII" />
  <title>log.cc</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">// Licensed to the Apache Software Foundation (ASF) under one</a>
<a name="ln2">// or more contributor license agreements.  See the NOTICE file</a>
<a name="ln3">// distributed with this work for additional information</a>
<a name="ln4">// regarding copyright ownership.  The ASF licenses this file</a>
<a name="ln5">// to you under the Apache License, Version 2.0 (the</a>
<a name="ln6">// &quot;License&quot;); you may not use this file except in compliance</a>
<a name="ln7">// with the License.  You may obtain a copy of the License at</a>
<a name="ln8">//</a>
<a name="ln9">//   http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln10">//</a>
<a name="ln11">// Unless required by applicable law or agreed to in writing,</a>
<a name="ln12">// software distributed under the License is distributed on an</a>
<a name="ln13">// &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</a>
<a name="ln14">// KIND, either express or implied.  See the License for the</a>
<a name="ln15">// specific language governing permissions and limitations</a>
<a name="ln16">// under the License.</a>
<a name="ln17">//</a>
<a name="ln18">// The following only applies to changes made to this file as part of YugaByte development.</a>
<a name="ln19">//</a>
<a name="ln20">// Portions Copyright (c) YugaByte, Inc.</a>
<a name="ln21">//</a>
<a name="ln22">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except</a>
<a name="ln23">// in compliance with the License.  You may obtain a copy of the License at</a>
<a name="ln24">//</a>
<a name="ln25">// http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln26">//</a>
<a name="ln27">// Unless required by applicable law or agreed to in writing, software distributed under the License</a>
<a name="ln28">// is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express</a>
<a name="ln29">// or implied.  See the License for the specific language governing permissions and limitations</a>
<a name="ln30">// under the License.</a>
<a name="ln31">//</a>
<a name="ln32"> </a>
<a name="ln33">#include &quot;yb/consensus/log.h&quot;</a>
<a name="ln34"> </a>
<a name="ln35">#include &lt;algorithm&gt;</a>
<a name="ln36">#include &lt;mutex&gt;</a>
<a name="ln37">#include &lt;thread&gt;</a>
<a name="ln38"> </a>
<a name="ln39">#include &lt;boost/thread/shared_mutex.hpp&gt;</a>
<a name="ln40"> </a>
<a name="ln41">#include &quot;yb/common/wire_protocol.h&quot;</a>
<a name="ln42"> </a>
<a name="ln43">#include &quot;yb/consensus/consensus_util.h&quot;</a>
<a name="ln44">#include &quot;yb/consensus/log_index.h&quot;</a>
<a name="ln45">#include &quot;yb/consensus/log_metrics.h&quot;</a>
<a name="ln46">#include &quot;yb/consensus/log_reader.h&quot;</a>
<a name="ln47">#include &quot;yb/consensus/log_util.h&quot;</a>
<a name="ln48">#include &quot;yb/consensus/opid_util.h&quot;</a>
<a name="ln49"> </a>
<a name="ln50">#include &quot;yb/fs/fs_manager.h&quot;</a>
<a name="ln51">#include &quot;yb/gutil/map-util.h&quot;</a>
<a name="ln52">#include &quot;yb/gutil/ref_counted.h&quot;</a>
<a name="ln53">#include &quot;yb/gutil/stl_util.h&quot;</a>
<a name="ln54">#include &quot;yb/gutil/strings/substitute.h&quot;</a>
<a name="ln55">#include &quot;yb/gutil/walltime.h&quot;</a>
<a name="ln56">#include &quot;yb/util/coding.h&quot;</a>
<a name="ln57">#include &quot;yb/util/countdown_latch.h&quot;</a>
<a name="ln58">#include &quot;yb/util/debug/trace_event.h&quot;</a>
<a name="ln59">#include &quot;yb/util/env_util.h&quot;</a>
<a name="ln60">#include &quot;yb/util/fault_injection.h&quot;</a>
<a name="ln61">#include &quot;yb/util/file_util.h&quot;</a>
<a name="ln62">#include &quot;yb/util/flag_tags.h&quot;</a>
<a name="ln63">#include &quot;yb/util/logging.h&quot;</a>
<a name="ln64">#include &quot;yb/util/debug/long_operation_tracker.h&quot;</a>
<a name="ln65">#include &quot;yb/util/metrics.h&quot;</a>
<a name="ln66">#include &quot;yb/util/opid.h&quot;</a>
<a name="ln67">#include &quot;yb/util/path_util.h&quot;</a>
<a name="ln68">#include &quot;yb/util/pb_util.h&quot;</a>
<a name="ln69">#include &quot;yb/util/random.h&quot;</a>
<a name="ln70">#include &quot;yb/util/scope_exit.h&quot;</a>
<a name="ln71">#include &quot;yb/util/size_literals.h&quot;</a>
<a name="ln72">#include &quot;yb/util/stopwatch.h&quot;</a>
<a name="ln73">#include &quot;yb/util/taskstream.h&quot;</a>
<a name="ln74">#include &quot;yb/util/thread.h&quot;</a>
<a name="ln75">#include &quot;yb/util/threadpool.h&quot;</a>
<a name="ln76">#include &quot;yb/util/trace.h&quot;</a>
<a name="ln77">#include &quot;yb/util/tsan_util.h&quot;</a>
<a name="ln78">#include &quot;yb/util/shared_lock.h&quot;</a>
<a name="ln79"> </a>
<a name="ln80">using namespace yb::size_literals;  // NOLINT.</a>
<a name="ln81">using namespace std::literals;  // NOLINT.</a>
<a name="ln82">using namespace std::placeholders;</a>
<a name="ln83"> </a>
<a name="ln84">// Log retention configuration.</a>
<a name="ln85">// -----------------------------</a>
<a name="ln86">DEFINE_int32(log_min_segments_to_retain, 2,</a>
<a name="ln87">             &quot;The minimum number of past log segments to keep at all times,&quot;</a>
<a name="ln88">             &quot; regardless of what is required for durability. &quot;</a>
<a name="ln89">             &quot;Must be at least 1.&quot;);</a>
<a name="ln90">TAG_FLAG(log_min_segments_to_retain, runtime);</a>
<a name="ln91">TAG_FLAG(log_min_segments_to_retain, advanced);</a>
<a name="ln92"> </a>
<a name="ln93">DEFINE_int32(log_min_seconds_to_retain, 900,</a>
<a name="ln94">             &quot;The minimum number of seconds for which to keep log segments to keep at all times, &quot;</a>
<a name="ln95">             &quot;regardless of what is required for durability. Logs may be still retained for &quot;</a>
<a name="ln96">             &quot;a longer amount of time if they are necessary for correct restart. This should be &quot;</a>
<a name="ln97">             &quot;set long enough such that a tablet server which has temporarily failed can be &quot;</a>
<a name="ln98">             &quot;restarted within the given time period. If a server is down for longer than this &quot;</a>
<a name="ln99">             &quot;amount of time, it is possible that its tablets will be re-replicated on other &quot;</a>
<a name="ln100">             &quot;machines.&quot;);</a>
<a name="ln101">TAG_FLAG(log_min_seconds_to_retain, runtime);</a>
<a name="ln102">TAG_FLAG(log_min_seconds_to_retain, advanced);</a>
<a name="ln103"> </a>
<a name="ln104">// Flags for controlling kernel watchdog limits.</a>
<a name="ln105">DEFINE_int32(consensus_log_scoped_watch_delay_callback_threshold_ms, 1000,</a>
<a name="ln106">             &quot;If calling consensus log callback(s) take longer than this, the kernel watchdog &quot;</a>
<a name="ln107">             &quot;will print out a stack trace.&quot;);</a>
<a name="ln108">TAG_FLAG(consensus_log_scoped_watch_delay_callback_threshold_ms, runtime);</a>
<a name="ln109">TAG_FLAG(consensus_log_scoped_watch_delay_callback_threshold_ms, advanced);</a>
<a name="ln110">DEFINE_int32(consensus_log_scoped_watch_delay_append_threshold_ms, 1000,</a>
<a name="ln111">             &quot;If consensus log append takes longer than this, the kernel watchdog &quot;</a>
<a name="ln112">             &quot;will print out a stack trace.&quot;);</a>
<a name="ln113">TAG_FLAG(consensus_log_scoped_watch_delay_append_threshold_ms, runtime);</a>
<a name="ln114">TAG_FLAG(consensus_log_scoped_watch_delay_append_threshold_ms, advanced);</a>
<a name="ln115"> </a>
<a name="ln116">// Fault/latency injection flags.</a>
<a name="ln117">// -----------------------------</a>
<a name="ln118">DEFINE_bool(log_inject_latency, false,</a>
<a name="ln119">            &quot;If true, injects artificial latency in log sync operations. &quot;</a>
<a name="ln120">            &quot;Advanced option. Use at your own risk -- has a negative effect &quot;</a>
<a name="ln121">            &quot;on performance for obvious reasons!&quot;);</a>
<a name="ln122">DEFINE_int32(log_inject_latency_ms_mean, 100,</a>
<a name="ln123">             &quot;The number of milliseconds of latency to inject, on average. &quot;</a>
<a name="ln124">             &quot;Only takes effect if --log_inject_latency is true&quot;);</a>
<a name="ln125">DEFINE_int32(log_inject_latency_ms_stddev, 100,</a>
<a name="ln126">             &quot;The standard deviation of latency to inject in before log sync operations. &quot;</a>
<a name="ln127">             &quot;Only takes effect if --log_inject_latency is true&quot;);</a>
<a name="ln128">TAG_FLAG(log_inject_latency, unsafe);</a>
<a name="ln129">TAG_FLAG(log_inject_latency_ms_mean, unsafe);</a>
<a name="ln130">TAG_FLAG(log_inject_latency_ms_stddev, unsafe);</a>
<a name="ln131"> </a>
<a name="ln132">DEFINE_int32(log_inject_append_latency_ms_max, 0,</a>
<a name="ln133">             &quot;The maximum latency to inject before the log append operation.&quot;);</a>
<a name="ln134"> </a>
<a name="ln135">DEFINE_test_flag(bool, log_consider_all_ops_safe, false,</a>
<a name="ln136">            &quot;If true, we consider all operations to be safe and will not wait&quot;</a>
<a name="ln137">            &quot;for the opId to apply to the local log. i.e. WaitForSafeOpIdToApply &quot;</a>
<a name="ln138">            &quot;becomes a noop.&quot;);</a>
<a name="ln139"> </a>
<a name="ln140">// TaskStream flags.</a>
<a name="ln141">// We have to make the queue length really long.</a>
<a name="ln142">// TODO: Create new flags log_taskstream_queue_max_size and log_taskstream_queue_max_wait_ms</a>
<a name="ln143">// and deprecate these flags.</a>
<a name="ln144">DEFINE_int32(taskstream_queue_max_size, 100000,</a>
<a name="ln145">             &quot;Maximum number of operations waiting in the taskstream queue.&quot;);</a>
<a name="ln146"> </a>
<a name="ln147">DEFINE_int32(taskstream_queue_max_wait_ms, 1000,</a>
<a name="ln148">             &quot;Maximum time in ms to wait for items in the taskstream queue to arrive.&quot;);</a>
<a name="ln149"> </a>
<a name="ln150">DEFINE_int32(wait_for_safe_op_id_to_apply_default_timeout_ms, 15000 * yb::kTimeMultiplier,</a>
<a name="ln151">             &quot;Timeout used by WaitForSafeOpIdToApply when it was not specified by caller.&quot;);</a>
<a name="ln152"> </a>
<a name="ln153">// Validate that log_min_segments_to_retain &gt;= 1</a>
<a name="ln154">static bool ValidateLogsToRetain(const char* flagname, int value) {</a>
<a name="ln155">  if (value &gt;= 1) {</a>
<a name="ln156">    return true;</a>
<a name="ln157">  }</a>
<a name="ln158">  LOG(ERROR) &lt;&lt; strings::Substitute(&quot;$0 must be at least 1, value $1 is invalid&quot;,</a>
<a name="ln159">                                    flagname, value);</a>
<a name="ln160">  return false;</a>
<a name="ln161">}</a>
<a name="ln162">static bool dummy = google::RegisterFlagValidator(</a>
<a name="ln163">    &amp;FLAGS_log_min_segments_to_retain, &amp;ValidateLogsToRetain);</a>
<a name="ln164"> </a>
<a name="ln165">static const char kSegmentPlaceholderFileTemplate[] = &quot;.tmp.newsegmentXXXXXX&quot;;</a>
<a name="ln166"> </a>
<a name="ln167">namespace yb {</a>
<a name="ln168">namespace log {</a>
<a name="ln169"> </a>
<a name="ln170">using env_util::OpenFileForRandom;</a>
<a name="ln171">using std::shared_ptr;</a>
<a name="ln172">using std::unique_ptr;</a>
<a name="ln173">using strings::Substitute;</a>
<a name="ln174"> </a>
<a name="ln175">// This class represents a batch of operations to be written and synced to the log. It is opaque to</a>
<a name="ln176">// the user and is managed by the Log class.</a>
<a name="ln177">class LogEntryBatch {</a>
<a name="ln178"> public:</a>
<a name="ln179">  LogEntryBatch(LogEntryTypePB type, LogEntryBatchPB&amp;&amp; entry_batch_pb);</a>
<a name="ln180">  ~LogEntryBatch();</a>
<a name="ln181"> </a>
<a name="ln182">  std::string ToString() const {</a>
<a name="ln183">    return Format(&quot;{ type: $0 state: $1 max_op_id: $2 }&quot;, type_, state_, MaxReplicateOpId());</a>
<a name="ln184">  }</a>
<a name="ln185"> </a>
<a name="ln186">  bool HasReplicateEntries() const {</a>
<a name="ln187">    return type_ == LogEntryTypePB::REPLICATE &amp;&amp; count() &gt; 0;</a>
<a name="ln188">  }</a>
<a name="ln189"> </a>
<a name="ln190"> private:</a>
<a name="ln191">  friend class Log;</a>
<a name="ln192">  friend class MultiThreadedLogTest;</a>
<a name="ln193"> </a>
<a name="ln194">  // Serializes contents of the entry to an internal buffer.</a>
<a name="ln195">  CHECKED_STATUS Serialize();</a>
<a name="ln196"> </a>
<a name="ln197">  // Sets the callback that will be invoked after the entry is</a>
<a name="ln198">  // appended and synced to disk</a>
<a name="ln199">  void set_callback(const StatusCallback&amp; cb) {</a>
<a name="ln200">    callback_ = cb;</a>
<a name="ln201">  }</a>
<a name="ln202"> </a>
<a name="ln203">  // Returns the callback that will be invoked after the entry is</a>
<a name="ln204">  // appended and synced to disk.</a>
<a name="ln205">  const StatusCallback&amp; callback() {</a>
<a name="ln206">    return callback_;</a>
<a name="ln207">  }</a>
<a name="ln208"> </a>
<a name="ln209">  bool failed_to_append() const {</a>
<a name="ln210">    return state_ == kEntryFailedToAppend;</a>
<a name="ln211">  }</a>
<a name="ln212"> </a>
<a name="ln213">  void set_failed_to_append() {</a>
<a name="ln214">    state_ = kEntryFailedToAppend;</a>
<a name="ln215">  }</a>
<a name="ln216"> </a>
<a name="ln217">  // Mark the entry as reserved, but not yet ready to write to the log.</a>
<a name="ln218">  void MarkReserved();</a>
<a name="ln219"> </a>
<a name="ln220">  // Mark the entry as ready to write to log.</a>
<a name="ln221">  void MarkReady();</a>
<a name="ln222"> </a>
<a name="ln223">  // Returns a Slice representing the serialized contents of the entry.</a>
<a name="ln224">  Slice data() const {</a>
<a name="ln225">    DCHECK_EQ(state_, kEntrySerialized);</a>
<a name="ln226">    return Slice(buffer_);</a>
<a name="ln227">  }</a>
<a name="ln228"> </a>
<a name="ln229">  bool flush_marker() const;</a>
<a name="ln230"> </a>
<a name="ln231">  size_t count() const { return count_; }</a>
<a name="ln232"> </a>
<a name="ln233">  // Returns the total size in bytes of the object.</a>
<a name="ln234">  size_t total_size_bytes() const {</a>
<a name="ln235">    return total_size_bytes_;</a>
<a name="ln236">  }</a>
<a name="ln237"> </a>
<a name="ln238">  // The highest OpId of a REPLICATE message in this batch.</a>
<a name="ln239">  OpIdPB MaxReplicateOpId() const {</a>
<a name="ln240">    DCHECK_EQ(REPLICATE, type_);</a>
<a name="ln241">    int idx = entry_batch_pb_.entry_size() - 1;</a>
<a name="ln242">    DCHECK(entry_batch_pb_.entry(idx).replicate().IsInitialized());</a>
<a name="ln243">    return entry_batch_pb_.entry(idx).replicate().id();</a>
<a name="ln244">  }</a>
<a name="ln245"> </a>
<a name="ln246">  void SetReplicates(const ReplicateMsgs&amp; replicates) {</a>
<a name="ln247">    replicates_ = replicates;</a>
<a name="ln248">  }</a>
<a name="ln249"> </a>
<a name="ln250">  // The type of entries in this batch.</a>
<a name="ln251">  const LogEntryTypePB type_;</a>
<a name="ln252"> </a>
<a name="ln253">  // Contents of the log entries that will be written to disk.</a>
<a name="ln254">  LogEntryBatchPB entry_batch_pb_;</a>
<a name="ln255"> </a>
<a name="ln256">  // Total size in bytes of all entries</a>
<a name="ln257">  uint32_t total_size_bytes_ = 0;</a>
<a name="ln258"> </a>
<a name="ln259">  // Number of entries in 'entry_batch_pb_'</a>
<a name="ln260">  const size_t count_;</a>
<a name="ln261"> </a>
<a name="ln262">  // The vector of refcounted replicates.  This makes sure there's at least a reference to each</a>
<a name="ln263">  // replicate message until we're finished appending.</a>
<a name="ln264">  ReplicateMsgs replicates_;</a>
<a name="ln265"> </a>
<a name="ln266">  // Callback to be invoked upon the entries being written and synced to disk.</a>
<a name="ln267">  StatusCallback callback_;</a>
<a name="ln268"> </a>
<a name="ln269">  // Buffer to which 'phys_entries_' are serialized by call to 'Serialize()'</a>
<a name="ln270">  faststring buffer_;</a>
<a name="ln271"> </a>
<a name="ln272">  // Offset into the log file for this entry batch.</a>
<a name="ln273">  int64_t offset_;</a>
<a name="ln274"> </a>
<a name="ln275">  // Segment sequence number for this entry batch.</a>
<a name="ln276">  uint64_t active_segment_sequence_number_;</a>
<a name="ln277"> </a>
<a name="ln278">  enum LogEntryState {</a>
<a name="ln279">    kEntryInitialized,</a>
<a name="ln280">    kEntryReserved,</a>
<a name="ln281">    kEntryReady,</a>
<a name="ln282">    kEntrySerialized,</a>
<a name="ln283">    kEntryFailedToAppend</a>
<a name="ln284">  };</a>
<a name="ln285">  LogEntryState state_ = kEntryInitialized;</a>
<a name="ln286"> </a>
<a name="ln287">  DISALLOW_COPY_AND_ASSIGN(LogEntryBatch);</a>
<a name="ln288">};</a>
<a name="ln289"> </a>
<a name="ln290">// This class is responsible for managing the task that appends to the log file.</a>
<a name="ln291">// This task runs in a common thread pool with append tasks from other tablets.</a>
<a name="ln292">// A token is used to ensure that only one append task per tablet is executed concurrently.</a>
<a name="ln293">class Log::Appender {</a>
<a name="ln294"> public:</a>
<a name="ln295">  explicit Appender(Log* log, ThreadPool* append_thread_pool);</a>
<a name="ln296"> </a>
<a name="ln297">  // Initializes the objects and starts the task.</a>
<a name="ln298">  Status Init();</a>
<a name="ln299"> </a>
<a name="ln300">  CHECKED_STATUS Submit(LogEntryBatch* item) {</a>
<a name="ln301">    return task_stream_-&gt;Submit(item);</a>
<a name="ln302">  }</a>
<a name="ln303"> </a>
<a name="ln304">  CHECKED_STATUS TEST_SubmitFunc(const std::function&lt;void()&gt;&amp; func) {</a>
<a name="ln305">    return task_stream_-&gt;TEST_SubmitFunc(func);</a>
<a name="ln306">  }</a>
<a name="ln307"> </a>
<a name="ln308">  // Waits until the last enqueued elements are processed, sets the appender_ to closing</a>
<a name="ln309">  // state. If any entries are added to the queue during the process, invoke their callbacks'</a>
<a name="ln310">  // 'OnFailure()' method.</a>
<a name="ln311">  void Shutdown();</a>
<a name="ln312"> </a>
<a name="ln313">  const std::string&amp; LogPrefix() const {</a>
<a name="ln314">    return log_-&gt;LogPrefix();</a>
<a name="ln315">  }</a>
<a name="ln316"> </a>
<a name="ln317">  std::string GetRunThreadStack() const {</a>
<a name="ln318">    return task_stream_-&gt;GetRunThreadStack();</a>
<a name="ln319">  }</a>
<a name="ln320"> </a>
<a name="ln321">  std::string ToString() const {</a>
<a name="ln322">    return task_stream_-&gt;ToString();</a>
<a name="ln323">  }</a>
<a name="ln324"> </a>
<a name="ln325"> private:</a>
<a name="ln326">  // Process the given log entry batch or does a sync if a null is passed.</a>
<a name="ln327">  void ProcessBatch(LogEntryBatch* entry_batch);</a>
<a name="ln328">  void GroupWork();</a>
<a name="ln329"> </a>
<a name="ln330">  Log* const log_;</a>
<a name="ln331"> </a>
<a name="ln332">  // Lock to protect access to thread_ during shutdown.</a>
<a name="ln333">  mutable std::mutex lock_;</a>
<a name="ln334">  unique_ptr&lt;TaskStream&lt;LogEntryBatch&gt;&gt; task_stream_;</a>
<a name="ln335"> </a>
<a name="ln336">  // vector of entry batches in group, to execute callbacks after call to Sync.</a>
<a name="ln337">  std::vector&lt;std::unique_ptr&lt;LogEntryBatch&gt;&gt; sync_batch_;</a>
<a name="ln338"> </a>
<a name="ln339">  // Time at which current group was started</a>
<a name="ln340">  MonoTime time_started_;</a>
<a name="ln341">};</a>
<a name="ln342"> </a>
<a name="ln343">Log::Appender::Appender(Log *log, ThreadPool* append_thread_pool)</a>
<a name="ln344">    : log_(log),</a>
<a name="ln345">      task_stream_(new TaskStream&lt;LogEntryBatch&gt;(</a>
<a name="ln346">          std::bind(&amp;Log::Appender::ProcessBatch, this, _1), append_thread_pool,</a>
<a name="ln347">          FLAGS_taskstream_queue_max_size,</a>
<a name="ln348">          MonoDelta::FromMilliseconds(FLAGS_taskstream_queue_max_wait_ms))) {</a>
<a name="ln349">  DCHECK(dummy);</a>
<a name="ln350">}</a>
<a name="ln351"> </a>
<a name="ln352">Status Log::Appender::Init() {</a>
<a name="ln353">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Starting log task stream&quot;;</a>
<a name="ln354">  return Status::OK();</a>
<a name="ln355">}</a>
<a name="ln356"> </a>
<a name="ln357">void Log::Appender::ProcessBatch(LogEntryBatch* entry_batch) {</a>
<a name="ln358">  // A callback function to TaskStream is expected to process the accumulated batch of entries.</a>
<a name="ln359">  if (entry_batch == nullptr) {</a>
<a name="ln360">    // Here, we do sync and call callbacks.</a>
<a name="ln361">    GroupWork();</a>
<a name="ln362">    return;</a>
<a name="ln363">  }</a>
<a name="ln364"> </a>
<a name="ln365">  if (sync_batch_.empty()) { // Start of batch.</a>
<a name="ln366">    // Used in tests to delay writing log entries.</a>
<a name="ln367">    auto sleep_duration = log_-&gt;sleep_duration_.load(std::memory_order_acquire);</a>
<a name="ln368">    if (sleep_duration.count() &gt; 0) {</a>
<a name="ln369">      std::this_thread::sleep_for(sleep_duration);</a>
<a name="ln370">    }</a>
<a name="ln371">    time_started_ = MonoTime::Now();</a>
<a name="ln372">  }</a>
<a name="ln373">  TRACE_EVENT_FLOW_END0(&quot;log&quot;, &quot;Batch&quot;, entry_batch);</a>
<a name="ln374">  Status s = log_-&gt;DoAppend(entry_batch);</a>
<a name="ln375"> </a>
<a name="ln376">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln377">    LOG_WITH_PREFIX(DFATAL) &lt;&lt; &quot;Error appending to the log: &quot; &lt;&lt; s;</a>
<a name="ln378">    entry_batch-&gt;set_failed_to_append();</a>
<a name="ln379">    // TODO If a single operation fails to append, should we abort all subsequent operations</a>
<a name="ln380">    // in this batch or allow them to be appended? What about operations in future batches?</a>
<a name="ln381">    if (!entry_batch-&gt;callback().is_null()) {</a>
<a name="ln382">      entry_batch-&gt;callback().Run(s);</a>
<a name="ln383">    }</a>
<a name="ln384">    return;</a>
<a name="ln385">  }</a>
<a name="ln386">  if (!log_-&gt;sync_disabled_) {</a>
<a name="ln387">    bool expected = false;</a>
<a name="ln388">    if (log_-&gt;periodic_sync_needed_.compare_exchange_strong(expected, true,</a>
<a name="ln389">                                                            std::memory_order_acq_rel)) {</a>
<a name="ln390">      log_-&gt;periodic_sync_earliest_unsync_entry_time_ = MonoTime::Now();</a>
<a name="ln391">    }</a>
<a name="ln392">    log_-&gt;periodic_sync_unsynced_bytes_ += entry_batch-&gt;total_size_bytes();</a>
<a name="ln393">  }</a>
<a name="ln394">  sync_batch_.emplace_back(entry_batch);</a>
<a name="ln395">}</a>
<a name="ln396"> </a>
<a name="ln397">void Log::Appender::GroupWork() {</a>
<a name="ln398">  if (sync_batch_.empty()) {</a>
<a name="ln399">    Status s = log_-&gt;Sync();</a>
<a name="ln400">    return;</a>
<a name="ln401">  }</a>
<a name="ln402">  if (log_-&gt;metrics_) {</a>
<a name="ln403">    log_-&gt;metrics_-&gt;entry_batches_per_group-&gt;Increment(sync_batch_.size());</a>
<a name="ln404">  }</a>
<a name="ln405">  TRACE_EVENT1(&quot;log&quot;, &quot;batch&quot;, &quot;batch_size&quot;, sync_batch_.size());</a>
<a name="ln406"> </a>
<a name="ln407">  auto se = ScopeExit([this] {</a>
<a name="ln408">    if (log_-&gt;metrics_) {</a>
<a name="ln409">      MonoTime time_now = MonoTime::Now();</a>
<a name="ln410">      log_-&gt;metrics_-&gt;group_commit_latency-&gt;Increment(</a>
<a name="ln411">          time_now.GetDeltaSince(time_started_).ToMicroseconds());</a>
<a name="ln412">    }</a>
<a name="ln413">    sync_batch_.clear();</a>
<a name="ln414">  });</a>
<a name="ln415"> </a>
<a name="ln416">  Status s = log_-&gt;Sync();</a>
<a name="ln417">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln418">    LOG_WITH_PREFIX(DFATAL) &lt;&lt; &quot;Error syncing log: &quot; &lt;&lt; s;</a>
<a name="ln419">    for (std::unique_ptr&lt;LogEntryBatch&gt;&amp; entry_batch : sync_batch_) {</a>
<a name="ln420">      if (!entry_batch-&gt;callback().is_null()) {</a>
<a name="ln421">        entry_batch-&gt;callback().Run(s);</a>
<a name="ln422">      }</a>
<a name="ln423">    }</a>
<a name="ln424">  } else {</a>
<a name="ln425">    TRACE_EVENT0(&quot;log&quot;, &quot;Callbacks&quot;);</a>
<a name="ln426">    VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Synchronized &quot; &lt;&lt; sync_batch_.size() &lt;&lt; &quot; entry batches&quot;;</a>
<a name="ln427">    LongOperationTracker long_operation_tracker(</a>
<a name="ln428">        &quot;Log callback&quot;, FLAGS_consensus_log_scoped_watch_delay_callback_threshold_ms * 1ms);</a>
<a name="ln429">    for (std::unique_ptr&lt;LogEntryBatch&gt;&amp; entry_batch : sync_batch_) {</a>
<a name="ln430">      if (PREDICT_TRUE(!entry_batch-&gt;failed_to_append() &amp;&amp; !entry_batch-&gt;callback().is_null())) {</a>
<a name="ln431">        entry_batch-&gt;callback().Run(Status::OK());</a>
<a name="ln432">      }</a>
<a name="ln433">      // It's important to delete each batch as we see it, because deleting it may free up memory</a>
<a name="ln434">      // from memory trackers, and the callback of a later batch may want to use that memory.</a>
<a name="ln435">      entry_batch.reset();</a>
<a name="ln436">    }</a>
<a name="ln437">    sync_batch_.clear();</a>
<a name="ln438">  }</a>
<a name="ln439">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Exiting AppendTask for tablet &quot; &lt;&lt; log_-&gt;tablet_id();</a>
<a name="ln440">}</a>
<a name="ln441"> </a>
<a name="ln442">void Log::Appender::Shutdown() {</a>
<a name="ln443">  std::lock_guard&lt;std::mutex&gt; lock_guard(lock_);</a>
<a name="ln444">  if (task_stream_) {</a>
<a name="ln445">    VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Shutting down log task stream&quot;;</a>
<a name="ln446">    task_stream_-&gt;Stop();</a>
<a name="ln447">    VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Log append task stream is shut down&quot;;</a>
<a name="ln448">    task_stream_.reset();</a>
<a name="ln449">  }</a>
<a name="ln450">}</a>
<a name="ln451"> </a>
<a name="ln452">// This task is submitted to allocation_pool_ in order to asynchronously pre-allocate new log</a>
<a name="ln453">// segments.</a>
<a name="ln454">void Log::SegmentAllocationTask() {</a>
<a name="ln455">  allocation_status_.Set(PreAllocateNewSegment());</a>
<a name="ln456">}</a>
<a name="ln457"> </a>
<a name="ln458">const Status Log::kLogShutdownStatus(</a>
<a name="ln459">    STATUS(ServiceUnavailable, &quot;WAL is shutting down&quot;, &quot;&quot;, Errno(ESHUTDOWN)));</a>
<a name="ln460"> </a>
<a name="ln461">Status Log::Open(const LogOptions &amp;options,</a>
<a name="ln462">                 const std::string&amp; tablet_id,</a>
<a name="ln463">                 const std::string&amp; wal_dir,</a>
<a name="ln464">                 const std::string&amp; peer_uuid,</a>
<a name="ln465">                 const Schema&amp; schema,</a>
<a name="ln466">                 uint32_t schema_version,</a>
<a name="ln467">                 const scoped_refptr&lt;MetricEntity&gt;&amp; metric_entity,</a>
<a name="ln468">                 ThreadPool* append_thread_pool,</a>
<a name="ln469">                 ThreadPool* allocation_thread_pool,</a>
<a name="ln470">                 int64_t cdc_min_replicated_index,</a>
<a name="ln471">                 scoped_refptr&lt;Log&gt;* log,</a>
<a name="ln472">                 CreateNewSegment create_new_segment) {</a>
<a name="ln473"> </a>
<a name="ln474">  RETURN_NOT_OK_PREPEND(env_util::CreateDirIfMissing(options.env, DirName(wal_dir)),</a>
<a name="ln475">                        Substitute(&quot;Failed to create table wal dir $0&quot;, DirName(wal_dir)));</a>
<a name="ln476"> </a>
<a name="ln477">  RETURN_NOT_OK_PREPEND(env_util::CreateDirIfMissing(options.env, wal_dir),</a>
<a name="ln478">                        Substitute(&quot;Failed to create tablet wal dir $0&quot;, wal_dir));</a>
<a name="ln479"> </a>
<a name="ln480">  scoped_refptr&lt;Log&gt; new_log(new Log(options,</a>
<a name="ln481">                                     wal_dir,</a>
<a name="ln482">                                     tablet_id,</a>
<a name="ln483">                                     peer_uuid,</a>
<a name="ln484">                                     schema,</a>
<a name="ln485">                                     schema_version,</a>
<a name="ln486">                                     metric_entity,</a>
<a name="ln487">                                     append_thread_pool,</a>
<a name="ln488">                                     allocation_thread_pool,</a>
<a name="ln489">                                     create_new_segment));</a>
<a name="ln490">  RETURN_NOT_OK(new_log-&gt;Init());</a>
<a name="ln491">  log-&gt;swap(new_log);</a>
<a name="ln492">  return Status::OK();</a>
<a name="ln493">}</a>
<a name="ln494"> </a>
<a name="ln495">Log::Log(</a>
<a name="ln496">    LogOptions options,</a>
<a name="ln497">    string wal_dir,</a>
<a name="ln498">    string tablet_id,</a>
<a name="ln499">    string peer_uuid,</a>
<a name="ln500">    const Schema&amp; schema,</a>
<a name="ln501">    uint32_t schema_version,</a>
<a name="ln502">    const scoped_refptr&lt;MetricEntity&gt;&amp; metric_entity,</a>
<a name="ln503">    ThreadPool* append_thread_pool,</a>
<a name="ln504">    ThreadPool* allocation_thread_pool,</a>
<a name="ln505">    CreateNewSegment create_new_segment)</a>
<a name="ln506">    : options_(std::move(options)),</a>
<a name="ln507">      wal_dir_(std::move(wal_dir)),</a>
<a name="ln508">      tablet_id_(std::move(tablet_id)),</a>
<a name="ln509">      peer_uuid_(std::move(peer_uuid)),</a>
<a name="ln510">      schema_(schema),</a>
<a name="ln511">      schema_version_(schema_version),</a>
<a name="ln512">      active_segment_sequence_number_(options.initial_active_segment_sequence_number),</a>
<a name="ln513">      log_state_(kLogInitialized),</a>
<a name="ln514">      max_segment_size_(options_.segment_size_bytes),</a>
<a name="ln515">      // We halve the initial log segment size here because we double it for every new segment,</a>
<a name="ln516">      // including the very first segment.</a>
<a name="ln517">      cur_max_segment_size_((options.initial_segment_size_bytes + 1) / 2),</a>
<a name="ln518">      appender_(new Appender(this, append_thread_pool)),</a>
<a name="ln519">      allocation_token_(allocation_thread_pool-&gt;NewToken(ThreadPool::ExecutionMode::SERIAL)),</a>
<a name="ln520">      durable_wal_write_(options_.durable_wal_write),</a>
<a name="ln521">      interval_durable_wal_write_(options_.interval_durable_wal_write),</a>
<a name="ln522">      bytes_durable_wal_write_mb_(options_.bytes_durable_wal_write_mb),</a>
<a name="ln523">      sync_disabled_(false),</a>
<a name="ln524">      allocation_state_(kAllocationNotStarted),</a>
<a name="ln525">      metric_entity_(metric_entity),</a>
<a name="ln526">      on_disk_size_(0),</a>
<a name="ln527">      log_prefix_(consensus::MakeTabletLogPrefix(tablet_id_, peer_uuid_)),</a>
<a name="ln528">      create_new_segment_at_start_(create_new_segment) {</a>
<a name="ln529">  set_wal_retention_secs(options.retention_secs);</a>
<a name="ln530">  if (metric_entity_) {</a>
<a name="ln531">    metrics_.reset(new LogMetrics(metric_entity_));</a>
<a name="ln532">  }</a>
<a name="ln533">}</a>
<a name="ln534"> </a>
<a name="ln535">Status Log::Init() {</a>
<a name="ln536">  std::lock_guard&lt;percpu_rwlock&gt; write_lock(state_lock_);</a>
<a name="ln537">  CHECK_EQ(kLogInitialized, log_state_);</a>
<a name="ln538">  // Init the index</a>
<a name="ln539">  log_index_.reset(new LogIndex(wal_dir_));</a>
<a name="ln540">  // Reader for previous segments.</a>
<a name="ln541">  RETURN_NOT_OK(LogReader::Open(get_env(),</a>
<a name="ln542">                                log_index_,</a>
<a name="ln543">                                tablet_id_,</a>
<a name="ln544">                                wal_dir_,</a>
<a name="ln545">                                peer_uuid_,</a>
<a name="ln546">                                metric_entity_.get(),</a>
<a name="ln547">                                &amp;reader_));</a>
<a name="ln548"> </a>
<a name="ln549">  // The case where we are continuing an existing log.  We must pick up where the previous WAL left</a>
<a name="ln550">  // off in terms of sequence numbers.</a>
<a name="ln551">  if (reader_-&gt;num_segments() != 0) {</a>
<a name="ln552">    VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Using existing &quot; &lt;&lt; reader_-&gt;num_segments()</a>
<a name="ln553">                        &lt;&lt; &quot; segments from path: &quot; &lt;&lt; wal_dir_;</a>
<a name="ln554"> </a>
<a name="ln555">    vector&lt;scoped_refptr&lt;ReadableLogSegment&gt; &gt; segments;</a>
<a name="ln556">    RETURN_NOT_OK(reader_-&gt;GetSegmentsSnapshot(&amp;segments));</a>
<a name="ln557">    active_segment_sequence_number_ = segments.back()-&gt;header().sequence_number();</a>
<a name="ln558">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Opened existing logs. Last segment is &quot; &lt;&lt; segments.back()-&gt;path();</a>
<a name="ln559">  }</a>
<a name="ln560"> </a>
<a name="ln561">  if (durable_wal_write_) {</a>
<a name="ln562">    YB_LOG_FIRST_N(INFO, 1) &lt;&lt; &quot;durable_wal_write is turned on.&quot;;</a>
<a name="ln563">  } else if (interval_durable_wal_write_) {</a>
<a name="ln564">    YB_LOG_FIRST_N(INFO, 1) &lt;&lt; &quot;interval_durable_wal_write_ms is turned on to sync every &quot;</a>
<a name="ln565">                            &lt;&lt; interval_durable_wal_write_.ToMilliseconds() &lt;&lt; &quot; ms.&quot;;</a>
<a name="ln566">  } else if (bytes_durable_wal_write_mb_ &gt; 0) {</a>
<a name="ln567">    YB_LOG_FIRST_N(INFO, 1) &lt;&lt; &quot;bytes_durable_wal_write_mb is turned on to sync every &quot;</a>
<a name="ln568">     &lt;&lt; bytes_durable_wal_write_mb_ &lt;&lt; &quot; MB of data.&quot;;</a>
<a name="ln569">  } else {</a>
<a name="ln570">    YB_LOG_FIRST_N(INFO, 1) &lt;&lt; &quot;durable_wal_write is turned off. Buffered IO will be used for WAL.&quot;;</a>
<a name="ln571">  }</a>
<a name="ln572"> </a>
<a name="ln573">  if (create_new_segment_at_start_) {</a>
<a name="ln574">    RETURN_NOT_OK(EnsureInitialNewSegmentAllocated());</a>
<a name="ln575">  }</a>
<a name="ln576">  return Status::OK();</a>
<a name="ln577">}</a>
<a name="ln578"> </a>
<a name="ln579">Status Log::AsyncAllocateSegment() {</a>
<a name="ln580">  SCHECK_EQ(</a>
<a name="ln581">      allocation_state_.load(std::memory_order_acquire), kAllocationNotStarted, AlreadyPresent,</a>
<a name="ln582">      &quot;Allocation already running&quot;);</a>
<a name="ln583">  allocation_status_.Reset();</a>
<a name="ln584">  allocation_state_.store(kAllocationInProgress, std::memory_order_release);</a>
<a name="ln585">  return allocation_token_-&gt;SubmitClosure(Bind(&amp;Log::SegmentAllocationTask, Unretained(this)));</a>
<a name="ln586">}</a>
<a name="ln587"> </a>
<a name="ln588">Status Log::CloseCurrentSegment() {</a>
<a name="ln589">  if (!footer_builder_.has_min_replicate_index()) {</a>
<a name="ln590">    VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Writing a segment without any REPLICATE message. Segment: &quot;</a>
<a name="ln591">                        &lt;&lt; active_segment_-&gt;path();</a>
<a name="ln592">  }</a>
<a name="ln593">  VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Segment footer for &quot; &lt;&lt; active_segment_-&gt;path()</a>
<a name="ln594">                      &lt;&lt; &quot;: &quot; &lt;&lt; footer_builder_.ShortDebugString();</a>
<a name="ln595"> </a>
<a name="ln596">  footer_builder_.set_close_timestamp_micros(GetCurrentTimeMicros());</a>
<a name="ln597">  return active_segment_-&gt;WriteFooterAndClose(footer_builder_);</a>
<a name="ln598">}</a>
<a name="ln599"> </a>
<a name="ln600">Status Log::RollOver() {</a>
<a name="ln601">  SCOPED_LATENCY_METRIC(metrics_, roll_latency);</a>
<a name="ln602"> </a>
<a name="ln603">  // Check if any errors have occurred during allocation</a>
<a name="ln604">  RETURN_NOT_OK(allocation_status_.Get());</a>
<a name="ln605"> </a>
<a name="ln606">  DCHECK_EQ(allocation_state(), kAllocationFinished);</a>
<a name="ln607"> </a>
<a name="ln608">  LOG_WITH_PREFIX(INFO) &lt;&lt; Format(&quot;Last appended OpId in segment $0: $1&quot;, active_segment_-&gt;path(),</a>
<a name="ln609">                                  last_appended_entry_op_id_.ToString());</a>
<a name="ln610"> </a>
<a name="ln611">  RETURN_NOT_OK(Sync());</a>
<a name="ln612">  RETURN_NOT_OK(CloseCurrentSegment());</a>
<a name="ln613"> </a>
<a name="ln614">  RETURN_NOT_OK(SwitchToAllocatedSegment());</a>
<a name="ln615"> </a>
<a name="ln616">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Rolled over to a new segment: &quot; &lt;&lt; active_segment_-&gt;path();</a>
<a name="ln617">  return Status::OK();</a>
<a name="ln618">}</a>
<a name="ln619"> </a>
<a name="ln620">Status Log::Reserve(LogEntryTypePB type,</a>
<a name="ln621">                    LogEntryBatchPB* entry_batch,</a>
<a name="ln622">                    LogEntryBatch** reserved_entry) {</a>
<a name="ln623">  TRACE_EVENT0(&quot;log&quot;, &quot;Log::Reserve&quot;);</a>
<a name="ln624">  DCHECK(reserved_entry != nullptr);</a>
<a name="ln625">  {</a>
<a name="ln626">    SharedLock&lt;rw_spinlock&gt; read_lock(state_lock_.get_lock());</a>
<a name="ln627">    CHECK_EQ(kLogWriting, log_state_);</a>
<a name="ln628">  }</a>
<a name="ln629"> </a>
<a name="ln630">  // In DEBUG builds, verify that all of the entries in the batch match the specified type.  In</a>
<a name="ln631">  // non-debug builds the foreach loop gets optimized out.</a>
<a name="ln632">#ifndef NDEBUG</a>
<a name="ln633">  for (const LogEntryPB&amp; entry : entry_batch-&gt;entry()) {</a>
<a name="ln634">    DCHECK_EQ(entry.type(), type) &lt;&lt; &quot;Bad batch: &quot; &lt;&lt; entry_batch-&gt;DebugString();</a>
<a name="ln635">  }</a>
<a name="ln636">#endif</a>
<a name="ln637"> </a>
<a name="ln638">  auto new_entry_batch = std::make_unique&lt;LogEntryBatch&gt;(type, std::move(*entry_batch));</a>
<a name="ln639">  new_entry_batch-&gt;MarkReserved();</a>
<a name="ln640"> </a>
<a name="ln641">  // Release the memory back to the caller: this will be freed when</a>
<a name="ln642">  // the entry is removed from the queue.</a>
<a name="ln643">  //</a>
<a name="ln644">  // TODO (perf) Use a ring buffer instead of a blocking queue and set</a>
<a name="ln645">  // 'reserved_entry' to a pre-allocated slot in the buffer.</a>
<a name="ln646">  *reserved_entry = new_entry_batch.release();</a>
<a name="ln647">  return Status::OK();</a>
<a name="ln648">}</a>
<a name="ln649"> </a>
<a name="ln650">Status Log::TEST_AsyncAppendWithReplicates(</a>
<a name="ln651">    LogEntryBatch* entry, const ReplicateMsgs&amp; replicates, const StatusCallback&amp; callback) {</a>
<a name="ln652">  entry-&gt;SetReplicates(replicates);</a>
<a name="ln653">  return AsyncAppend(entry, callback);</a>
<a name="ln654">}</a>
<a name="ln655"> </a>
<a name="ln656">Status Log::AsyncAppend(LogEntryBatch* entry_batch, const StatusCallback&amp; callback) {</a>
<a name="ln657">  {</a>
<a name="ln658">    SharedLock&lt;rw_spinlock&gt; read_lock(state_lock_.get_lock());</a>
<a name="ln659">    CHECK_EQ(kLogWriting, log_state_);</a>
<a name="ln660">  }</a>
<a name="ln661"> </a>
<a name="ln662">  entry_batch-&gt;set_callback(callback);</a>
<a name="ln663">  entry_batch-&gt;MarkReady();</a>
<a name="ln664"> </a>
<a name="ln665">  if (entry_batch-&gt;HasReplicateEntries()) {</a>
<a name="ln666">    last_submitted_op_id_ = yb::OpId::FromPB(entry_batch-&gt;MaxReplicateOpId());</a>
<a name="ln667">  }</a>
<a name="ln668"> </a>
<a name="ln669">  auto submit_status = appender_-&gt;Submit(entry_batch);</a>
<a name="ln670">  if (PREDICT_FALSE(!submit_status.ok())) {</a>
<a name="ln671">    LOG_WITH_PREFIX(WARNING)</a>
<a name="ln672">        &lt;&lt; &quot;Failed to submit batch &quot; &lt;&lt; entry_batch-&gt;MaxReplicateOpId().ShortDebugString() &lt;&lt; &quot;: &quot;</a>
<a name="ln673">        &lt;&lt; submit_status;</a>
<a name="ln674">    delete entry_batch;</a>
<a name="ln675">    return kLogShutdownStatus;</a>
<a name="ln676">  }</a>
<a name="ln677"> </a>
<a name="ln678">  return Status::OK();</a>
<a name="ln679">}</a>
<a name="ln680"> </a>
<a name="ln681">Status Log::AsyncAppendReplicates(const ReplicateMsgs&amp; msgs, const yb::OpId&amp; committed_op_id,</a>
<a name="ln682">                                  RestartSafeCoarseTimePoint batch_mono_time,</a>
<a name="ln683">                                  const StatusCallback&amp; callback) {</a>
<a name="ln684">  auto batch = CreateBatchFromAllocatedOperations(msgs);</a>
<a name="ln685">  if (committed_op_id) {</a>
<a name="ln686">    committed_op_id.ToPB(batch.mutable_committed_op_id());</a>
<a name="ln687">  }</a>
<a name="ln688">  // Set batch mono time if it was specified.</a>
<a name="ln689">  if (batch_mono_time != RestartSafeCoarseTimePoint()) {</a>
<a name="ln690">    batch.set_mono_time(batch_mono_time.ToUInt64());</a>
<a name="ln691">  }</a>
<a name="ln692"> </a>
<a name="ln693">  LogEntryBatch* reserved_entry_batch;</a>
<a name="ln694">  RETURN_NOT_OK(Reserve(REPLICATE, &amp;batch, &amp;reserved_entry_batch));</a>
<a name="ln695"> </a>
<a name="ln696">  // If we're able to reserve, set the vector of replicate shared pointers in the LogEntryBatch.</a>
<a name="ln697">  // This will make sure there's a reference for each replicate while we're appending.</a>
<a name="ln698">  reserved_entry_batch-&gt;SetReplicates(msgs);</a>
<a name="ln699"> </a>
<a name="ln700">  RETURN_NOT_OK(AsyncAppend(reserved_entry_batch, callback));</a>
<a name="ln701">  return Status::OK();</a>
<a name="ln702">}</a>
<a name="ln703"> </a>
<a name="ln704">bool Log::NeedNewSegment(uint32_t entry_batch_bytes) {</a>
<a name="ln705">  return (active_segment_-&gt;Size() + entry_batch_bytes + 4) &gt; cur_max_segment_size_;</a>
<a name="ln706">}</a>
<a name="ln707"> </a>
<a name="ln708">Status Log::RollOverIfNecessary(uint32_t entry_batch_bytes) {</a>
<a name="ln709">  // If the size of this entry overflows the current segment, get a new one.</a>
<a name="ln710">  auto allocation_state = this-&gt;allocation_state();</a>
<a name="ln711">  if (allocation_state == kAllocationNotStarted) {</a>
<a name="ln712">    if (!NeedNewSegment(entry_batch_bytes)) {</a>
<a name="ln713">      return Status::OK();</a>
<a name="ln714">    }</a>
<a name="ln715">  }</a>
<a name="ln716">  enum class Outcome {</a>
<a name="ln717">    kNotDefined,</a>
<a name="ln718">    kRunRollOver,</a>
<a name="ln719">    kWaitRollOver,</a>
<a name="ln720">    kDoNothing,</a>
<a name="ln721">  };</a>
<a name="ln722">  Outcome outcome = Outcome::kNotDefined;</a>
<a name="ln723">  {</a>
<a name="ln724">    std::lock_guard&lt;std::mutex&gt; lock(allocation_mutex_);</a>
<a name="ln725">    switch (allocation_state) {</a>
<a name="ln726">      case kAllocationNotStarted: {</a>
<a name="ln727">          if (!NeedNewSegment(entry_batch_bytes)) {</a>
<a name="ln728">            return Status::OK();</a>
<a name="ln729">          }</a>
<a name="ln730">          LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Max segment size &quot; &lt;&lt; cur_max_segment_size_ &lt;&lt; &quot; reached. &quot;</a>
<a name="ln731">                                &lt;&lt; &quot;Starting new segment allocation. &quot;;</a>
<a name="ln732">          auto status = AsyncAllocateSegment();</a>
<a name="ln733">          if (!status.ok()) {</a>
<a name="ln734">            if (!status.IsAlreadyPresent()) {</a>
<a name="ln735">              return status;</a>
<a name="ln736">            }</a>
<a name="ln737">            outcome = Outcome::kWaitRollOver;</a>
<a name="ln738">          } else if (options_.async_preallocate_segments) {</a>
<a name="ln739">            allocation_requested_ = true;</a>
<a name="ln740">            outcome = Outcome::kDoNothing;</a>
<a name="ln741">          } else {</a>
<a name="ln742">            outcome = Outcome::kRunRollOver;</a>
<a name="ln743">          }</a>
<a name="ln744">        } break;</a>
<a name="ln745">      case kAllocationFinished: {</a>
<a name="ln746">          if (!allocation_requested_) {</a>
<a name="ln747">            outcome = Outcome::kWaitRollOver;</a>
<a name="ln748">          } else {</a>
<a name="ln749">            outcome = Outcome::kRunRollOver;</a>
<a name="ln750">            allocation_requested_ = false;</a>
<a name="ln751">          }</a>
<a name="ln752">        } break;</a>
<a name="ln753">      case kAllocationInProgress: {</a>
<a name="ln754">        VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Segment allocation already in progress...&quot;;</a>
<a name="ln755">        outcome = allocation_requested_ ? Outcome::kDoNothing : Outcome::kWaitRollOver;</a>
<a name="ln756">      } break;</a>
<a name="ln757">    }</a>
<a name="ln758">  }</a>
<a name="ln759">  switch (outcome) {</a>
<a name="ln760">    case Outcome::kNotDefined:</a>
<a name="ln761">      FATAL_INVALID_ENUM_VALUE(SegmentAllocationState, allocation_state);</a>
<a name="ln762">      return Status::OK();</a>
<a name="ln763">    case Outcome::kRunRollOver:</a>
<a name="ln764">      LOG_SLOW_EXECUTION(WARNING, 50, &quot;Log roll took a long time&quot;) {</a>
<a name="ln765">        return RollOver();</a>
<a name="ln766">      }</a>
<a name="ln767">      return Status::OK();</a>
<a name="ln768">    case Outcome::kWaitRollOver: {</a>
<a name="ln769">        std::unique_lock&lt;std::mutex&gt; lock(allocation_mutex_);</a>
<a name="ln770">        allocation_cond_.wait(lock, [this] {</a>
<a name="ln771">          return allocation_state_.load(std::memory_order_acquire) == kAllocationNotStarted;</a>
<a name="ln772">        });</a>
<a name="ln773">      }</a>
<a name="ln774">      return Status::OK();</a>
<a name="ln775">    case Outcome::kDoNothing:</a>
<a name="ln776">      return Status::OK();</a>
<a name="ln777">  }</a>
<a name="ln778">  FATAL_INVALID_ENUM_VALUE(Outcome, outcome);</a>
<a name="ln779">}</a>
<a name="ln780"> </a>
<a name="ln781">Status Log::DoAppend(LogEntryBatch* entry_batch,</a>
<a name="ln782">                     bool caller_owns_operation,</a>
<a name="ln783">                     bool skip_wal_write) {</a>
<a name="ln784">  if (!skip_wal_write) {</a>
<a name="ln785">    RETURN_NOT_OK(entry_batch-&gt;Serialize());</a>
<a name="ln786">    Slice entry_batch_data = entry_batch-&gt;data();</a>
<a name="ln787">    LOG_IF(DFATAL, entry_batch_data.size() &lt;= 0 &amp;&amp; !entry_batch-&gt;flush_marker())</a>
<a name="ln788">        &lt;&lt; &quot;Cannot call DoAppend() with no data&quot;;</a>
<a name="ln789"> </a>
<a name="ln790">    uint32_t entry_batch_bytes = entry_batch-&gt;total_size_bytes();</a>
<a name="ln791">    // If there is no data to write return OK.</a>
<a name="ln792">    if (PREDICT_FALSE(entry_batch_bytes == 0)) {</a>
<a name="ln793">      return Status::OK();</a>
<a name="ln794">    }</a>
<a name="ln795"> </a>
<a name="ln796">    RETURN_NOT_OK(RollOverIfNecessary(entry_batch_bytes));</a>
<a name="ln797"> </a>
<a name="ln798">    int64_t start_offset = active_segment_-&gt;written_offset();</a>
<a name="ln799"> </a>
<a name="ln800">    LOG_SLOW_EXECUTION(WARNING, 50, &quot;Append to log took a long time&quot;) {</a>
<a name="ln801">      SCOPED_LATENCY_METRIC(metrics_, append_latency);</a>
<a name="ln802">      LongOperationTracker long_operation_tracker(</a>
<a name="ln803">          &quot;Log append&quot;, FLAGS_consensus_log_scoped_watch_delay_append_threshold_ms * 1ms);</a>
<a name="ln804"> </a>
<a name="ln805">      RETURN_NOT_OK(active_segment_-&gt;WriteEntryBatch(entry_batch_data));</a>
<a name="ln806">    }</a>
<a name="ln807"> </a>
<a name="ln808">    if (metrics_) {</a>
<a name="ln809">      metrics_-&gt;bytes_logged-&gt;IncrementBy(entry_batch_bytes);</a>
<a name="ln810">    }</a>
<a name="ln811"> </a>
<a name="ln812">    // Populate the offset and sequence number for the entry batch if we did a WAL write.</a>
<a name="ln813">    entry_batch-&gt;offset_ = start_offset;</a>
<a name="ln814">    entry_batch-&gt;active_segment_sequence_number_ = active_segment_sequence_number_;</a>
<a name="ln815">  }</a>
<a name="ln816"> </a>
<a name="ln817">  // We keep track of the last-written OpId here. This is needed to initialize Consensus on</a>
<a name="ln818">  // startup.</a>
<a name="ln819">  if (entry_batch-&gt;HasReplicateEntries()) {</a>
<a name="ln820">    last_appended_entry_op_id_ = yb::OpId::FromPB(entry_batch-&gt;MaxReplicateOpId());</a>
<a name="ln821">  }</a>
<a name="ln822"> </a>
<a name="ln823">  CHECK_OK(UpdateIndexForBatch(*entry_batch));</a>
<a name="ln824">  UpdateFooterForBatch(entry_batch);</a>
<a name="ln825"> </a>
<a name="ln826">  // We expect the caller to free the actual entries if caller_owns_operation is set.</a>
<a name="ln827">  if (caller_owns_operation) {</a>
<a name="ln828">    for (int i = 0; i &lt; entry_batch-&gt;entry_batch_pb_.entry_size(); i++) {</a>
<a name="ln829">      LogEntryPB* entry_pb = entry_batch-&gt;entry_batch_pb_.mutable_entry(i);</a>
<a name="ln830">      entry_pb-&gt;release_replicate();</a>
<a name="ln831">    }</a>
<a name="ln832">  }</a>
<a name="ln833"> </a>
<a name="ln834">  return Status::OK();</a>
<a name="ln835">}</a>
<a name="ln836"> </a>
<a name="ln837">Status Log::UpdateIndexForBatch(const LogEntryBatch&amp; batch) {</a>
<a name="ln838">  if (batch.type_ != REPLICATE) {</a>
<a name="ln839">    return Status::OK();</a>
<a name="ln840">  }</a>
<a name="ln841"> </a>
<a name="ln842">  for (const LogEntryPB&amp; entry_pb : batch.entry_batch_pb_.entry()) {</a>
<a name="ln843">    LogIndexEntry index_entry;</a>
<a name="ln844"> </a>
<a name="ln845">    index_entry.op_id = yb::OpId::FromPB(entry_pb.replicate().id());</a>
<a name="ln846">    index_entry.segment_sequence_number = batch.active_segment_sequence_number_;</a>
<a name="ln847">    index_entry.offset_in_segment = batch.offset_;</a>
<a name="ln848">    RETURN_NOT_OK(log_index_-&gt;AddEntry(index_entry));</a>
<a name="ln849">  }</a>
<a name="ln850">  return Status::OK();</a>
<a name="ln851">}</a>
<a name="ln852"> </a>
<a name="ln853">void Log::UpdateFooterForBatch(LogEntryBatch* batch) {</a>
<a name="ln854">  footer_builder_.set_num_entries(footer_builder_.num_entries() + batch-&gt;count());</a>
<a name="ln855"> </a>
<a name="ln856">  // We keep track of the last-written OpId here.  This is needed to initialize Consensus on</a>
<a name="ln857">  // startup.  We also retrieve the OpId of the first operation in the batch so that, if we roll</a>
<a name="ln858">  // over to a new segment, we set the first operation in the footer immediately.</a>
<a name="ln859">  // Update the index bounds for the current segment.</a>
<a name="ln860">  for (const LogEntryPB&amp; entry_pb : batch-&gt;entry_batch_pb_.entry()) {</a>
<a name="ln861">    int64_t index = entry_pb.replicate().id().index();</a>
<a name="ln862">    if (!footer_builder_.has_min_replicate_index() ||</a>
<a name="ln863">        index &lt; footer_builder_.min_replicate_index()) {</a>
<a name="ln864">      footer_builder_.set_min_replicate_index(index);</a>
<a name="ln865">      min_replicate_index_.store(index, std::memory_order_release);</a>
<a name="ln866">    }</a>
<a name="ln867">    if (!footer_builder_.has_max_replicate_index() ||</a>
<a name="ln868">        index &gt; footer_builder_.max_replicate_index()) {</a>
<a name="ln869">      footer_builder_.set_max_replicate_index(index);</a>
<a name="ln870">    }</a>
<a name="ln871">  }</a>
<a name="ln872">}</a>
<a name="ln873"> </a>
<a name="ln874">Status Log::AllocateSegmentAndRollOver() {</a>
<a name="ln875">  {</a>
<a name="ln876">    std::lock_guard&lt;std::mutex&gt; lock(allocation_mutex_);</a>
<a name="ln877">    RETURN_NOT_OK(AsyncAllocateSegment());</a>
<a name="ln878">  }</a>
<a name="ln879">  return RollOver();</a>
<a name="ln880">}</a>
<a name="ln881"> </a>
<a name="ln882">Status Log::EnsureInitialNewSegmentAllocated() {</a>
<a name="ln883">  if (log_state_ == LogState::kLogWriting) {</a>
<a name="ln884">    // New segment already created.</a>
<a name="ln885">    return Status::OK();</a>
<a name="ln886">  }</a>
<a name="ln887">  if (log_state_ != LogState::kLogInitialized) {</a>
<a name="ln888">    return STATUS_FORMAT(</a>
<a name="ln889">        IllegalState, &quot;Unexpected log state in EnsureInitialNewSegmentAllocated: $0&quot;, log_state_);</a>
<a name="ln890">  }</a>
<a name="ln891">  {</a>
<a name="ln892">    std::lock_guard&lt;std::mutex&gt; lock(allocation_mutex_);</a>
<a name="ln893">    RETURN_NOT_OK(AsyncAllocateSegment());</a>
<a name="ln894">  }</a>
<a name="ln895">  RETURN_NOT_OK(allocation_status_.Get());</a>
<a name="ln896">  RETURN_NOT_OK(SwitchToAllocatedSegment());</a>
<a name="ln897"> </a>
<a name="ln898">  RETURN_NOT_OK(appender_-&gt;Init());</a>
<a name="ln899">  log_state_ = LogState::kLogWriting;</a>
<a name="ln900">  return Status::OK();</a>
<a name="ln901">}</a>
<a name="ln902"> </a>
<a name="ln903">Status Log::Sync() {</a>
<a name="ln904">  TRACE_EVENT0(&quot;log&quot;, &quot;Sync&quot;);</a>
<a name="ln905">  SCOPED_LATENCY_METRIC(metrics_, sync_latency);</a>
<a name="ln906"> </a>
<a name="ln907">  if (!sync_disabled_) {</a>
<a name="ln908">    if (PREDICT_FALSE(GetAtomicFlag(&amp;FLAGS_log_inject_latency))) {</a>
<a name="ln909">      Random r(GetCurrentTimeMicros());</a>
<a name="ln910">      int sleep_ms = r.Normal(GetAtomicFlag(&amp;FLAGS_log_inject_latency_ms_mean),</a>
<a name="ln911">                              GetAtomicFlag(&amp;FLAGS_log_inject_latency_ms_stddev));</a>
<a name="ln912">      if (sleep_ms &gt; 0) {</a>
<a name="ln913">        LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Injecting &quot; &lt;&lt; sleep_ms &lt;&lt; &quot;ms of latency in Log::Sync()&quot;;</a>
<a name="ln914">        SleepFor(MonoDelta::FromMilliseconds(sleep_ms));</a>
<a name="ln915">      }</a>
<a name="ln916">    }</a>
<a name="ln917"> </a>
<a name="ln918">    bool timed_or_data_limit_sync = false;</a>
<a name="ln919">    if (!durable_wal_write_ &amp;&amp; periodic_sync_needed_.load()) {</a>
<a name="ln920">      if (interval_durable_wal_write_) {</a>
<a name="ln921">        if (MonoTime::Now() &gt; periodic_sync_earliest_unsync_entry_time_</a>
<a name="ln922">            + interval_durable_wal_write_) {</a>
<a name="ln923">          timed_or_data_limit_sync = true;</a>
<a name="ln924">        }</a>
<a name="ln925">      }</a>
<a name="ln926">      if (bytes_durable_wal_write_mb_ &gt; 0) {</a>
<a name="ln927">        if (periodic_sync_unsynced_bytes_ &gt;= bytes_durable_wal_write_mb_ * 1_MB) {</a>
<a name="ln928">          timed_or_data_limit_sync = true;</a>
<a name="ln929">        }</a>
<a name="ln930">      }</a>
<a name="ln931">    }</a>
<a name="ln932"> </a>
<a name="ln933">    if (durable_wal_write_ || timed_or_data_limit_sync) {</a>
<a name="ln934">      periodic_sync_needed_.store(false);</a>
<a name="ln935">      periodic_sync_unsynced_bytes_ = 0;</a>
<a name="ln936">      LOG_SLOW_EXECUTION(WARNING, 50, &quot;Fsync log took a long time&quot;) {</a>
<a name="ln937">        RETURN_NOT_OK(active_segment_-&gt;Sync());</a>
<a name="ln938">      }</a>
<a name="ln939">    }</a>
<a name="ln940">  }</a>
<a name="ln941"> </a>
<a name="ln942">  // Update the reader on how far it can read the active segment.</a>
<a name="ln943">  reader_-&gt;UpdateLastSegmentOffset(active_segment_-&gt;written_offset());</a>
<a name="ln944"> </a>
<a name="ln945">  {</a>
<a name="ln946">    std::lock_guard&lt;std::mutex&gt; write_lock(last_synced_entry_op_id_mutex_);</a>
<a name="ln947">    last_synced_entry_op_id_.store(last_appended_entry_op_id_, boost::memory_order_release);</a>
<a name="ln948">    last_synced_entry_op_id_cond_.notify_all();</a>
<a name="ln949">  }</a>
<a name="ln950"> </a>
<a name="ln951">  return Status::OK();</a>
<a name="ln952">}</a>
<a name="ln953"> </a>
<a name="ln954">Status Log::GetSegmentsToGCUnlocked(int64_t min_op_idx, SegmentSequence* segments_to_gc) const {</a>
<a name="ln955">  // Find the prefix of segments in the segment sequence that is guaranteed not to include</a>
<a name="ln956">  // 'min_op_idx'.</a>
<a name="ln957">  RETURN_NOT_OK(reader_-&gt;GetSegmentPrefixNotIncluding(</a>
<a name="ln958">      min_op_idx, cdc_min_replicated_index_.load(std::memory_order_acquire), segments_to_gc));</a>
<a name="ln959"> </a>
<a name="ln960">  int max_to_delete = std::max(reader_-&gt;num_segments() - FLAGS_log_min_segments_to_retain, 0);</a>
<a name="ln961">  if (segments_to_gc-&gt;size() &gt; max_to_delete) {</a>
<a name="ln962">    VLOG_WITH_PREFIX(2)</a>
<a name="ln963">        &lt;&lt; &quot;GCing &quot; &lt;&lt; segments_to_gc-&gt;size() &lt;&lt; &quot; in &quot; &lt;&lt; wal_dir_</a>
<a name="ln964">        &lt;&lt; &quot; would not leave enough remaining segments to satisfy minimum &quot;</a>
<a name="ln965">        &lt;&lt; &quot;retention requirement. Only considering &quot;</a>
<a name="ln966">        &lt;&lt; max_to_delete &lt;&lt; &quot;/&quot; &lt;&lt; reader_-&gt;num_segments();</a>
<a name="ln967">    segments_to_gc-&gt;resize(max_to_delete);</a>
<a name="ln968">  } else if (segments_to_gc-&gt;size() &lt; max_to_delete) {</a>
<a name="ln969">    int extra_segments = max_to_delete - segments_to_gc-&gt;size();</a>
<a name="ln970">    VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Too many log segments, need to GC &quot; &lt;&lt; extra_segments &lt;&lt; &quot; more.&quot;;</a>
<a name="ln971">  }</a>
<a name="ln972"> </a>
<a name="ln973">  // Don't GC segments that are newer than the configured time-based retention.</a>
<a name="ln974">  int64_t now = GetCurrentTimeMicros();</a>
<a name="ln975">  for (int i = 0; i &lt; segments_to_gc-&gt;size(); i++) {</a>
<a name="ln976">    const scoped_refptr&lt;ReadableLogSegment&gt;&amp; segment = (*segments_to_gc)[i];</a>
<a name="ln977"> </a>
<a name="ln978">    // Segments here will always have a footer, since we don't return the in-progress segment up</a>
<a name="ln979">    // above. However, segments written by older YB builds may not have the timestamp info (TODO:</a>
<a name="ln980">    // make sure we indeed care about these old builds). In that case, we're allowed to GC them.</a>
<a name="ln981">    if (!segment-&gt;footer().has_close_timestamp_micros()) continue;</a>
<a name="ln982"> </a>
<a name="ln983">    int64_t age_seconds = (now - segment-&gt;footer().close_timestamp_micros()) / 1000000;</a>
<a name="ln984">    if (age_seconds &lt; wal_retention_secs()) {</a>
<a name="ln985">      VLOG_WITH_PREFIX(2)</a>
<a name="ln986">          &lt;&lt; &quot;Segment &quot; &lt;&lt; segment-&gt;path() &lt;&lt; &quot; is only &quot; &lt;&lt; age_seconds &lt;&lt; &quot;s old: &quot;</a>
<a name="ln987">          &lt;&lt; &quot;cannot GC it yet due to configured time-based retention policy.&quot;;</a>
<a name="ln988">      // Truncate the list of segments to GC here -- if this one is too new, then all later ones are</a>
<a name="ln989">      // also too new.</a>
<a name="ln990">      segments_to_gc-&gt;resize(i);</a>
<a name="ln991">      break;</a>
<a name="ln992">    }</a>
<a name="ln993">  }</a>
<a name="ln994"> </a>
<a name="ln995">  return Status::OK();</a>
<a name="ln996">}</a>
<a name="ln997"> </a>
<a name="ln998">Status Log::Append(LogEntryPB* phys_entry,</a>
<a name="ln999">                   LogEntryMetadata entry_metadata,</a>
<a name="ln1000">                   bool skip_wal_write) {</a>
<a name="ln1001">  LogEntryBatchPB entry_batch_pb;</a>
<a name="ln1002">  if (entry_metadata.entry_time != RestartSafeCoarseTimePoint()) {</a>
<a name="ln1003">    entry_batch_pb.set_mono_time(entry_metadata.entry_time.ToUInt64());</a>
<a name="ln1004">  }</a>
<a name="ln1005"> </a>
<a name="ln1006">  entry_batch_pb.mutable_entry()-&gt;AddAllocated(phys_entry);</a>
<a name="ln1007">  LogEntryBatch entry_batch(phys_entry-&gt;type(), std::move(entry_batch_pb));</a>
<a name="ln1008">  // Mark this as reserved, as we're building it from preallocated data.</a>
<a name="ln1009">  entry_batch.state_ = LogEntryBatch::kEntryReserved;</a>
<a name="ln1010">  // Ready assumes the data is reserved before it is ready.</a>
<a name="ln1011">  entry_batch.MarkReady();</a>
<a name="ln1012">  if (skip_wal_write) {</a>
<a name="ln1013">    // Get the LogIndex entry from read path metadata.</a>
<a name="ln1014">    entry_batch.offset_ = entry_metadata.offset;</a>
<a name="ln1015">    entry_batch.active_segment_sequence_number_ = entry_metadata.active_segment_sequence_number;</a>
<a name="ln1016">  }</a>
<a name="ln1017">  Status s = DoAppend(&amp;entry_batch, false, skip_wal_write);</a>
<a name="ln1018">  if (s.ok() &amp;&amp; !skip_wal_write) {</a>
<a name="ln1019">    // Only sync if we actually performed a wal write.</a>
<a name="ln1020">    s = Sync();</a>
<a name="ln1021">  }</a>
<a name="ln1022">  entry_batch.entry_batch_pb_.mutable_entry()-&gt;ExtractSubrange(0, 1, nullptr);</a>
<a name="ln1023">  return s;</a>
<a name="ln1024">}</a>
<a name="ln1025"> </a>
<a name="ln1026">Status Log::WaitUntilAllFlushed() {</a>
<a name="ln1027">  // In order to make sure we empty the queue we need to use the async API.</a>
<a name="ln1028">  LogEntryBatchPB entry_batch;</a>
<a name="ln1029">  entry_batch.add_entry()-&gt;set_type(log::FLUSH_MARKER);</a>
<a name="ln1030">  LogEntryBatch* reserved_entry_batch;</a>
<a name="ln1031">  RETURN_NOT_OK(Reserve(FLUSH_MARKER, &amp;entry_batch, &amp;reserved_entry_batch));</a>
<a name="ln1032">  Synchronizer s;</a>
<a name="ln1033">  RETURN_NOT_OK(AsyncAppend(reserved_entry_batch, s.AsStatusCallback()));</a>
<a name="ln1034">  return s.Wait();</a>
<a name="ln1035">}</a>
<a name="ln1036"> </a>
<a name="ln1037">void Log::set_wal_retention_secs(uint32_t wal_retention_secs) {</a>
<a name="ln1038">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Setting log wal retention time to &quot; &lt;&lt; wal_retention_secs &lt;&lt; &quot; seconds&quot;;</a>
<a name="ln1039">  wal_retention_secs_.store(wal_retention_secs, std::memory_order_release);</a>
<a name="ln1040">}</a>
<a name="ln1041"> </a>
<a name="ln1042">uint32_t Log::wal_retention_secs() const {</a>
<a name="ln1043">  uint32_t wal_retention_secs = wal_retention_secs_.load(std::memory_order_acquire);</a>
<a name="ln1044">  auto flag_wal_retention = ANNOTATE_UNPROTECTED_READ(FLAGS_log_min_seconds_to_retain);</a>
<a name="ln1045">  return flag_wal_retention &gt; 0 ?</a>
<a name="ln1046">      std::max(wal_retention_secs, static_cast&lt;uint32_t&gt;(flag_wal_retention)) :</a>
<a name="ln1047">      wal_retention_secs;</a>
<a name="ln1048">}</a>
<a name="ln1049"> </a>
<a name="ln1050">yb::OpId Log::GetLatestEntryOpId() const {</a>
<a name="ln1051">  return last_synced_entry_op_id_.load(boost::memory_order_acquire);</a>
<a name="ln1052">}</a>
<a name="ln1053"> </a>
<a name="ln1054">int64_t Log::GetMinReplicateIndex() const {</a>
<a name="ln1055">  return min_replicate_index_.load(std::memory_order_acquire);</a>
<a name="ln1056">}</a>
<a name="ln1057"> </a>
<a name="ln1058">yb::OpId Log::WaitForSafeOpIdToApply(const yb::OpId&amp; min_allowed, MonoDelta duration) {</a>
<a name="ln1059">  if (FLAGS_TEST_log_consider_all_ops_safe || all_op_ids_safe_) {</a>
<a name="ln1060">    return min_allowed;</a>
<a name="ln1061">  }</a>
<a name="ln1062"> </a>
<a name="ln1063">  auto result = last_synced_entry_op_id_.load(boost::memory_order_acquire);</a>
<a name="ln1064"> </a>
<a name="ln1065">  if (result &lt; min_allowed) {</a>
<a name="ln1066">    auto start = CoarseMonoClock::Now();</a>
<a name="ln1067">    std::unique_lock&lt;std::mutex&gt; lock(last_synced_entry_op_id_mutex_);</a>
<a name="ln1068">    auto wait_time = duration ? duration.ToSteadyDuration()</a>
<a name="ln1069">                              : FLAGS_wait_for_safe_op_id_to_apply_default_timeout_ms * 1ms;</a>
<a name="ln1070">    for (;;) {</a>
<a name="ln1071">      if (last_synced_entry_op_id_cond_.wait_for(</a>
<a name="ln1072">              lock, wait_time, [this, min_allowed, &amp;result] {</a>
<a name="ln1073">            result = last_synced_entry_op_id_.load(boost::memory_order_acquire);</a>
<a name="ln1074">            return result &gt;= min_allowed;</a>
<a name="ln1075">      })) {</a>
<a name="ln1076">        break;</a>
<a name="ln1077">      }</a>
<a name="ln1078">      if (duration) {</a>
<a name="ln1079">        return yb::OpId();</a>
<a name="ln1080">      }</a>
<a name="ln1081">      // TODO(bogdan): If the log is closed at this point, consider refactoring to return status</a>
<a name="ln1082">      // and fail cleanly.</a>
<a name="ln1083">      LOG_WITH_PREFIX(ERROR) &lt;&lt; &quot;Appender stack: &quot; &lt;&lt; appender_-&gt;GetRunThreadStack();</a>
<a name="ln1084">      LOG_WITH_PREFIX(DFATAL)</a>
<a name="ln1085">          &lt;&lt; &quot;Long wait for safe op id: &quot; &lt;&lt; min_allowed</a>
<a name="ln1086">          &lt;&lt; &quot;, current: &quot; &lt;&lt; GetLatestEntryOpId()</a>
<a name="ln1087">          &lt;&lt; &quot;, last appended: &quot; &lt;&lt; last_appended_entry_op_id_</a>
<a name="ln1088">          &lt;&lt; &quot;, last submitted: &quot; &lt;&lt; last_submitted_op_id_</a>
<a name="ln1089">          &lt;&lt; &quot;, appender: &quot; &lt;&lt; appender_-&gt;ToString()</a>
<a name="ln1090">          &lt;&lt; &quot;, passed: &quot; &lt;&lt; (CoarseMonoClock::Now() - start);</a>
<a name="ln1091">    }</a>
<a name="ln1092">  }</a>
<a name="ln1093"> </a>
<a name="ln1094">  DCHECK_GE(result.term, min_allowed.term)</a>
<a name="ln1095">      &lt;&lt; &quot;result: &quot; &lt;&lt; result &lt;&lt; &quot;, min_allowed: &quot; &lt;&lt; min_allowed;</a>
<a name="ln1096">  return result;</a>
<a name="ln1097">}</a>
<a name="ln1098"> </a>
<a name="ln1099">Status Log::GC(int64_t min_op_idx, int32_t* num_gced) {</a>
<a name="ln1100">  CHECK_GE(min_op_idx, 0);</a>
<a name="ln1101"> </a>
<a name="ln1102">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Running Log GC on &quot; &lt;&lt; wal_dir_ &lt;&lt; &quot;: retaining ops &gt;= &quot; &lt;&lt; min_op_idx</a>
<a name="ln1103">                        &lt;&lt; &quot;, log segment size = &quot; &lt;&lt; options_.segment_size_bytes;</a>
<a name="ln1104">  VLOG_TIMING(1, &quot;Log GC&quot;) {</a>
<a name="ln1105">    SegmentSequence segments_to_delete;</a>
<a name="ln1106"> </a>
<a name="ln1107">    {</a>
<a name="ln1108">      std::lock_guard&lt;percpu_rwlock&gt; l(state_lock_);</a>
<a name="ln1109">      CHECK_EQ(kLogWriting, log_state_);</a>
<a name="ln1110"> </a>
<a name="ln1111">      RETURN_NOT_OK(GetSegmentsToGCUnlocked(min_op_idx, &amp;segments_to_delete));</a>
<a name="ln1112"> </a>
<a name="ln1113">      if (segments_to_delete.size() == 0) {</a>
<a name="ln1114">        VLOG_WITH_PREFIX(1) &lt;&lt; &quot;No segments to delete.&quot;;</a>
<a name="ln1115">        *num_gced = 0;</a>
<a name="ln1116">        return Status::OK();</a>
<a name="ln1117">      }</a>
<a name="ln1118">      // Trim the prefix of segments from the reader so that they are no longer referenced by the</a>
<a name="ln1119">      // log.</a>
<a name="ln1120">      RETURN_NOT_OK(reader_-&gt;TrimSegmentsUpToAndIncluding(</a>
<a name="ln1121">          segments_to_delete[segments_to_delete.size() - 1]-&gt;header().sequence_number()));</a>
<a name="ln1122">    }</a>
<a name="ln1123"> </a>
<a name="ln1124">    // Now that they are no longer referenced by the Log, delete the files.</a>
<a name="ln1125">    *num_gced = 0;</a>
<a name="ln1126">    for (const scoped_refptr&lt;ReadableLogSegment&gt;&amp; segment : segments_to_delete) {</a>
<a name="ln1127">      LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Deleting log segment in path: &quot; &lt;&lt; segment-&gt;path()</a>
<a name="ln1128">                            &lt;&lt; &quot; (GCed ops &lt; &quot; &lt;&lt; min_op_idx &lt;&lt; &quot;)&quot;;</a>
<a name="ln1129">      RETURN_NOT_OK(get_env()-&gt;DeleteFile(segment-&gt;path()));</a>
<a name="ln1130">      (*num_gced)++;</a>
<a name="ln1131">    }</a>
<a name="ln1132"> </a>
<a name="ln1133">    // Determine the minimum remaining replicate index in order to properly GC the index chunks.</a>
<a name="ln1134">    int64_t min_remaining_op_idx = reader_-&gt;GetMinReplicateIndex();</a>
<a name="ln1135">    if (min_remaining_op_idx &gt; 0) {</a>
<a name="ln1136">      log_index_-&gt;GC(min_remaining_op_idx);</a>
<a name="ln1137">    }</a>
<a name="ln1138">  }</a>
<a name="ln1139">  return Status::OK();</a>
<a name="ln1140">}</a>
<a name="ln1141"> </a>
<a name="ln1142">Status Log::GetGCableDataSize(int64_t min_op_idx, int64_t* total_size) const {</a>
<a name="ln1143">  if (min_op_idx &lt; 0) {</a>
<a name="ln1144">    return STATUS_FORMAT(InvalidArgument, &quot;Invalid min op index $0&quot;, min_op_idx);</a>
<a name="ln1145">  }</a>
<a name="ln1146"> </a>
<a name="ln1147">  SegmentSequence segments_to_delete;</a>
<a name="ln1148">  *total_size = 0;</a>
<a name="ln1149">  {</a>
<a name="ln1150">    SharedLock&lt;rw_spinlock&gt; read_lock(state_lock_.get_lock());</a>
<a name="ln1151">    if (log_state_ != kLogWriting) {</a>
<a name="ln1152">      return STATUS_FORMAT(IllegalState, &quot;Invalid log state $0, expected $1&quot;,</a>
<a name="ln1153">          log_state_, kLogWriting);</a>
<a name="ln1154">    }</a>
<a name="ln1155">    Status s = GetSegmentsToGCUnlocked(min_op_idx, &amp;segments_to_delete);</a>
<a name="ln1156"> </a>
<a name="ln1157">    if (!s.ok() || segments_to_delete.size() == 0) {</a>
<a name="ln1158">      return Status::OK();</a>
<a name="ln1159">    }</a>
<a name="ln1160">  }</a>
<a name="ln1161">  for (const scoped_refptr&lt;ReadableLogSegment&gt;&amp; segment : segments_to_delete) {</a>
<a name="ln1162">    *total_size += segment-&gt;file_size();</a>
<a name="ln1163">  }</a>
<a name="ln1164">  return Status::OK();</a>
<a name="ln1165">}</a>
<a name="ln1166"> </a>
<a name="ln1167">void Log::GetMaxIndexesToSegmentSizeMap(int64_t min_op_idx,</a>
<a name="ln1168">                                        std::map&lt;int64_t, int64_t&gt;* max_idx_to_segment_size)</a>
<a name="ln1169">                                        const {</a>
<a name="ln1170">  SharedLock&lt;rw_spinlock&gt; read_lock(state_lock_.get_lock());</a>
<a name="ln1171">  CHECK_EQ(kLogWriting, log_state_);</a>
<a name="ln1172">  // We want to retain segments so we're only asking the extra ones.</a>
<a name="ln1173">  int segments_count = std::max(reader_-&gt;num_segments() - FLAGS_log_min_segments_to_retain, 0);</a>
<a name="ln1174">  if (segments_count == 0) {</a>
<a name="ln1175">    return;</a>
<a name="ln1176">  }</a>
<a name="ln1177"> </a>
<a name="ln1178">  int64_t now = GetCurrentTimeMicros();</a>
<a name="ln1179">  int64_t max_close_time_us = now - (wal_retention_secs() * 1000000);</a>
<a name="ln1180">  reader_-&gt;GetMaxIndexesToSegmentSizeMap(min_op_idx, segments_count, max_close_time_us,</a>
<a name="ln1181">                                         max_idx_to_segment_size);</a>
<a name="ln1182">}</a>
<a name="ln1183"> </a>
<a name="ln1184">LogReader* Log::GetLogReader() const {</a>
<a name="ln1185">  return reader_.get();</a>
<a name="ln1186">}</a>
<a name="ln1187"> </a>
<a name="ln1188">Status Log::GetSegmentsSnapshot(SegmentSequence* segments) const {</a>
<a name="ln1189">  SharedLock&lt;rw_spinlock&gt; read_lock(state_lock_.get_lock());</a>
<a name="ln1190">  if (!reader_) {</a>
<a name="ln1191">    return STATUS(IllegalState, &quot;Log already closed&quot;);</a>
<a name="ln1192">  }</a>
<a name="ln1193"> </a>
<a name="ln1194">  return reader_-&gt;GetSegmentsSnapshot(segments);</a>
<a name="ln1195">}</a>
<a name="ln1196"> </a>
<a name="ln1197">uint64_t Log::OnDiskSize() {</a>
<a name="ln1198">  SegmentSequence segments;</a>
<a name="ln1199">  {</a>
<a name="ln1200">    shared_lock&lt;rw_spinlock&gt; l(state_lock_.get_lock());</a>
<a name="ln1201">    // If the log is closed, the tablet is either being deleted or tombstoned,</a>
<a name="ln1202">    // so we don't count the size of its log anymore as it should be deleted.</a>
<a name="ln1203">    if (log_state_ == kLogClosed || !reader_-&gt;GetSegmentsSnapshot(&amp;segments).ok()) {</a>
<a name="ln1204">      return on_disk_size_.load();</a>
<a name="ln1205">    }</a>
<a name="ln1206">  }</a>
<a name="ln1207">  uint64_t ret = 0;</a>
<a name="ln1208">  for (const auto&amp; segment : segments) {</a>
<a name="ln1209">    ret += segment-&gt;file_size();</a>
<a name="ln1210">  }</a>
<a name="ln1211"> </a>
<a name="ln1212">  on_disk_size_.store(ret, std::memory_order_release);</a>
<a name="ln1213">  return ret;</a>
<a name="ln1214">}</a>
<a name="ln1215"> </a>
<a name="ln1216">void Log::SetSchemaForNextLogSegment(const Schema&amp; schema,</a>
<a name="ln1217">                                     uint32_t version) {</a>
<a name="ln1218">  std::lock_guard&lt;rw_spinlock&gt; l(schema_lock_);</a>
<a name="ln1219">  schema_ = schema;</a>
<a name="ln1220">  schema_version_ = version;</a>
<a name="ln1221">}</a>
<a name="ln1222"> </a>
<a name="ln1223">Status Log::Close() {</a>
<a name="ln1224">  // Allocation pool is used from appender pool, so we should shutdown appender first.</a>
<a name="ln1225">  appender_-&gt;Shutdown();</a>
<a name="ln1226">  allocation_token_.reset();</a>
<a name="ln1227"> </a>
<a name="ln1228">  std::lock_guard&lt;percpu_rwlock&gt; l(state_lock_);</a>
<a name="ln1229">  switch (log_state_) {</a>
<a name="ln1230">    case kLogWriting:</a>
<a name="ln1231">      RETURN_NOT_OK(Sync());</a>
<a name="ln1232">      RETURN_NOT_OK(CloseCurrentSegment());</a>
<a name="ln1233">      RETURN_NOT_OK(ReplaceSegmentInReaderUnlocked());</a>
<a name="ln1234">      log_state_ = kLogClosed;</a>
<a name="ln1235">      VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Log closed&quot;;</a>
<a name="ln1236"> </a>
<a name="ln1237">      // Release FDs held by these objects.</a>
<a name="ln1238">      log_index_.reset();</a>
<a name="ln1239">      reader_.reset();</a>
<a name="ln1240"> </a>
<a name="ln1241">      return Status::OK();</a>
<a name="ln1242"> </a>
<a name="ln1243">    case kLogClosed:</a>
<a name="ln1244">      VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Log already closed&quot;;</a>
<a name="ln1245">      return Status::OK();</a>
<a name="ln1246"> </a>
<a name="ln1247">    default:</a>
<a name="ln1248">      return STATUS(IllegalState, Substitute(&quot;Bad state for Close() $0&quot;, log_state_));</a>
<a name="ln1249">  }</a>
<a name="ln1250">}</a>
<a name="ln1251"> </a>
<a name="ln1252">const int Log::num_segments() const {</a>
<a name="ln1253">  boost::shared_lock&lt;rw_spinlock&gt; read_lock(state_lock_.get_lock());</a>
<a name="ln1254">  return (reader_) ? reader_-&gt;num_segments() : 0;</a>
<a name="ln1255">}</a>
<a name="ln1256"> </a>
<a name="ln1257">scoped_refptr&lt;ReadableLogSegment&gt; Log::GetSegmentBySequenceNumber(int64_t seq) const {</a>
<a name="ln1258">  SharedLock&lt;rw_spinlock&gt; read_lock(state_lock_.get_lock());</a>
<a name="ln1259">  if (!reader_) {</a>
<a name="ln1260">    return nullptr;</a>
<a name="ln1261">  }</a>
<a name="ln1262"> </a>
<a name="ln1263">  return reader_-&gt;GetSegmentBySequenceNumber(seq);</a>
<a name="ln1264">}</a>
<a name="ln1265"> </a>
<a name="ln1266">bool Log::HasOnDiskData(FsManager* fs_manager, const string&amp; wal_dir) {</a>
<a name="ln1267">  return fs_manager-&gt;env()-&gt;FileExists(wal_dir);</a>
<a name="ln1268">}</a>
<a name="ln1269"> </a>
<a name="ln1270">Status Log::DeleteOnDiskData(Env* env,</a>
<a name="ln1271">                             const string&amp; tablet_id,</a>
<a name="ln1272">                             const string&amp; wal_dir,</a>
<a name="ln1273">                             const string&amp; peer_uuid) {</a>
<a name="ln1274">  if (!env-&gt;FileExists(wal_dir)) {</a>
<a name="ln1275">    return Status::OK();</a>
<a name="ln1276">  }</a>
<a name="ln1277">  LOG(INFO) &lt;&lt; &quot;T &quot; &lt;&lt; tablet_id &lt;&lt; &quot;P &quot; &lt;&lt; peer_uuid</a>
<a name="ln1278">            &lt;&lt; &quot;: Deleting WAL dir &quot; &lt;&lt; wal_dir;</a>
<a name="ln1279">  RETURN_NOT_OK_PREPEND(env-&gt;DeleteRecursively(wal_dir),</a>
<a name="ln1280">                        &quot;Unable to recursively delete WAL dir for tablet &quot; + tablet_id);</a>
<a name="ln1281">  return Status::OK();</a>
<a name="ln1282">}</a>
<a name="ln1283"> </a>
<a name="ln1284">Status Log::FlushIndex() {</a>
<a name="ln1285">  if (!log_index_) {</a>
<a name="ln1286">    return Status::OK();</a>
<a name="ln1287">  }</a>
<a name="ln1288">  return log_index_-&gt;Flush();</a>
<a name="ln1289">}</a>
<a name="ln1290"> </a>
<a name="ln1291">Status Log::CopyTo(const std::string&amp; dest_wal_dir) {</a>
<a name="ln1292">  RETURN_NOT_OK_PREPEND(env_util::CreateDirIfMissing(options_.env, dest_wal_dir),</a>
<a name="ln1293">                        Format(&quot;Failed to create tablet WAL dir $0&quot;, dest_wal_dir));</a>
<a name="ln1294">  // Make sure log segments we have so far are immutable, so we can hardlink them instead of</a>
<a name="ln1295">  // copying.</a>
<a name="ln1296">  if (footer_builder_.IsInitialized() &amp;&amp; footer_builder_.num_entries() &gt; 0) {</a>
<a name="ln1297">    // If active log segment has entries - close it and rollover to next one, so this one become</a>
<a name="ln1298">    // immutable. If active log segment empty - we will just skip it.</a>
<a name="ln1299">    RETURN_NOT_OK(AllocateSegmentAndRollOver());</a>
<a name="ln1300">  }</a>
<a name="ln1301">  RETURN_NOT_OK(log_index_-&gt;Flush());</a>
<a name="ln1302"> </a>
<a name="ln1303">  auto* const env = options_.env;</a>
<a name="ln1304">  const auto files = VERIFY_RESULT(env-&gt;GetChildren(wal_dir_, ExcludeDots::kTrue));</a>
<a name="ln1305"> </a>
<a name="ln1306">  const auto active_segment_filename =</a>
<a name="ln1307">      FsManager::GetWalSegmentFileName(active_segment_sequence_number_);</a>
<a name="ln1308"> </a>
<a name="ln1309">  for (const auto&amp; file : files) {</a>
<a name="ln1310">    const auto src_path = JoinPathSegments(wal_dir_, file);</a>
<a name="ln1311">    const auto dest_path = JoinPathSegments(dest_wal_dir, file);</a>
<a name="ln1312"> </a>
<a name="ln1313">    // Segment files except the active one are immutable, so we can use hardlinks.</a>
<a name="ln1314">    if (file == active_segment_filename) {</a>
<a name="ln1315">      // Skip active segment file, because we've just rolled over to it and it is empty and not</a>
<a name="ln1316">      // closed.</a>
<a name="ln1317">      continue;</a>
<a name="ln1318">    } else if (FsManager::IsWalSegmentFileName(file)) {</a>
<a name="ln1319">      RETURN_NOT_OK(env-&gt;LinkFile(src_path, dest_path));</a>
<a name="ln1320">      VLOG_WITH_PREFIX(1) &lt;&lt; Format(&quot;Hard linked $0 to $1&quot;, src_path, dest_path);</a>
<a name="ln1321">    } else {</a>
<a name="ln1322">      RETURN_NOT_OK_PREPEND(</a>
<a name="ln1323">          CopyFile(env, src_path, dest_path),</a>
<a name="ln1324">          Format(&quot;Failed to copy file $0 to $1&quot;, src_path, dest_path));</a>
<a name="ln1325">      VLOG_WITH_PREFIX(1) &lt;&lt; Format(&quot;Copied $0 to $1&quot;, src_path, dest_path);</a>
<a name="ln1326">    }</a>
<a name="ln1327">  }</a>
<a name="ln1328">  return Status::OK();</a>
<a name="ln1329">}</a>
<a name="ln1330"> </a>
<a name="ln1331">uint64_t Log::NextSegmentDesiredSize() {</a>
<a name="ln1332">  return std::min(cur_max_segment_size_ * 2, max_segment_size_);</a>
<a name="ln1333">}</a>
<a name="ln1334"> </a>
<a name="ln1335">Status Log::PreAllocateNewSegment() {</a>
<a name="ln1336">  TRACE_EVENT1(&quot;log&quot;, &quot;PreAllocateNewSegment&quot;, &quot;file&quot;, next_segment_path_);</a>
<a name="ln1337">  CHECK_EQ(allocation_state(), kAllocationInProgress);</a>
<a name="ln1338"> </a>
<a name="ln1339">  WritableFileOptions opts;</a>
<a name="ln1340">  // We always want to sync on close: https://github.com/yugabyte/yugabyte-db/issues/3490</a>
<a name="ln1341">  opts.sync_on_close = true;</a>
<a name="ln1342">  opts.o_direct = durable_wal_write_;</a>
<a name="ln1343">  RETURN_NOT_OK(CreatePlaceholderSegment(opts, &amp;next_segment_path_, &amp;next_segment_file_));</a>
<a name="ln1344"> </a>
<a name="ln1345">  if (options_.preallocate_segments) {</a>
<a name="ln1346">    uint64_t next_segment_size = NextSegmentDesiredSize();</a>
<a name="ln1347">    TRACE(&quot;Preallocating $0 byte segment in $1&quot;, next_segment_size, next_segment_path_);</a>
<a name="ln1348">    // TODO (perf) zero the new segments -- this could result in additional performance</a>
<a name="ln1349">    // improvements.</a>
<a name="ln1350">    RETURN_NOT_OK(next_segment_file_-&gt;PreAllocate(next_segment_size));</a>
<a name="ln1351">  }</a>
<a name="ln1352"> </a>
<a name="ln1353">  {</a>
<a name="ln1354">    std::lock_guard&lt;std::mutex&gt; lock(allocation_mutex_);</a>
<a name="ln1355">    // We implement something like shared lock for allocation_state_, so modifications should be</a>
<a name="ln1356">    // done while holding the mutex.</a>
<a name="ln1357">    allocation_state_.store(kAllocationFinished, std::memory_order_release);</a>
<a name="ln1358">  }</a>
<a name="ln1359">  return Status::OK();</a>
<a name="ln1360">}</a>
<a name="ln1361"> </a>
<a name="ln1362">Status Log::SwitchToAllocatedSegment() {</a>
<a name="ln1363">  CHECK_EQ(allocation_state(), kAllocationFinished);</a>
<a name="ln1364"> </a>
<a name="ln1365">  // Increment &quot;next&quot; log segment seqno.</a>
<a name="ln1366">  active_segment_sequence_number_++;</a>
<a name="ln1367">  const string new_segment_path =</a>
<a name="ln1368">      FsManager::GetWalSegmentFilePath(wal_dir_, active_segment_sequence_number_);</a>
<a name="ln1369"> </a>
<a name="ln1370">  RETURN_NOT_OK(get_env()-&gt;RenameFile(next_segment_path_, new_segment_path));</a>
<a name="ln1371">  RETURN_NOT_OK(get_env()-&gt;SyncDir(wal_dir_));</a>
<a name="ln1372"> </a>
<a name="ln1373">  // Create a new segment.</a>
<a name="ln1374">  std::unique_ptr&lt;WritableLogSegment&gt; new_segment(</a>
<a name="ln1375">      new WritableLogSegment(new_segment_path, next_segment_file_));</a>
<a name="ln1376"> </a>
<a name="ln1377">  // Set up the new header and footer.</a>
<a name="ln1378">  LogSegmentHeaderPB header;</a>
<a name="ln1379">  header.set_major_version(kLogMajorVersion);</a>
<a name="ln1380">  header.set_minor_version(kLogMinorVersion);</a>
<a name="ln1381">  header.set_sequence_number(active_segment_sequence_number_);</a>
<a name="ln1382">  header.set_tablet_id(tablet_id_);</a>
<a name="ln1383"> </a>
<a name="ln1384">  // Set up the new footer. This will be maintained as the segment is written.</a>
<a name="ln1385">  footer_builder_.Clear();</a>
<a name="ln1386">  footer_builder_.set_num_entries(0);</a>
<a name="ln1387"> </a>
<a name="ln1388">  // Set the new segment's schema.</a>
<a name="ln1389">  {</a>
<a name="ln1390">    SharedLock&lt;decltype(schema_lock_)&gt; l(schema_lock_);</a>
<a name="ln1391">    SchemaToPB(schema_, header.mutable_schema());</a>
<a name="ln1392">    header.set_schema_version(schema_version_);</a>
<a name="ln1393">  }</a>
<a name="ln1394"> </a>
<a name="ln1395">  RETURN_NOT_OK(new_segment-&gt;WriteHeaderAndOpen(header));</a>
<a name="ln1396">  // Transform the currently-active segment into a readable one, since we need to be able to replay</a>
<a name="ln1397">  // the segments for other peers.</a>
<a name="ln1398">  {</a>
<a name="ln1399">    if (active_segment_.get() != nullptr) {</a>
<a name="ln1400">      std::lock_guard&lt;decltype(state_lock_)&gt; l(state_lock_);</a>
<a name="ln1401">      CHECK_OK(ReplaceSegmentInReaderUnlocked());</a>
<a name="ln1402">    }</a>
<a name="ln1403">  }</a>
<a name="ln1404"> </a>
<a name="ln1405">  // Open the segment we just created in readable form and add it to the reader.</a>
<a name="ln1406">  std::unique_ptr&lt;RandomAccessFile&gt; readable_file;</a>
<a name="ln1407">  RETURN_NOT_OK(get_env()-&gt;NewRandomAccessFile(new_segment_path, &amp;readable_file));</a>
<a name="ln1408"> </a>
<a name="ln1409">  scoped_refptr&lt;ReadableLogSegment&gt; readable_segment(</a>
<a name="ln1410">    new ReadableLogSegment(new_segment_path,</a>
<a name="ln1411">                           shared_ptr&lt;RandomAccessFile&gt;(readable_file.release())));</a>
<a name="ln1412">  RETURN_NOT_OK(readable_segment-&gt;Init(header, new_segment-&gt;first_entry_offset()));</a>
<a name="ln1413">  RETURN_NOT_OK(reader_-&gt;AppendEmptySegment(readable_segment));</a>
<a name="ln1414"> </a>
<a name="ln1415">  // Now set 'active_segment_' to the new segment.</a>
<a name="ln1416">  active_segment_.reset(new_segment.release());</a>
<a name="ln1417">  cur_max_segment_size_ = NextSegmentDesiredSize();</a>
<a name="ln1418"> </a>
<a name="ln1419">  {</a>
<a name="ln1420">    std::lock_guard&lt;decltype(allocation_mutex_)&gt; lock_guard(allocation_mutex_);</a>
<a name="ln1421">    allocation_state_.store(kAllocationNotStarted, std::memory_order_release);</a>
<a name="ln1422">  }</a>
<a name="ln1423">  // Notify roll over waiters.</a>
<a name="ln1424">  allocation_cond_.notify_all();</a>
<a name="ln1425"> </a>
<a name="ln1426">  return Status::OK();</a>
<a name="ln1427">}</a>
<a name="ln1428"> </a>
<a name="ln1429">Status Log::ReplaceSegmentInReaderUnlocked() {</a>
<a name="ln1430">  // We should never switch to a new segment if we wrote nothing to the old one.</a>
<a name="ln1431">  CHECK(active_segment_-&gt;IsClosed());</a>
<a name="ln1432">  shared_ptr&lt;RandomAccessFile&gt; readable_file;</a>
<a name="ln1433">  RETURN_NOT_OK(OpenFileForRandom(</a>
<a name="ln1434">      get_env(), active_segment_-&gt;path(), &amp;readable_file));</a>
<a name="ln1435"> </a>
<a name="ln1436">  scoped_refptr&lt;ReadableLogSegment&gt; readable_segment(</a>
<a name="ln1437">      new ReadableLogSegment(active_segment_-&gt;path(), readable_file));</a>
<a name="ln1438">  // Note: active_segment_-&gt;header() will only contain an initialized PB if we wrote the header out.</a>
<a name="ln1439">  RETURN_NOT_OK(readable_segment-&gt;Init(active_segment_-&gt;header(),</a>
<a name="ln1440">                                       active_segment_-&gt;footer(),</a>
<a name="ln1441">                                       active_segment_-&gt;first_entry_offset()));</a>
<a name="ln1442"> </a>
<a name="ln1443">  return reader_-&gt;ReplaceLastSegment(readable_segment);</a>
<a name="ln1444">}</a>
<a name="ln1445"> </a>
<a name="ln1446">Status Log::CreatePlaceholderSegment(const WritableFileOptions&amp; opts,</a>
<a name="ln1447">                                     string* result_path,</a>
<a name="ln1448">                                     shared_ptr&lt;WritableFile&gt;* out) {</a>
<a name="ln1449">  string path_tmpl = JoinPathSegments(wal_dir_, kSegmentPlaceholderFileTemplate);</a>
<a name="ln1450">  VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Creating temp. file for place holder segment, template: &quot; &lt;&lt; path_tmpl;</a>
<a name="ln1451">  std::unique_ptr&lt;WritableFile&gt; segment_file;</a>
<a name="ln1452">  RETURN_NOT_OK(get_env()-&gt;NewTempWritableFile(opts,</a>
<a name="ln1453">                                               path_tmpl,</a>
<a name="ln1454">                                               result_path,</a>
<a name="ln1455">                                               &amp;segment_file));</a>
<a name="ln1456">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Created next WAL segment, placeholder path: &quot; &lt;&lt; *result_path;</a>
<a name="ln1457">  out-&gt;reset(segment_file.release());</a>
<a name="ln1458">  return Status::OK();</a>
<a name="ln1459">}</a>
<a name="ln1460"> </a>
<a name="ln1461">uint64_t Log::active_segment_sequence_number() const {</a>
<a name="ln1462">  return active_segment_sequence_number_;</a>
<a name="ln1463">}</a>
<a name="ln1464"> </a>
<a name="ln1465">Status Log::TEST_SubmitFuncToAppendToken(const std::function&lt;void()&gt;&amp; func) {</a>
<a name="ln1466">  return appender_-&gt;TEST_SubmitFunc(func);</a>
<a name="ln1467">}</a>
<a name="ln1468"> </a>
<a name="ln1469">Log::~Log() {</a>
<a name="ln1470">  WARN_NOT_OK(Close(), &quot;Error closing log&quot;);</a>
<a name="ln1471">}</a>
<a name="ln1472"> </a>
<a name="ln1473">// ------------------------------------------------------------------------------------------------</a>
<a name="ln1474">// LogEntryBatch</a>
<a name="ln1475"> </a>
<a name="ln1476">LogEntryBatch::LogEntryBatch(LogEntryTypePB type, LogEntryBatchPB&amp;&amp; entry_batch_pb)</a>
<a name="ln1477">    : type_(type),</a>
<a name="ln1478">      entry_batch_pb_(std::move(entry_batch_pb)),</a>
<a name="ln1479">      count_(entry_batch_pb_.entry().size()) {</a>
<a name="ln1480">  if (type_ != LogEntryTypePB::FLUSH_MARKER) {</a>
<a name="ln1481">    DCHECK_NE(entry_batch_pb_.mono_time(), 0);</a>
<a name="ln1482">  }</a>
<a name="ln1483">}</a>
<a name="ln1484"> </a>
<a name="ln1485">LogEntryBatch::~LogEntryBatch() {</a>
<a name="ln1486">  // ReplicateMsg objects are pointed to by LogEntryBatchPB but are really owned by shared pointers</a>
<a name="ln1487">  // in replicates_. To avoid double freeing, release them from the protobuf.</a>
<a name="ln1488">  for (auto&amp; entry : *entry_batch_pb_.mutable_entry()) {</a>
<a name="ln1489">    if (entry.has_replicate()) {</a>
<a name="ln1490">      entry.release_replicate();</a>
<a name="ln1491">    }</a>
<a name="ln1492">  }</a>
<a name="ln1493">}</a>
<a name="ln1494"> </a>
<a name="ln1495">void LogEntryBatch::MarkReserved() {</a>
<a name="ln1496">  DCHECK_EQ(state_, kEntryInitialized);</a>
<a name="ln1497">  state_ = kEntryReserved;</a>
<a name="ln1498">}</a>
<a name="ln1499"> </a>
<a name="ln1500">bool LogEntryBatch::flush_marker() const {</a>
<a name="ln1501">  return count() == 1 &amp;&amp; entry_batch_pb_.entry(0).type() == FLUSH_MARKER;</a>
<a name="ln1502">}</a>
<a name="ln1503"> </a>
<a name="ln1504">Status LogEntryBatch::Serialize() {</a>
<a name="ln1505">  DCHECK_EQ(state_, kEntryReady);</a>
<a name="ln1506">  buffer_.clear();</a>
<a name="ln1507">  // FLUSH_MARKER LogEntries are markers and are not serialized.</a>
<a name="ln1508">  if (PREDICT_FALSE(flush_marker())) {</a>
<a name="ln1509">    total_size_bytes_ = 0;</a>
<a name="ln1510">    state_ = kEntrySerialized;</a>
<a name="ln1511">    return Status::OK();</a>
<a name="ln1512">  }</a>
<a name="ln1513">  DCHECK_NE(entry_batch_pb_.mono_time(), 0);</a>
<a name="ln1514">  total_size_bytes_ = entry_batch_pb_.ByteSize();</a>
<a name="ln1515">  buffer_.reserve(total_size_bytes_);</a>
<a name="ln1516"> </a>
<a name="ln1517">  if (!pb_util::AppendToString(entry_batch_pb_, &amp;buffer_)) {</a>
<a name="ln1518">    return STATUS(IOError, Substitute(&quot;unable to serialize the entry batch, contents: $1&quot;,</a>
<a name="ln1519">                                      entry_batch_pb_.DebugString()));</a>
<a name="ln1520">  }</a>
<a name="ln1521"> </a>
<a name="ln1522">  state_ = kEntrySerialized;</a>
<a name="ln1523">  return Status::OK();</a>
<a name="ln1524">}</a>
<a name="ln1525"> </a>
<a name="ln1526">void LogEntryBatch::MarkReady() {</a>
<a name="ln1527">  DCHECK_EQ(state_, kEntryReserved);</a>
<a name="ln1528">  state_ = kEntryReady;</a>
<a name="ln1529">}</a>
<a name="ln1530"> </a>
<a name="ln1531">}  // namespace log</a>
<a name="ln1532">}  // namespace yb</a>

</code></pre>
<div class="balloon" rel="242"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="349"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="353"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="426"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="439"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="445"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="447"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="552"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="590"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="593"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="624"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="646"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v1004/" target="_blank">V1004</a> The 'reserved_entry' pointer was used unsafely after it was verified against nullptr. Check lines: 624, 646.</p></div>
<div class="balloon" rel="754"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="765"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v612/" target="_blank">V612</a> An unconditional 'return' within a loop.</p></div>
<div class="balloon" rel="787"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="823"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="962"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="970"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="985"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1114"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1235"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1244"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1320"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1325"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1401"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1431"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1450"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1456"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
