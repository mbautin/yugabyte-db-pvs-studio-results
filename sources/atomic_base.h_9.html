
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII" />
  <title>atomic_base.h</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">// -*- C++ -*- header.</a>
<a name="ln2"> </a>
<a name="ln3">// Copyright (C) 2008-2018 Free Software Foundation, Inc.</a>
<a name="ln4">//</a>
<a name="ln5">// This file is part of the GNU ISO C++ Library.  This library is free</a>
<a name="ln6">// software; you can redistribute it and/or modify it under the</a>
<a name="ln7">// terms of the GNU General Public License as published by the</a>
<a name="ln8">// Free Software Foundation; either version 3, or (at your option)</a>
<a name="ln9">// any later version.</a>
<a name="ln10"> </a>
<a name="ln11">// This library is distributed in the hope that it will be useful,</a>
<a name="ln12">// but WITHOUT ANY WARRANTY; without even the implied warranty of</a>
<a name="ln13">// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</a>
<a name="ln14">// GNU General Public License for more details.</a>
<a name="ln15"> </a>
<a name="ln16">// Under Section 7 of GPL version 3, you are granted additional</a>
<a name="ln17">// permissions described in the GCC Runtime Library Exception, version</a>
<a name="ln18">// 3.1, as published by the Free Software Foundation.</a>
<a name="ln19"> </a>
<a name="ln20">// You should have received a copy of the GNU General Public License and</a>
<a name="ln21">// a copy of the GCC Runtime Library Exception along with this program;</a>
<a name="ln22">// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see</a>
<a name="ln23">// &lt;http://www.gnu.org/licenses/&gt;.</a>
<a name="ln24"> </a>
<a name="ln25">/** @file bits/atomic_base.h</a>
<a name="ln26"> *  This is an internal header file, included by other library headers.</a>
<a name="ln27"> *  Do not attempt to use it directly. @headername{atomic}</a>
<a name="ln28"> */</a>
<a name="ln29"> </a>
<a name="ln30">#ifndef _GLIBCXX_ATOMIC_BASE_H</a>
<a name="ln31">#define _GLIBCXX_ATOMIC_BASE_H 1</a>
<a name="ln32"> </a>
<a name="ln33">#pragma GCC system_header</a>
<a name="ln34"> </a>
<a name="ln35">#include &lt;bits/c++config.h&gt;</a>
<a name="ln36">#include &lt;stdint.h&gt;</a>
<a name="ln37">#include &lt;bits/atomic_lockfree_defines.h&gt;</a>
<a name="ln38"> </a>
<a name="ln39">#ifndef _GLIBCXX_ALWAYS_INLINE</a>
<a name="ln40">#define _GLIBCXX_ALWAYS_INLINE inline __attribute__((__always_inline__))</a>
<a name="ln41">#endif</a>
<a name="ln42"> </a>
<a name="ln43">namespace std _GLIBCXX_VISIBILITY(default)</a>
<a name="ln44">{</a>
<a name="ln45">_GLIBCXX_BEGIN_NAMESPACE_VERSION</a>
<a name="ln46"> </a>
<a name="ln47">  /**</a>
<a name="ln48">   * @defgroup atomics Atomics</a>
<a name="ln49">   *</a>
<a name="ln50">   * Components for performing atomic operations.</a>
<a name="ln51">   * @{</a>
<a name="ln52">   */</a>
<a name="ln53"> </a>
<a name="ln54">  /// Enumeration for memory_order</a>
<a name="ln55">  typedef enum memory_order</a>
<a name="ln56">    {</a>
<a name="ln57">      memory_order_relaxed,</a>
<a name="ln58">      memory_order_consume,</a>
<a name="ln59">      memory_order_acquire,</a>
<a name="ln60">      memory_order_release,</a>
<a name="ln61">      memory_order_acq_rel,</a>
<a name="ln62">      memory_order_seq_cst</a>
<a name="ln63">    } memory_order;</a>
<a name="ln64"> </a>
<a name="ln65">  enum __memory_order_modifier</a>
<a name="ln66">    {</a>
<a name="ln67">      __memory_order_mask          = 0x0ffff,</a>
<a name="ln68">      __memory_order_modifier_mask = 0xffff0000,</a>
<a name="ln69">      __memory_order_hle_acquire   = 0x10000,</a>
<a name="ln70">      __memory_order_hle_release   = 0x20000</a>
<a name="ln71">    };</a>
<a name="ln72"> </a>
<a name="ln73">  constexpr memory_order</a>
<a name="ln74">  operator|(memory_order __m, __memory_order_modifier __mod)</a>
<a name="ln75">  {</a>
<a name="ln76">    return memory_order(__m | int(__mod));</a>
<a name="ln77">  }</a>
<a name="ln78"> </a>
<a name="ln79">  constexpr memory_order</a>
<a name="ln80">  operator&amp;(memory_order __m, __memory_order_modifier __mod)</a>
<a name="ln81">  {</a>
<a name="ln82">    return memory_order(__m &amp; int(__mod));</a>
<a name="ln83">  }</a>
<a name="ln84"> </a>
<a name="ln85">  // Drop release ordering as per [atomics.types.operations.req]/21</a>
<a name="ln86">  constexpr memory_order</a>
<a name="ln87">  __cmpexch_failure_order2(memory_order __m) noexcept</a>
<a name="ln88">  {</a>
<a name="ln89">    return __m == memory_order_acq_rel ? memory_order_acquire</a>
<a name="ln90">      : __m == memory_order_release ? memory_order_relaxed : __m;</a>
<a name="ln91">  }</a>
<a name="ln92"> </a>
<a name="ln93">  constexpr memory_order</a>
<a name="ln94">  __cmpexch_failure_order(memory_order __m) noexcept</a>
<a name="ln95">  {</a>
<a name="ln96">    return memory_order(__cmpexch_failure_order2(__m &amp; __memory_order_mask)</a>
<a name="ln97">      | (__m &amp; __memory_order_modifier_mask));</a>
<a name="ln98">  }</a>
<a name="ln99"> </a>
<a name="ln100">  _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln101">  atomic_thread_fence(memory_order __m) noexcept</a>
<a name="ln102">  { __atomic_thread_fence(__m); }</a>
<a name="ln103"> </a>
<a name="ln104">  _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln105">  atomic_signal_fence(memory_order __m) noexcept</a>
<a name="ln106">  { __atomic_signal_fence(__m); }</a>
<a name="ln107"> </a>
<a name="ln108">  /// kill_dependency</a>
<a name="ln109">  template&lt;typename _Tp&gt;</a>
<a name="ln110">    inline _Tp</a>
<a name="ln111">    kill_dependency(_Tp __y) noexcept</a>
<a name="ln112">    {</a>
<a name="ln113">      _Tp __ret(__y);</a>
<a name="ln114">      return __ret;</a>
<a name="ln115">    }</a>
<a name="ln116"> </a>
<a name="ln117"> </a>
<a name="ln118">  // Base types for atomics.</a>
<a name="ln119">  template&lt;typename _IntTp&gt;</a>
<a name="ln120">    struct __atomic_base;</a>
<a name="ln121"> </a>
<a name="ln122"> </a>
<a name="ln123">#define ATOMIC_VAR_INIT(_VI) { _VI }</a>
<a name="ln124"> </a>
<a name="ln125">  template&lt;typename _Tp&gt;</a>
<a name="ln126">    struct atomic;</a>
<a name="ln127"> </a>
<a name="ln128">  template&lt;typename _Tp&gt;</a>
<a name="ln129">    struct atomic&lt;_Tp*&gt;;</a>
<a name="ln130"> </a>
<a name="ln131">    /* The target's &quot;set&quot; value for test-and-set may not be exactly 1.  */</a>
<a name="ln132">#if __GCC_ATOMIC_TEST_AND_SET_TRUEVAL == 1</a>
<a name="ln133">    typedef bool __atomic_flag_data_type;</a>
<a name="ln134">#else</a>
<a name="ln135">    typedef unsigned char __atomic_flag_data_type;</a>
<a name="ln136">#endif</a>
<a name="ln137"> </a>
<a name="ln138">  /**</a>
<a name="ln139">   *  @brief Base type for atomic_flag.</a>
<a name="ln140">   *</a>
<a name="ln141">   *  Base type is POD with data, allowing atomic_flag to derive from</a>
<a name="ln142">   *  it and meet the standard layout type requirement. In addition to</a>
<a name="ln143">   *  compatibility with a C interface, this allows different</a>
<a name="ln144">   *  implementations of atomic_flag to use the same atomic operation</a>
<a name="ln145">   *  functions, via a standard conversion to the __atomic_flag_base</a>
<a name="ln146">   *  argument.</a>
<a name="ln147">  */</a>
<a name="ln148">  _GLIBCXX_BEGIN_EXTERN_C</a>
<a name="ln149"> </a>
<a name="ln150">  struct __atomic_flag_base</a>
<a name="ln151">  {</a>
<a name="ln152">    __atomic_flag_data_type _M_i;</a>
<a name="ln153">  };</a>
<a name="ln154"> </a>
<a name="ln155">  _GLIBCXX_END_EXTERN_C</a>
<a name="ln156"> </a>
<a name="ln157">#define ATOMIC_FLAG_INIT { 0 }</a>
<a name="ln158"> </a>
<a name="ln159">  /// atomic_flag</a>
<a name="ln160">  struct atomic_flag : public __atomic_flag_base</a>
<a name="ln161">  {</a>
<a name="ln162">    atomic_flag() noexcept = default;</a>
<a name="ln163">    ~atomic_flag() noexcept = default;</a>
<a name="ln164">    atomic_flag(const atomic_flag&amp;) = delete;</a>
<a name="ln165">    atomic_flag&amp; operator=(const atomic_flag&amp;) = delete;</a>
<a name="ln166">    atomic_flag&amp; operator=(const atomic_flag&amp;) volatile = delete;</a>
<a name="ln167"> </a>
<a name="ln168">    // Conversion to ATOMIC_FLAG_INIT.</a>
<a name="ln169">    constexpr atomic_flag(bool __i) noexcept</a>
<a name="ln170">      : __atomic_flag_base{ _S_init(__i) }</a>
<a name="ln171">    { }</a>
<a name="ln172"> </a>
<a name="ln173">    _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln174">    test_and_set(memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln175">    {</a>
<a name="ln176">      return __atomic_test_and_set (&amp;_M_i, __m);</a>
<a name="ln177">    }</a>
<a name="ln178"> </a>
<a name="ln179">    _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln180">    test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln181">    {</a>
<a name="ln182">      return __atomic_test_and_set (&amp;_M_i, __m);</a>
<a name="ln183">    }</a>
<a name="ln184"> </a>
<a name="ln185">    _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln186">    clear(memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln187">    {</a>
<a name="ln188">      memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln189">      __glibcxx_assert(__b != memory_order_consume);</a>
<a name="ln190">      __glibcxx_assert(__b != memory_order_acquire);</a>
<a name="ln191">      __glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln192"> </a>
<a name="ln193">      __atomic_clear (&amp;_M_i, __m);</a>
<a name="ln194">    }</a>
<a name="ln195"> </a>
<a name="ln196">    _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln197">    clear(memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln198">    {</a>
<a name="ln199">      memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln200">      __glibcxx_assert(__b != memory_order_consume);</a>
<a name="ln201">      __glibcxx_assert(__b != memory_order_acquire);</a>
<a name="ln202">      __glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln203"> </a>
<a name="ln204">      __atomic_clear (&amp;_M_i, __m);</a>
<a name="ln205">    }</a>
<a name="ln206"> </a>
<a name="ln207">  private:</a>
<a name="ln208">    static constexpr __atomic_flag_data_type</a>
<a name="ln209">    _S_init(bool __i)</a>
<a name="ln210">    { return __i ? __GCC_ATOMIC_TEST_AND_SET_TRUEVAL : 0; }</a>
<a name="ln211">  };</a>
<a name="ln212"> </a>
<a name="ln213"> </a>
<a name="ln214">  /// Base class for atomic integrals.</a>
<a name="ln215">  //</a>
<a name="ln216">  // For each of the integral types, define atomic_[integral type] struct</a>
<a name="ln217">  //</a>
<a name="ln218">  // atomic_bool     bool</a>
<a name="ln219">  // atomic_char     char</a>
<a name="ln220">  // atomic_schar    signed char</a>
<a name="ln221">  // atomic_uchar    unsigned char</a>
<a name="ln222">  // atomic_short    short</a>
<a name="ln223">  // atomic_ushort   unsigned short</a>
<a name="ln224">  // atomic_int      int</a>
<a name="ln225">  // atomic_uint     unsigned int</a>
<a name="ln226">  // atomic_long     long</a>
<a name="ln227">  // atomic_ulong    unsigned long</a>
<a name="ln228">  // atomic_llong    long long</a>
<a name="ln229">  // atomic_ullong   unsigned long long</a>
<a name="ln230">  // atomic_char16_t char16_t</a>
<a name="ln231">  // atomic_char32_t char32_t</a>
<a name="ln232">  // atomic_wchar_t  wchar_t</a>
<a name="ln233">  //</a>
<a name="ln234">  // NB: Assuming _ITp is an integral scalar type that is 1, 2, 4, or</a>
<a name="ln235">  // 8 bytes, since that is what GCC built-in functions for atomic</a>
<a name="ln236">  // memory access expect.</a>
<a name="ln237">  template&lt;typename _ITp&gt;</a>
<a name="ln238">    struct __atomic_base</a>
<a name="ln239">    {</a>
<a name="ln240">    private:</a>
<a name="ln241">      typedef _ITp 	__int_type;</a>
<a name="ln242"> </a>
<a name="ln243">      static constexpr int _S_alignment =</a>
<a name="ln244">	sizeof(_ITp) &gt; alignof(_ITp) ? sizeof(_ITp) : alignof(_ITp);</a>
<a name="ln245"> </a>
<a name="ln246">      alignas(_S_alignment) __int_type _M_i;</a>
<a name="ln247"> </a>
<a name="ln248">    public:</a>
<a name="ln249">      __atomic_base() noexcept = default;</a>
<a name="ln250">      ~__atomic_base() noexcept = default;</a>
<a name="ln251">      __atomic_base(const __atomic_base&amp;) = delete;</a>
<a name="ln252">      __atomic_base&amp; operator=(const __atomic_base&amp;) = delete;</a>
<a name="ln253">      __atomic_base&amp; operator=(const __atomic_base&amp;) volatile = delete;</a>
<a name="ln254"> </a>
<a name="ln255">      // Requires __int_type convertible to _M_i.</a>
<a name="ln256">      constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }</a>
<a name="ln257"> </a>
<a name="ln258">      operator __int_type() const noexcept</a>
<a name="ln259">      { return load(); }</a>
<a name="ln260"> </a>
<a name="ln261">      operator __int_type() const volatile noexcept</a>
<a name="ln262">      { return load(); }</a>
<a name="ln263"> </a>
<a name="ln264">      __int_type</a>
<a name="ln265">      operator=(__int_type __i) noexcept</a>
<a name="ln266">      {</a>
<a name="ln267">	store(__i);</a>
<a name="ln268">	return __i;</a>
<a name="ln269">      }</a>
<a name="ln270"> </a>
<a name="ln271">      __int_type</a>
<a name="ln272">      operator=(__int_type __i) volatile noexcept</a>
<a name="ln273">      {</a>
<a name="ln274">	store(__i);</a>
<a name="ln275">	return __i;</a>
<a name="ln276">      }</a>
<a name="ln277"> </a>
<a name="ln278">      __int_type</a>
<a name="ln279">      operator++(int) noexcept</a>
<a name="ln280">      { return fetch_add(1); }</a>
<a name="ln281"> </a>
<a name="ln282">      __int_type</a>
<a name="ln283">      operator++(int) volatile noexcept</a>
<a name="ln284">      { return fetch_add(1); }</a>
<a name="ln285"> </a>
<a name="ln286">      __int_type</a>
<a name="ln287">      operator--(int) noexcept</a>
<a name="ln288">      { return fetch_sub(1); }</a>
<a name="ln289"> </a>
<a name="ln290">      __int_type</a>
<a name="ln291">      operator--(int) volatile noexcept</a>
<a name="ln292">      { return fetch_sub(1); }</a>
<a name="ln293"> </a>
<a name="ln294">      __int_type</a>
<a name="ln295">      operator++() noexcept</a>
<a name="ln296">      { return __atomic_add_fetch(&amp;_M_i, 1, memory_order_seq_cst); }</a>
<a name="ln297"> </a>
<a name="ln298">      __int_type</a>
<a name="ln299">      operator++() volatile noexcept</a>
<a name="ln300">      { return __atomic_add_fetch(&amp;_M_i, 1, memory_order_seq_cst); }</a>
<a name="ln301"> </a>
<a name="ln302">      __int_type</a>
<a name="ln303">      operator--() noexcept</a>
<a name="ln304">      { return __atomic_sub_fetch(&amp;_M_i, 1, memory_order_seq_cst); }</a>
<a name="ln305"> </a>
<a name="ln306">      __int_type</a>
<a name="ln307">      operator--() volatile noexcept</a>
<a name="ln308">      { return __atomic_sub_fetch(&amp;_M_i, 1, memory_order_seq_cst); }</a>
<a name="ln309"> </a>
<a name="ln310">      __int_type</a>
<a name="ln311">      operator+=(__int_type __i) noexcept</a>
<a name="ln312">      { return __atomic_add_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln313"> </a>
<a name="ln314">      __int_type</a>
<a name="ln315">      operator+=(__int_type __i) volatile noexcept</a>
<a name="ln316">      { return __atomic_add_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln317"> </a>
<a name="ln318">      __int_type</a>
<a name="ln319">      operator-=(__int_type __i) noexcept</a>
<a name="ln320">      { return __atomic_sub_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln321"> </a>
<a name="ln322">      __int_type</a>
<a name="ln323">      operator-=(__int_type __i) volatile noexcept</a>
<a name="ln324">      { return __atomic_sub_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln325"> </a>
<a name="ln326">      __int_type</a>
<a name="ln327">      operator&amp;=(__int_type __i) noexcept</a>
<a name="ln328">      { return __atomic_and_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln329"> </a>
<a name="ln330">      __int_type</a>
<a name="ln331">      operator&amp;=(__int_type __i) volatile noexcept</a>
<a name="ln332">      { return __atomic_and_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln333"> </a>
<a name="ln334">      __int_type</a>
<a name="ln335">      operator|=(__int_type __i) noexcept</a>
<a name="ln336">      { return __atomic_or_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln337"> </a>
<a name="ln338">      __int_type</a>
<a name="ln339">      operator|=(__int_type __i) volatile noexcept</a>
<a name="ln340">      { return __atomic_or_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln341"> </a>
<a name="ln342">      __int_type</a>
<a name="ln343">      operator^=(__int_type __i) noexcept</a>
<a name="ln344">      { return __atomic_xor_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln345"> </a>
<a name="ln346">      __int_type</a>
<a name="ln347">      operator^=(__int_type __i) volatile noexcept</a>
<a name="ln348">      { return __atomic_xor_fetch(&amp;_M_i, __i, memory_order_seq_cst); }</a>
<a name="ln349"> </a>
<a name="ln350">      bool</a>
<a name="ln351">      is_lock_free() const noexcept</a>
<a name="ln352">      {</a>
<a name="ln353">	// Use a fake, minimally aligned pointer.</a>
<a name="ln354">	return __atomic_is_lock_free(sizeof(_M_i),</a>
<a name="ln355">	    reinterpret_cast&lt;void *&gt;(-__alignof(_M_i)));</a>
<a name="ln356">      }</a>
<a name="ln357"> </a>
<a name="ln358">      bool</a>
<a name="ln359">      is_lock_free() const volatile noexcept</a>
<a name="ln360">      {</a>
<a name="ln361">	// Use a fake, minimally aligned pointer.</a>
<a name="ln362">	return __atomic_is_lock_free(sizeof(_M_i),</a>
<a name="ln363">	    reinterpret_cast&lt;void *&gt;(-__alignof(_M_i)));</a>
<a name="ln364">      }</a>
<a name="ln365"> </a>
<a name="ln366">      _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln367">      store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln368">      {</a>
<a name="ln369">	memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln370">	__glibcxx_assert(__b != memory_order_acquire);</a>
<a name="ln371">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln372">	__glibcxx_assert(__b != memory_order_consume);</a>
<a name="ln373"> </a>
<a name="ln374">	__atomic_store_n(&amp;_M_i, __i, __m);</a>
<a name="ln375">      }</a>
<a name="ln376"> </a>
<a name="ln377">      _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln378">      store(__int_type __i,</a>
<a name="ln379">	    memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln380">      {</a>
<a name="ln381">	memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln382">	__glibcxx_assert(__b != memory_order_acquire);</a>
<a name="ln383">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln384">	__glibcxx_assert(__b != memory_order_consume);</a>
<a name="ln385"> </a>
<a name="ln386">	__atomic_store_n(&amp;_M_i, __i, __m);</a>
<a name="ln387">      }</a>
<a name="ln388"> </a>
<a name="ln389">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln390">      load(memory_order __m = memory_order_seq_cst) const noexcept</a>
<a name="ln391">      {</a>
<a name="ln392">	memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln393">	__glibcxx_assert(__b != memory_order_release);</a>
<a name="ln394">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln395"> </a>
<a name="ln396">	return __atomic_load_n(&amp;_M_i, __m);</a>
<a name="ln397">      }</a>
<a name="ln398"> </a>
<a name="ln399">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln400">      load(memory_order __m = memory_order_seq_cst) const volatile noexcept</a>
<a name="ln401">      {</a>
<a name="ln402">	memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln403">	__glibcxx_assert(__b != memory_order_release);</a>
<a name="ln404">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln405"> </a>
<a name="ln406">	return __atomic_load_n(&amp;_M_i, __m);</a>
<a name="ln407">      }</a>
<a name="ln408"> </a>
<a name="ln409">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln410">      exchange(__int_type __i,</a>
<a name="ln411">	       memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln412">      {</a>
<a name="ln413">	return __atomic_exchange_n(&amp;_M_i, __i, __m);</a>
<a name="ln414">      }</a>
<a name="ln415"> </a>
<a name="ln416"> </a>
<a name="ln417">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln418">      exchange(__int_type __i,</a>
<a name="ln419">	       memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln420">      {</a>
<a name="ln421">	return __atomic_exchange_n(&amp;_M_i, __i, __m);</a>
<a name="ln422">      }</a>
<a name="ln423"> </a>
<a name="ln424">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln425">      compare_exchange_weak(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln426">			    memory_order __m1, memory_order __m2) noexcept</a>
<a name="ln427">      {</a>
<a name="ln428">	memory_order __b2 = __m2 &amp; __memory_order_mask;</a>
<a name="ln429">	memory_order __b1 = __m1 &amp; __memory_order_mask;</a>
<a name="ln430">	__glibcxx_assert(__b2 != memory_order_release);</a>
<a name="ln431">	__glibcxx_assert(__b2 != memory_order_acq_rel);</a>
<a name="ln432">	__glibcxx_assert(__b2 &lt;= __b1);</a>
<a name="ln433"> </a>
<a name="ln434">	return __atomic_compare_exchange_n(&amp;_M_i, &amp;__i1, __i2, 1, __m1, __m2);</a>
<a name="ln435">      }</a>
<a name="ln436"> </a>
<a name="ln437">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln438">      compare_exchange_weak(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln439">			    memory_order __m1,</a>
<a name="ln440">			    memory_order __m2) volatile noexcept</a>
<a name="ln441">      {</a>
<a name="ln442">	memory_order __b2 = __m2 &amp; __memory_order_mask;</a>
<a name="ln443">	memory_order __b1 = __m1 &amp; __memory_order_mask;</a>
<a name="ln444">	__glibcxx_assert(__b2 != memory_order_release);</a>
<a name="ln445">	__glibcxx_assert(__b2 != memory_order_acq_rel);</a>
<a name="ln446">	__glibcxx_assert(__b2 &lt;= __b1);</a>
<a name="ln447"> </a>
<a name="ln448">	return __atomic_compare_exchange_n(&amp;_M_i, &amp;__i1, __i2, 1, __m1, __m2);</a>
<a name="ln449">      }</a>
<a name="ln450"> </a>
<a name="ln451">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln452">      compare_exchange_weak(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln453">			    memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln454">      {</a>
<a name="ln455">	return compare_exchange_weak(__i1, __i2, __m,</a>
<a name="ln456">				     __cmpexch_failure_order(__m));</a>
<a name="ln457">      }</a>
<a name="ln458"> </a>
<a name="ln459">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln460">      compare_exchange_weak(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln461">		   memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln462">      {</a>
<a name="ln463">	return compare_exchange_weak(__i1, __i2, __m,</a>
<a name="ln464">				     __cmpexch_failure_order(__m));</a>
<a name="ln465">      }</a>
<a name="ln466"> </a>
<a name="ln467">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln468">      compare_exchange_strong(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln469">			      memory_order __m1, memory_order __m2) noexcept</a>
<a name="ln470">      {</a>
<a name="ln471">	memory_order __b2 = __m2 &amp; __memory_order_mask;</a>
<a name="ln472">	memory_order __b1 = __m1 &amp; __memory_order_mask;</a>
<a name="ln473">	__glibcxx_assert(__b2 != memory_order_release);</a>
<a name="ln474">	__glibcxx_assert(__b2 != memory_order_acq_rel);</a>
<a name="ln475">	__glibcxx_assert(__b2 &lt;= __b1);</a>
<a name="ln476"> </a>
<a name="ln477">	return __atomic_compare_exchange_n(&amp;_M_i, &amp;__i1, __i2, 0, __m1, __m2);</a>
<a name="ln478">      }</a>
<a name="ln479"> </a>
<a name="ln480">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln481">      compare_exchange_strong(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln482">			      memory_order __m1,</a>
<a name="ln483">			      memory_order __m2) volatile noexcept</a>
<a name="ln484">      {</a>
<a name="ln485">	memory_order __b2 = __m2 &amp; __memory_order_mask;</a>
<a name="ln486">	memory_order __b1 = __m1 &amp; __memory_order_mask;</a>
<a name="ln487"> </a>
<a name="ln488">	__glibcxx_assert(__b2 != memory_order_release);</a>
<a name="ln489">	__glibcxx_assert(__b2 != memory_order_acq_rel);</a>
<a name="ln490">	__glibcxx_assert(__b2 &lt;= __b1);</a>
<a name="ln491"> </a>
<a name="ln492">	return __atomic_compare_exchange_n(&amp;_M_i, &amp;__i1, __i2, 0, __m1, __m2);</a>
<a name="ln493">      }</a>
<a name="ln494"> </a>
<a name="ln495">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln496">      compare_exchange_strong(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln497">			      memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln498">      {</a>
<a name="ln499">	return compare_exchange_strong(__i1, __i2, __m,</a>
<a name="ln500">				       __cmpexch_failure_order(__m));</a>
<a name="ln501">      }</a>
<a name="ln502"> </a>
<a name="ln503">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln504">      compare_exchange_strong(__int_type&amp; __i1, __int_type __i2,</a>
<a name="ln505">		 memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln506">      {</a>
<a name="ln507">	return compare_exchange_strong(__i1, __i2, __m,</a>
<a name="ln508">				       __cmpexch_failure_order(__m));</a>
<a name="ln509">      }</a>
<a name="ln510"> </a>
<a name="ln511">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln512">      fetch_add(__int_type __i,</a>
<a name="ln513">		memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln514">      { return __atomic_fetch_add(&amp;_M_i, __i, __m); }</a>
<a name="ln515"> </a>
<a name="ln516">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln517">      fetch_add(__int_type __i,</a>
<a name="ln518">		memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln519">      { return __atomic_fetch_add(&amp;_M_i, __i, __m); }</a>
<a name="ln520"> </a>
<a name="ln521">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln522">      fetch_sub(__int_type __i,</a>
<a name="ln523">		memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln524">      { return __atomic_fetch_sub(&amp;_M_i, __i, __m); }</a>
<a name="ln525"> </a>
<a name="ln526">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln527">      fetch_sub(__int_type __i,</a>
<a name="ln528">		memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln529">      { return __atomic_fetch_sub(&amp;_M_i, __i, __m); }</a>
<a name="ln530"> </a>
<a name="ln531">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln532">      fetch_and(__int_type __i,</a>
<a name="ln533">		memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln534">      { return __atomic_fetch_and(&amp;_M_i, __i, __m); }</a>
<a name="ln535"> </a>
<a name="ln536">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln537">      fetch_and(__int_type __i,</a>
<a name="ln538">		memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln539">      { return __atomic_fetch_and(&amp;_M_i, __i, __m); }</a>
<a name="ln540"> </a>
<a name="ln541">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln542">      fetch_or(__int_type __i,</a>
<a name="ln543">	       memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln544">      { return __atomic_fetch_or(&amp;_M_i, __i, __m); }</a>
<a name="ln545"> </a>
<a name="ln546">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln547">      fetch_or(__int_type __i,</a>
<a name="ln548">	       memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln549">      { return __atomic_fetch_or(&amp;_M_i, __i, __m); }</a>
<a name="ln550"> </a>
<a name="ln551">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln552">      fetch_xor(__int_type __i,</a>
<a name="ln553">		memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln554">      { return __atomic_fetch_xor(&amp;_M_i, __i, __m); }</a>
<a name="ln555"> </a>
<a name="ln556">      _GLIBCXX_ALWAYS_INLINE __int_type</a>
<a name="ln557">      fetch_xor(__int_type __i,</a>
<a name="ln558">		memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln559">      { return __atomic_fetch_xor(&amp;_M_i, __i, __m); }</a>
<a name="ln560">    };</a>
<a name="ln561"> </a>
<a name="ln562"> </a>
<a name="ln563">  /// Partial specialization for pointer types.</a>
<a name="ln564">  template&lt;typename _PTp&gt;</a>
<a name="ln565">    struct __atomic_base&lt;_PTp*&gt;</a>
<a name="ln566">    {</a>
<a name="ln567">    private:</a>
<a name="ln568">      typedef _PTp* 	__pointer_type;</a>
<a name="ln569"> </a>
<a name="ln570">      __pointer_type 	_M_p;</a>
<a name="ln571"> </a>
<a name="ln572">      // Factored out to facilitate explicit specialization.</a>
<a name="ln573">      constexpr ptrdiff_t</a>
<a name="ln574">      _M_type_size(ptrdiff_t __d) const { return __d * sizeof(_PTp); }</a>
<a name="ln575"> </a>
<a name="ln576">      constexpr ptrdiff_t</a>
<a name="ln577">      _M_type_size(ptrdiff_t __d) const volatile { return __d * sizeof(_PTp); }</a>
<a name="ln578"> </a>
<a name="ln579">    public:</a>
<a name="ln580">      __atomic_base() noexcept = default;</a>
<a name="ln581">      ~__atomic_base() noexcept = default;</a>
<a name="ln582">      __atomic_base(const __atomic_base&amp;) = delete;</a>
<a name="ln583">      __atomic_base&amp; operator=(const __atomic_base&amp;) = delete;</a>
<a name="ln584">      __atomic_base&amp; operator=(const __atomic_base&amp;) volatile = delete;</a>
<a name="ln585"> </a>
<a name="ln586">      // Requires __pointer_type convertible to _M_p.</a>
<a name="ln587">      constexpr __atomic_base(__pointer_type __p) noexcept : _M_p (__p) { }</a>
<a name="ln588"> </a>
<a name="ln589">      operator __pointer_type() const noexcept</a>
<a name="ln590">      { return load(); }</a>
<a name="ln591"> </a>
<a name="ln592">      operator __pointer_type() const volatile noexcept</a>
<a name="ln593">      { return load(); }</a>
<a name="ln594"> </a>
<a name="ln595">      __pointer_type</a>
<a name="ln596">      operator=(__pointer_type __p) noexcept</a>
<a name="ln597">      {</a>
<a name="ln598">	store(__p);</a>
<a name="ln599">	return __p;</a>
<a name="ln600">      }</a>
<a name="ln601"> </a>
<a name="ln602">      __pointer_type</a>
<a name="ln603">      operator=(__pointer_type __p) volatile noexcept</a>
<a name="ln604">      {</a>
<a name="ln605">	store(__p);</a>
<a name="ln606">	return __p;</a>
<a name="ln607">      }</a>
<a name="ln608"> </a>
<a name="ln609">      __pointer_type</a>
<a name="ln610">      operator++(int) noexcept</a>
<a name="ln611">      { return fetch_add(1); }</a>
<a name="ln612"> </a>
<a name="ln613">      __pointer_type</a>
<a name="ln614">      operator++(int) volatile noexcept</a>
<a name="ln615">      { return fetch_add(1); }</a>
<a name="ln616"> </a>
<a name="ln617">      __pointer_type</a>
<a name="ln618">      operator--(int) noexcept</a>
<a name="ln619">      { return fetch_sub(1); }</a>
<a name="ln620"> </a>
<a name="ln621">      __pointer_type</a>
<a name="ln622">      operator--(int) volatile noexcept</a>
<a name="ln623">      { return fetch_sub(1); }</a>
<a name="ln624"> </a>
<a name="ln625">      __pointer_type</a>
<a name="ln626">      operator++() noexcept</a>
<a name="ln627">      { return __atomic_add_fetch(&amp;_M_p, _M_type_size(1),</a>
<a name="ln628">				  memory_order_seq_cst); }</a>
<a name="ln629"> </a>
<a name="ln630">      __pointer_type</a>
<a name="ln631">      operator++() volatile noexcept</a>
<a name="ln632">      { return __atomic_add_fetch(&amp;_M_p, _M_type_size(1),</a>
<a name="ln633">				  memory_order_seq_cst); }</a>
<a name="ln634"> </a>
<a name="ln635">      __pointer_type</a>
<a name="ln636">      operator--() noexcept</a>
<a name="ln637">      { return __atomic_sub_fetch(&amp;_M_p, _M_type_size(1),</a>
<a name="ln638">				  memory_order_seq_cst); }</a>
<a name="ln639"> </a>
<a name="ln640">      __pointer_type</a>
<a name="ln641">      operator--() volatile noexcept</a>
<a name="ln642">      { return __atomic_sub_fetch(&amp;_M_p, _M_type_size(1),</a>
<a name="ln643">				  memory_order_seq_cst); }</a>
<a name="ln644"> </a>
<a name="ln645">      __pointer_type</a>
<a name="ln646">      operator+=(ptrdiff_t __d) noexcept</a>
<a name="ln647">      { return __atomic_add_fetch(&amp;_M_p, _M_type_size(__d),</a>
<a name="ln648">				  memory_order_seq_cst); }</a>
<a name="ln649"> </a>
<a name="ln650">      __pointer_type</a>
<a name="ln651">      operator+=(ptrdiff_t __d) volatile noexcept</a>
<a name="ln652">      { return __atomic_add_fetch(&amp;_M_p, _M_type_size(__d),</a>
<a name="ln653">				  memory_order_seq_cst); }</a>
<a name="ln654"> </a>
<a name="ln655">      __pointer_type</a>
<a name="ln656">      operator-=(ptrdiff_t __d) noexcept</a>
<a name="ln657">      { return __atomic_sub_fetch(&amp;_M_p, _M_type_size(__d),</a>
<a name="ln658">				  memory_order_seq_cst); }</a>
<a name="ln659"> </a>
<a name="ln660">      __pointer_type</a>
<a name="ln661">      operator-=(ptrdiff_t __d) volatile noexcept</a>
<a name="ln662">      { return __atomic_sub_fetch(&amp;_M_p, _M_type_size(__d),</a>
<a name="ln663">				  memory_order_seq_cst); }</a>
<a name="ln664"> </a>
<a name="ln665">      bool</a>
<a name="ln666">      is_lock_free() const noexcept</a>
<a name="ln667">      {</a>
<a name="ln668">	// Produce a fake, minimally aligned pointer.</a>
<a name="ln669">	return __atomic_is_lock_free(sizeof(_M_p),</a>
<a name="ln670">	    reinterpret_cast&lt;void *&gt;(-__alignof(_M_p)));</a>
<a name="ln671">      }</a>
<a name="ln672"> </a>
<a name="ln673">      bool</a>
<a name="ln674">      is_lock_free() const volatile noexcept</a>
<a name="ln675">      {</a>
<a name="ln676">	// Produce a fake, minimally aligned pointer.</a>
<a name="ln677">	return __atomic_is_lock_free(sizeof(_M_p),</a>
<a name="ln678">	    reinterpret_cast&lt;void *&gt;(-__alignof(_M_p)));</a>
<a name="ln679">      }</a>
<a name="ln680"> </a>
<a name="ln681">      _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln682">      store(__pointer_type __p,</a>
<a name="ln683">	    memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln684">      {</a>
<a name="ln685">        memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln686"> </a>
<a name="ln687">	__glibcxx_assert(__b != memory_order_acquire);</a>
<a name="ln688">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln689">	__glibcxx_assert(__b != memory_order_consume);</a>
<a name="ln690"> </a>
<a name="ln691">	__atomic_store_n(&amp;_M_p, __p, __m);</a>
<a name="ln692">      }</a>
<a name="ln693"> </a>
<a name="ln694">      _GLIBCXX_ALWAYS_INLINE void</a>
<a name="ln695">      store(__pointer_type __p,</a>
<a name="ln696">	    memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln697">      {</a>
<a name="ln698">	memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln699">	__glibcxx_assert(__b != memory_order_acquire);</a>
<a name="ln700">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln701">	__glibcxx_assert(__b != memory_order_consume);</a>
<a name="ln702"> </a>
<a name="ln703">	__atomic_store_n(&amp;_M_p, __p, __m);</a>
<a name="ln704">      }</a>
<a name="ln705"> </a>
<a name="ln706">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln707">      load(memory_order __m = memory_order_seq_cst) const noexcept</a>
<a name="ln708">      {</a>
<a name="ln709">	memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln710">	__glibcxx_assert(__b != memory_order_release);</a>
<a name="ln711">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln712"> </a>
<a name="ln713">	return __atomic_load_n(&amp;_M_p, __m);</a>
<a name="ln714">      }</a>
<a name="ln715"> </a>
<a name="ln716">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln717">      load(memory_order __m = memory_order_seq_cst) const volatile noexcept</a>
<a name="ln718">      {</a>
<a name="ln719">	memory_order __b = __m &amp; __memory_order_mask;</a>
<a name="ln720">	__glibcxx_assert(__b != memory_order_release);</a>
<a name="ln721">	__glibcxx_assert(__b != memory_order_acq_rel);</a>
<a name="ln722"> </a>
<a name="ln723">	return __atomic_load_n(&amp;_M_p, __m);</a>
<a name="ln724">      }</a>
<a name="ln725"> </a>
<a name="ln726">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln727">      exchange(__pointer_type __p,</a>
<a name="ln728">	       memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln729">      {</a>
<a name="ln730">	return __atomic_exchange_n(&amp;_M_p, __p, __m);</a>
<a name="ln731">      }</a>
<a name="ln732"> </a>
<a name="ln733"> </a>
<a name="ln734">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln735">      exchange(__pointer_type __p,</a>
<a name="ln736">	       memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln737">      {</a>
<a name="ln738">	return __atomic_exchange_n(&amp;_M_p, __p, __m);</a>
<a name="ln739">      }</a>
<a name="ln740"> </a>
<a name="ln741">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln742">      compare_exchange_strong(__pointer_type&amp; __p1, __pointer_type __p2,</a>
<a name="ln743">			      memory_order __m1,</a>
<a name="ln744">			      memory_order __m2) noexcept</a>
<a name="ln745">      {</a>
<a name="ln746">	memory_order __b2 = __m2 &amp; __memory_order_mask;</a>
<a name="ln747">	memory_order __b1 = __m1 &amp; __memory_order_mask;</a>
<a name="ln748">	__glibcxx_assert(__b2 != memory_order_release);</a>
<a name="ln749">	__glibcxx_assert(__b2 != memory_order_acq_rel);</a>
<a name="ln750">	__glibcxx_assert(__b2 &lt;= __b1);</a>
<a name="ln751"> </a>
<a name="ln752">	return __atomic_compare_exchange_n(&amp;_M_p, &amp;__p1, __p2, 0, __m1, __m2);</a>
<a name="ln753">      }</a>
<a name="ln754"> </a>
<a name="ln755">      _GLIBCXX_ALWAYS_INLINE bool</a>
<a name="ln756">      compare_exchange_strong(__pointer_type&amp; __p1, __pointer_type __p2,</a>
<a name="ln757">			      memory_order __m1,</a>
<a name="ln758">			      memory_order __m2) volatile noexcept</a>
<a name="ln759">      {</a>
<a name="ln760">	memory_order __b2 = __m2 &amp; __memory_order_mask;</a>
<a name="ln761">	memory_order __b1 = __m1 &amp; __memory_order_mask;</a>
<a name="ln762"> </a>
<a name="ln763">	__glibcxx_assert(__b2 != memory_order_release);</a>
<a name="ln764">	__glibcxx_assert(__b2 != memory_order_acq_rel);</a>
<a name="ln765">	__glibcxx_assert(__b2 &lt;= __b1);</a>
<a name="ln766"> </a>
<a name="ln767">	return __atomic_compare_exchange_n(&amp;_M_p, &amp;__p1, __p2, 0, __m1, __m2);</a>
<a name="ln768">      }</a>
<a name="ln769"> </a>
<a name="ln770">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln771">      fetch_add(ptrdiff_t __d,</a>
<a name="ln772">		memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln773">      { return __atomic_fetch_add(&amp;_M_p, _M_type_size(__d), __m); }</a>
<a name="ln774"> </a>
<a name="ln775">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln776">      fetch_add(ptrdiff_t __d,</a>
<a name="ln777">		memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln778">      { return __atomic_fetch_add(&amp;_M_p, _M_type_size(__d), __m); }</a>
<a name="ln779"> </a>
<a name="ln780">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln781">      fetch_sub(ptrdiff_t __d,</a>
<a name="ln782">		memory_order __m = memory_order_seq_cst) noexcept</a>
<a name="ln783">      { return __atomic_fetch_sub(&amp;_M_p, _M_type_size(__d), __m); }</a>
<a name="ln784"> </a>
<a name="ln785">      _GLIBCXX_ALWAYS_INLINE __pointer_type</a>
<a name="ln786">      fetch_sub(ptrdiff_t __d,</a>
<a name="ln787">		memory_order __m = memory_order_seq_cst) volatile noexcept</a>
<a name="ln788">      { return __atomic_fetch_sub(&amp;_M_p, _M_type_size(__d), __m); }</a>
<a name="ln789">    };</a>
<a name="ln790"> </a>
<a name="ln791">  // @} group atomics</a>
<a name="ln792"> </a>
<a name="ln793">_GLIBCXX_END_NAMESPACE_VERSION</a>
<a name="ln794">} // namespace std</a>
<a name="ln795"> </a>
<a name="ln796">#endif</a>

</code></pre>
<div class="balloon" rel="268"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v790/" target="_blank">V790</a> It is odd that the assignment operator takes an object by a non-constant reference and returns this object.</p></div>
<div class="balloon" rel="275"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v790/" target="_blank">V790</a> It is odd that the assignment operator takes an object by a non-constant reference and returns this object.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
