
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII" />
  <title>concurrent_arena.h</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">//  Copyright (c) 2011-present, Facebook, Inc.  All rights reserved.</a>
<a name="ln2">//  This source code is licensed under the BSD-style license found in the</a>
<a name="ln3">//  LICENSE file in the root directory of this source tree. An additional grant</a>
<a name="ln4">//  of patent rights can be found in the PATENTS file in the same directory.</a>
<a name="ln5">//</a>
<a name="ln6">// The following only applies to changes made to this file as part of YugaByte development.</a>
<a name="ln7">//</a>
<a name="ln8">// Portions Copyright (c) YugaByte, Inc.</a>
<a name="ln9">//</a>
<a name="ln10">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except</a>
<a name="ln11">// in compliance with the License.  You may obtain a copy of the License at</a>
<a name="ln12">//</a>
<a name="ln13">// http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln14">//</a>
<a name="ln15">// Unless required by applicable law or agreed to in writing, software distributed under the License</a>
<a name="ln16">// is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express</a>
<a name="ln17">// or implied.  See the License for the specific language governing permissions and limitations</a>
<a name="ln18">// under the License.</a>
<a name="ln19">//</a>
<a name="ln20">// Copyright (c) 2011 The LevelDB Authors. All rights reserved.</a>
<a name="ln21">// Use of this source code is governed by a BSD-style license that can be</a>
<a name="ln22">// found in the LICENSE file. See the AUTHORS file for names of contributors.</a>
<a name="ln23"> </a>
<a name="ln24">#ifndef YB_ROCKSDB_UTIL_CONCURRENT_ARENA_H</a>
<a name="ln25">#define YB_ROCKSDB_UTIL_CONCURRENT_ARENA_H</a>
<a name="ln26"> </a>
<a name="ln27">#pragma once</a>
<a name="ln28">#include &lt;atomic&gt;</a>
<a name="ln29">#include &lt;memory&gt;</a>
<a name="ln30">#include &lt;utility&gt;</a>
<a name="ln31">#include &quot;yb/rocksdb/port/likely.h&quot;</a>
<a name="ln32">#include &quot;yb/rocksdb/util/allocator.h&quot;</a>
<a name="ln33">#include &quot;yb/rocksdb/util/arena.h&quot;</a>
<a name="ln34">#include &quot;yb/rocksdb/util/mutexlock.h&quot;</a>
<a name="ln35">#include &quot;yb/rocksdb/util/thread_local.h&quot;</a>
<a name="ln36"> </a>
<a name="ln37">// Only generate field unused warning for padding array, or build under</a>
<a name="ln38">// GCC 4.8.1 will fail.</a>
<a name="ln39">#ifdef __clang__</a>
<a name="ln40">#define ROCKSDB_FIELD_UNUSED __attribute__((__unused__))</a>
<a name="ln41">#else</a>
<a name="ln42">#define ROCKSDB_FIELD_UNUSED</a>
<a name="ln43">#endif  // __clang__</a>
<a name="ln44"> </a>
<a name="ln45">namespace yb {</a>
<a name="ln46"> </a>
<a name="ln47">class MemTracker;</a>
<a name="ln48"> </a>
<a name="ln49">}</a>
<a name="ln50"> </a>
<a name="ln51">namespace rocksdb {</a>
<a name="ln52"> </a>
<a name="ln53">class Logger;</a>
<a name="ln54"> </a>
<a name="ln55">// ConcurrentArena wraps an Arena.  It makes it thread safe using a fast</a>
<a name="ln56">// inlined spinlock, and adds small per-core allocation caches to avoid</a>
<a name="ln57">// contention for small allocations.  To avoid any memory waste from the</a>
<a name="ln58">// per-core shards, they are kept small, they are lazily instantiated</a>
<a name="ln59">// only if ConcurrentArena actually notices concurrent use, and they</a>
<a name="ln60">// adjust their size so that there is no fragmentation waste when the</a>
<a name="ln61">// shard blocks are allocated from the underlying main arena.</a>
<a name="ln62">class ConcurrentArena : public Allocator {</a>
<a name="ln63"> public:</a>
<a name="ln64">  // block_size and huge_page_size are the same as for Arena (and are</a>
<a name="ln65">  // in fact just passed to the constructor of arena_.  The core-local</a>
<a name="ln66">  // shards compute their shard_block_size as a fraction of block_size</a>
<a name="ln67">  // that varies according to the hardware concurrency level.</a>
<a name="ln68">  explicit ConcurrentArena(size_t block_size = Arena::kMinBlockSize,</a>
<a name="ln69">                           size_t huge_page_size = 0);</a>
<a name="ln70"> </a>
<a name="ln71">  char* Allocate(size_t bytes) override {</a>
<a name="ln72">    return AllocateImpl(bytes, false /*force_arena*/,</a>
<a name="ln73">                        [=]() { return arena_.Allocate(bytes); });</a>
<a name="ln74">  }</a>
<a name="ln75"> </a>
<a name="ln76">  char* AllocateAligned(size_t bytes, size_t huge_page_size = 0,</a>
<a name="ln77">                        Logger* logger = nullptr) override {</a>
<a name="ln78">    size_t rounded_up = ((bytes - 1) | (sizeof(void*) - 1)) + 1;</a>
<a name="ln79">    assert(rounded_up &gt;= bytes &amp;&amp; rounded_up &lt; bytes + sizeof(void*) &amp;&amp;</a>
<a name="ln80">           (rounded_up % sizeof(void*)) == 0);</a>
<a name="ln81"> </a>
<a name="ln82">    return AllocateImpl(rounded_up, huge_page_size != 0 /*force_arena*/, [=]() {</a>
<a name="ln83">      return arena_.AllocateAligned(rounded_up, huge_page_size, logger);</a>
<a name="ln84">    });</a>
<a name="ln85">  }</a>
<a name="ln86"> </a>
<a name="ln87">  size_t ApproximateMemoryUsage() const {</a>
<a name="ln88">    std::unique_lock&lt;SpinMutex&gt; lock(arena_mutex_, std::defer_lock);</a>
<a name="ln89">    if (index_mask_ != 0) {</a>
<a name="ln90">      lock.lock();</a>
<a name="ln91">    }</a>
<a name="ln92">    return arena_.ApproximateMemoryUsage() - ShardAllocatedAndUnused();</a>
<a name="ln93">  }</a>
<a name="ln94"> </a>
<a name="ln95">  size_t MemoryAllocatedBytes() const {</a>
<a name="ln96">    return memory_allocated_bytes_.load(std::memory_order_relaxed);</a>
<a name="ln97">  }</a>
<a name="ln98"> </a>
<a name="ln99">  size_t AllocatedAndUnused() const {</a>
<a name="ln100">    return arena_allocated_and_unused_.load(std::memory_order_relaxed) +</a>
<a name="ln101">           ShardAllocatedAndUnused();</a>
<a name="ln102">  }</a>
<a name="ln103"> </a>
<a name="ln104">  size_t IrregularBlockNum() const {</a>
<a name="ln105">    return irregular_block_num_.load(std::memory_order_relaxed);</a>
<a name="ln106">  }</a>
<a name="ln107"> </a>
<a name="ln108">  size_t BlockSize() const override { return arena_.BlockSize(); }</a>
<a name="ln109"> </a>
<a name="ln110">  void SetMemTracker(std::shared_ptr&lt;yb::MemTracker&gt; mem_tracker);</a>
<a name="ln111"> </a>
<a name="ln112"> private:</a>
<a name="ln113">  struct Shard {</a>
<a name="ln114">    char padding[40] ROCKSDB_FIELD_UNUSED;</a>
<a name="ln115">    mutable SpinMutex mutex;</a>
<a name="ln116">    char* free_begin_;</a>
<a name="ln117">    std::atomic&lt;size_t&gt; allocated_and_unused_;</a>
<a name="ln118"> </a>
<a name="ln119">    Shard() : allocated_and_unused_(0) {}</a>
<a name="ln120">  };</a>
<a name="ln121"> </a>
<a name="ln122">#if ROCKSDB_SUPPORT_THREAD_LOCAL</a>
<a name="ln123">  static __thread uint32_t tls_cpuid;</a>
<a name="ln124">#else</a>
<a name="ln125">  enum ZeroFirstEnum : uint32_t { tls_cpuid = 0 };</a>
<a name="ln126">#endif</a>
<a name="ln127"> </a>
<a name="ln128">  char padding0[56] ROCKSDB_FIELD_UNUSED;</a>
<a name="ln129"> </a>
<a name="ln130">  size_t shard_block_size_;</a>
<a name="ln131"> </a>
<a name="ln132">  // shards_[i &amp; index_mask_] is valid</a>
<a name="ln133">  size_t index_mask_;</a>
<a name="ln134">  std::unique_ptr&lt;Shard[]&gt; shards_;</a>
<a name="ln135"> </a>
<a name="ln136">  Arena arena_;</a>
<a name="ln137">  mutable SpinMutex arena_mutex_;</a>
<a name="ln138">  std::atomic&lt;size_t&gt; arena_allocated_and_unused_;</a>
<a name="ln139">  std::atomic&lt;size_t&gt; memory_allocated_bytes_;</a>
<a name="ln140">  std::atomic&lt;size_t&gt; irregular_block_num_;</a>
<a name="ln141"> </a>
<a name="ln142">  char padding1[56] ROCKSDB_FIELD_UNUSED;</a>
<a name="ln143"> </a>
<a name="ln144">  Shard* Repick();</a>
<a name="ln145"> </a>
<a name="ln146">  size_t ShardAllocatedAndUnused() const {</a>
<a name="ln147">    size_t total = 0;</a>
<a name="ln148">    for (size_t i = 0; i &lt;= index_mask_; ++i) {</a>
<a name="ln149">      total += shards_[i].allocated_and_unused_.load(std::memory_order_relaxed);</a>
<a name="ln150">    }</a>
<a name="ln151">    return total;</a>
<a name="ln152">  }</a>
<a name="ln153"> </a>
<a name="ln154">  template &lt;typename Func&gt;</a>
<a name="ln155">  char* AllocateImpl(size_t bytes, bool force_arena, const Func&amp; func) {</a>
<a name="ln156">    uint32_t cpu;</a>
<a name="ln157"> </a>
<a name="ln158">    // Go directly to the arena if the allocation is too large, or if</a>
<a name="ln159">    // we've never needed to Repick() and the arena mutex is available</a>
<a name="ln160">    // with no waiting.  This keeps the fragmentation penalty of</a>
<a name="ln161">    // concurrency zero unless it might actually confer an advantage.</a>
<a name="ln162">    std::unique_lock&lt;SpinMutex&gt; arena_lock(arena_mutex_, std::defer_lock);</a>
<a name="ln163">    if (bytes &gt; shard_block_size_ / 4 || force_arena ||</a>
<a name="ln164">        ((cpu = tls_cpuid) == 0 &amp;&amp;</a>
<a name="ln165">         !shards_[0].allocated_and_unused_.load(std::memory_order_relaxed) &amp;&amp;</a>
<a name="ln166">         arena_lock.try_lock())) {</a>
<a name="ln167">      if (!arena_lock.owns_lock()) {</a>
<a name="ln168">        arena_lock.lock();</a>
<a name="ln169">      }</a>
<a name="ln170">      auto rv = func();</a>
<a name="ln171">      Fixup();</a>
<a name="ln172">      return rv;</a>
<a name="ln173">    }</a>
<a name="ln174"> </a>
<a name="ln175">    // pick a shard from which to allocate</a>
<a name="ln176">    Shard* s = &amp;shards_[cpu &amp; index_mask_];</a>
<a name="ln177">    if (!s-&gt;mutex.try_lock()) {</a>
<a name="ln178">      s = Repick();</a>
<a name="ln179">      s-&gt;mutex.lock();</a>
<a name="ln180">    }</a>
<a name="ln181">    std::unique_lock&lt;SpinMutex&gt; lock(s-&gt;mutex, std::adopt_lock);</a>
<a name="ln182"> </a>
<a name="ln183">    size_t avail = s-&gt;allocated_and_unused_.load(std::memory_order_relaxed);</a>
<a name="ln184">    if (avail &lt; bytes) {</a>
<a name="ln185">      // reload</a>
<a name="ln186">      std::lock_guard&lt;SpinMutex&gt; reload_lock(arena_mutex_);</a>
<a name="ln187"> </a>
<a name="ln188">      // If the arena's current block is within a factor of 2 of the right</a>
<a name="ln189">      // size, we adjust our request to avoid arena waste.</a>
<a name="ln190">      auto exact = arena_allocated_and_unused_.load(std::memory_order_relaxed);</a>
<a name="ln191">      assert(exact == arena_.AllocatedAndUnused());</a>
<a name="ln192">      avail = exact &gt;= shard_block_size_ / 2 &amp;&amp; exact &lt; shard_block_size_ * 2</a>
<a name="ln193">                  ? exact</a>
<a name="ln194">                  : shard_block_size_;</a>
<a name="ln195">      s-&gt;free_begin_ = arena_.AllocateAligned(avail);</a>
<a name="ln196">      Fixup();</a>
<a name="ln197">    }</a>
<a name="ln198">    s-&gt;allocated_and_unused_.store(avail - bytes, std::memory_order_relaxed);</a>
<a name="ln199"> </a>
<a name="ln200">    char* rv;</a>
<a name="ln201">    if ((bytes % sizeof(void*)) == 0) {</a>
<a name="ln202">      // aligned allocation from the beginning</a>
<a name="ln203">      rv = s-&gt;free_begin_;</a>
<a name="ln204">      s-&gt;free_begin_ += bytes;</a>
<a name="ln205">    } else {</a>
<a name="ln206">      // unaligned from the end</a>
<a name="ln207">      rv = s-&gt;free_begin_ + avail - bytes;</a>
<a name="ln208">    }</a>
<a name="ln209">    return rv;</a>
<a name="ln210">  }</a>
<a name="ln211"> </a>
<a name="ln212">  void Fixup() {</a>
<a name="ln213">    arena_allocated_and_unused_.store(arena_.AllocatedAndUnused(),</a>
<a name="ln214">                                      std::memory_order_relaxed);</a>
<a name="ln215">    memory_allocated_bytes_.store(arena_.MemoryAllocatedBytes(),</a>
<a name="ln216">                                  std::memory_order_relaxed);</a>
<a name="ln217">    irregular_block_num_.store(arena_.IrregularBlockNum(),</a>
<a name="ln218">                               std::memory_order_relaxed);</a>
<a name="ln219">  }</a>
<a name="ln220"> </a>
<a name="ln221">  ConcurrentArena(const ConcurrentArena&amp;) = delete;</a>
<a name="ln222">  ConcurrentArena&amp; operator=(const ConcurrentArena&amp;) = delete;</a>
<a name="ln223">};</a>
<a name="ln224"> </a>
<a name="ln225">}  // namespace rocksdb</a>
<a name="ln226"> </a>
<a name="ln227">#endif // YB_ROCKSDB_UTIL_CONCURRENT_ARENA_H</a>

</code></pre>
<div class="balloon" rel="119"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v730/" target="_blank">V730</a> Not all members of a class are initialized inside the constructor. Consider inspecting: free_begin_.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
