
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII" />
  <title>compaction_job.cc</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">//  Copyright (c) 2011-present, Facebook, Inc.  All rights reserved.</a>
<a name="ln2">//  This source code is licensed under the BSD-style license found in the</a>
<a name="ln3">//  LICENSE file in the root directory of this source tree. An additional grant</a>
<a name="ln4">//  of patent rights can be found in the PATENTS file in the same directory.</a>
<a name="ln5">//</a>
<a name="ln6">// The following only applies to changes made to this file as part of YugaByte development.</a>
<a name="ln7">//</a>
<a name="ln8">// Portions Copyright (c) YugaByte, Inc.</a>
<a name="ln9">//</a>
<a name="ln10">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except</a>
<a name="ln11">// in compliance with the License.  You may obtain a copy of the License at</a>
<a name="ln12">//</a>
<a name="ln13">// http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln14">//</a>
<a name="ln15">// Unless required by applicable law or agreed to in writing, software distributed under the License</a>
<a name="ln16">// is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express</a>
<a name="ln17">// or implied.  See the License for the specific language governing permissions and limitations</a>
<a name="ln18">// under the License.</a>
<a name="ln19">//</a>
<a name="ln20">// Copyright (c) 2011 The LevelDB Authors. All rights reserved.</a>
<a name="ln21">// Use of this source code is governed by a BSD-style license that can be</a>
<a name="ln22">// found in the LICENSE file. See the AUTHORS file for names of contributors.</a>
<a name="ln23"> </a>
<a name="ln24">#include &quot;yb/rocksdb/db/compaction_job.h&quot;</a>
<a name="ln25"> </a>
<a name="ln26">#ifndef __STDC_FORMAT_MACROS</a>
<a name="ln27">#define __STDC_FORMAT_MACROS</a>
<a name="ln28">#endif</a>
<a name="ln29"> </a>
<a name="ln30">#include &lt;inttypes.h&gt;</a>
<a name="ln31">#include &lt;algorithm&gt;</a>
<a name="ln32">#include &lt;functional&gt;</a>
<a name="ln33">#include &lt;vector&gt;</a>
<a name="ln34">#include &lt;memory&gt;</a>
<a name="ln35">#include &lt;list&gt;</a>
<a name="ln36">#include &lt;set&gt;</a>
<a name="ln37">#include &lt;thread&gt;</a>
<a name="ln38">#include &lt;utility&gt;</a>
<a name="ln39"> </a>
<a name="ln40">#include &quot;yb/rocksdb/db/builder.h&quot;</a>
<a name="ln41">#include &quot;yb/rocksdb/db/db_iter.h&quot;</a>
<a name="ln42">#include &quot;yb/rocksdb/db/dbformat.h&quot;</a>
<a name="ln43">#include &quot;yb/rocksdb/db/event_helpers.h&quot;</a>
<a name="ln44">#include &quot;yb/rocksdb/db/filename.h&quot;</a>
<a name="ln45">#include &quot;yb/rocksdb/db/file_numbers.h&quot;</a>
<a name="ln46">#include &quot;yb/rocksdb/db/log_reader.h&quot;</a>
<a name="ln47">#include &quot;yb/rocksdb/db/log_writer.h&quot;</a>
<a name="ln48">#include &quot;yb/rocksdb/db/memtable.h&quot;</a>
<a name="ln49">#include &quot;yb/rocksdb/db/memtable_list.h&quot;</a>
<a name="ln50">#include &quot;yb/rocksdb/db/merge_context.h&quot;</a>
<a name="ln51">#include &quot;yb/rocksdb/db/merge_helper.h&quot;</a>
<a name="ln52">#include &quot;yb/rocksdb/db/version_set.h&quot;</a>
<a name="ln53">#include &quot;yb/rocksdb/port/likely.h&quot;</a>
<a name="ln54">#include &quot;yb/rocksdb/port/port.h&quot;</a>
<a name="ln55">#include &quot;yb/rocksdb/db.h&quot;</a>
<a name="ln56">#include &quot;yb/rocksdb/env.h&quot;</a>
<a name="ln57">#include &quot;yb/rocksdb/statistics.h&quot;</a>
<a name="ln58">#include &quot;yb/rocksdb/status.h&quot;</a>
<a name="ln59">#include &quot;yb/rocksdb/table.h&quot;</a>
<a name="ln60">#include &quot;yb/rocksdb/table/block.h&quot;</a>
<a name="ln61">#include &quot;yb/rocksdb/table/block_based_table_factory.h&quot;</a>
<a name="ln62">#include &quot;yb/rocksdb/table/merger.h&quot;</a>
<a name="ln63">#include &quot;yb/rocksdb/table/table_builder.h&quot;</a>
<a name="ln64">#include &quot;yb/rocksdb/util/coding.h&quot;</a>
<a name="ln65">#include &quot;yb/rocksdb/util/file_reader_writer.h&quot;</a>
<a name="ln66">#include &quot;yb/rocksdb/util/log_buffer.h&quot;</a>
<a name="ln67">#include &quot;yb/rocksdb/util/logging.h&quot;</a>
<a name="ln68">#include &quot;yb/rocksdb/util/sst_file_manager_impl.h&quot;</a>
<a name="ln69">#include &quot;yb/rocksdb/util/mutexlock.h&quot;</a>
<a name="ln70">#include &quot;yb/rocksdb/util/perf_context_imp.h&quot;</a>
<a name="ln71">#include &quot;yb/rocksdb/util/stop_watch.h&quot;</a>
<a name="ln72">#include &quot;yb/rocksdb/util/sync_point.h&quot;</a>
<a name="ln73">#include &quot;yb/rocksdb/util/thread_status_util.h&quot;</a>
<a name="ln74"> </a>
<a name="ln75">#include &quot;yb/util/stats/iostats_context_imp.h&quot;</a>
<a name="ln76">#include &quot;yb/util/string_util.h&quot;</a>
<a name="ln77"> </a>
<a name="ln78">namespace rocksdb {</a>
<a name="ln79"> </a>
<a name="ln80">// Maintains state for each sub-compaction</a>
<a name="ln81">struct CompactionJob::SubcompactionState {</a>
<a name="ln82">  Compaction* compaction;</a>
<a name="ln83">  std::unique_ptr&lt;CompactionIterator&gt; c_iter;</a>
<a name="ln84"> </a>
<a name="ln85">  // The boundaries of the key-range this compaction is interested in. No two</a>
<a name="ln86">  // subcompactions may have overlapping key-ranges.</a>
<a name="ln87">  // 'start' is inclusive, 'end' is exclusive, and nullptr means unbounded</a>
<a name="ln88">  Slice *start, *end;</a>
<a name="ln89"> </a>
<a name="ln90">  // The return status of this subcompaction</a>
<a name="ln91">  Status status;</a>
<a name="ln92"> </a>
<a name="ln93">  // Files produced by this subcompaction</a>
<a name="ln94">  struct Output {</a>
<a name="ln95">    FileMetaData meta;</a>
<a name="ln96">    bool finished;</a>
<a name="ln97">    std::shared_ptr&lt;const TableProperties&gt; table_properties;</a>
<a name="ln98">  };</a>
<a name="ln99"> </a>
<a name="ln100">  // State kept for output being generated</a>
<a name="ln101">  std::vector&lt;Output&gt; outputs;</a>
<a name="ln102">  std::unique_ptr&lt;WritableFileWriter&gt; base_outfile;</a>
<a name="ln103">  std::unique_ptr&lt;WritableFileWriter&gt; data_outfile;</a>
<a name="ln104">  std::unique_ptr&lt;TableBuilder&gt; builder;</a>
<a name="ln105">  Output* current_output() {</a>
<a name="ln106">    if (outputs.empty()) {</a>
<a name="ln107">      // This subcompaction's outptut could be empty if compaction was aborted</a>
<a name="ln108">      // before this subcompaction had a chance to generate any output files.</a>
<a name="ln109">      // When subcompactions are executed sequentially this is more likely and</a>
<a name="ln110">      // will be particulalry likely for the later subcompactions to be empty.</a>
<a name="ln111">      // Once they are run in parallel however it should be much rarer.</a>
<a name="ln112">      return nullptr;</a>
<a name="ln113">    } else {</a>
<a name="ln114">      return &amp;outputs.back();</a>
<a name="ln115">    }</a>
<a name="ln116">  }</a>
<a name="ln117"> </a>
<a name="ln118">  // State during the subcompaction</a>
<a name="ln119">  uint64_t total_bytes;</a>
<a name="ln120">  uint64_t num_input_records;</a>
<a name="ln121">  uint64_t num_output_records;</a>
<a name="ln122">  CompactionJobStats compaction_job_stats;</a>
<a name="ln123">  uint64_t approx_size;</a>
<a name="ln124"> </a>
<a name="ln125">  SubcompactionState(Compaction* c, Slice* _start, Slice* _end,</a>
<a name="ln126">                     uint64_t size = 0)</a>
<a name="ln127">      : compaction(c),</a>
<a name="ln128">        start(_start),</a>
<a name="ln129">        end(_end),</a>
<a name="ln130">        base_outfile(nullptr),</a>
<a name="ln131">        data_outfile(nullptr),</a>
<a name="ln132">        builder(nullptr),</a>
<a name="ln133">        total_bytes(0),</a>
<a name="ln134">        num_input_records(0),</a>
<a name="ln135">        num_output_records(0),</a>
<a name="ln136">        approx_size(size) {</a>
<a name="ln137">    assert(compaction != nullptr);</a>
<a name="ln138">  }</a>
<a name="ln139"> </a>
<a name="ln140">  SubcompactionState(SubcompactionState&amp;&amp; o) { *this = std::move(o); }</a>
<a name="ln141"> </a>
<a name="ln142">  SubcompactionState&amp; operator=(SubcompactionState&amp;&amp; o) {</a>
<a name="ln143">    compaction = std::move(o.compaction);</a>
<a name="ln144">    start = std::move(o.start);</a>
<a name="ln145">    end = std::move(o.end);</a>
<a name="ln146">    status = std::move(o.status);</a>
<a name="ln147">    outputs = std::move(o.outputs);</a>
<a name="ln148">    base_outfile = std::move(o.base_outfile);</a>
<a name="ln149">    data_outfile = std::move(o.data_outfile);</a>
<a name="ln150">    builder = std::move(o.builder);</a>
<a name="ln151">    total_bytes = std::move(o.total_bytes);</a>
<a name="ln152">    num_input_records = std::move(o.num_input_records);</a>
<a name="ln153">    num_output_records = std::move(o.num_output_records);</a>
<a name="ln154">    compaction_job_stats = std::move(o.compaction_job_stats);</a>
<a name="ln155">    approx_size = std::move(o.approx_size);</a>
<a name="ln156">    return *this;</a>
<a name="ln157">  }</a>
<a name="ln158"> </a>
<a name="ln159">  // Because member unique_ptrs do not have these.</a>
<a name="ln160">  SubcompactionState(const SubcompactionState&amp;) = delete;</a>
<a name="ln161"> </a>
<a name="ln162">  SubcompactionState&amp; operator=(const SubcompactionState&amp;) = delete;</a>
<a name="ln163">};</a>
<a name="ln164"> </a>
<a name="ln165">// Maintains state for the entire compaction</a>
<a name="ln166">struct CompactionJob::CompactionState {</a>
<a name="ln167">  Compaction* const compaction;</a>
<a name="ln168"> </a>
<a name="ln169">  // REQUIRED: subcompaction states are stored in order of increasing</a>
<a name="ln170">  // key-range</a>
<a name="ln171">  std::vector&lt;CompactionJob::SubcompactionState&gt; sub_compact_states;</a>
<a name="ln172">  Status status;</a>
<a name="ln173"> </a>
<a name="ln174">  uint64_t total_bytes;</a>
<a name="ln175">  uint64_t num_input_records;</a>
<a name="ln176">  uint64_t num_output_records;</a>
<a name="ln177"> </a>
<a name="ln178">  explicit CompactionState(Compaction* c)</a>
<a name="ln179">      : compaction(c),</a>
<a name="ln180">        total_bytes(0),</a>
<a name="ln181">        num_input_records(0),</a>
<a name="ln182">        num_output_records(0) {}</a>
<a name="ln183"> </a>
<a name="ln184">  size_t NumOutputFiles() {</a>
<a name="ln185">    size_t total = 0;</a>
<a name="ln186">    for (auto&amp; s : sub_compact_states) {</a>
<a name="ln187">      total += s.outputs.size();</a>
<a name="ln188">    }</a>
<a name="ln189">    return total;</a>
<a name="ln190">  }</a>
<a name="ln191"> </a>
<a name="ln192">  Slice SmallestUserKey() {</a>
<a name="ln193">    for (const auto&amp; sub_compact_state : sub_compact_states) {</a>
<a name="ln194">      if (!sub_compact_state.outputs.empty() &amp;&amp;</a>
<a name="ln195">          sub_compact_state.outputs[0].finished) {</a>
<a name="ln196">        return sub_compact_state.outputs[0].meta.smallest.key.user_key();</a>
<a name="ln197">      }</a>
<a name="ln198">    }</a>
<a name="ln199">    // If there is no finished output, return an empty slice.</a>
<a name="ln200">    return Slice();</a>
<a name="ln201">  }</a>
<a name="ln202"> </a>
<a name="ln203">  Slice LargestUserKey() {</a>
<a name="ln204">    for (auto it = sub_compact_states.rbegin(); it &lt; sub_compact_states.rend();</a>
<a name="ln205">         ++it) {</a>
<a name="ln206">      if (!it-&gt;outputs.empty() &amp;&amp; it-&gt;current_output()-&gt;finished) {</a>
<a name="ln207">        assert(it-&gt;current_output() != nullptr);</a>
<a name="ln208">        return it-&gt;current_output()-&gt;meta.largest.key.user_key();</a>
<a name="ln209">      }</a>
<a name="ln210">    }</a>
<a name="ln211">    // If there is no finished output, return an empty slice.</a>
<a name="ln212">    return Slice();</a>
<a name="ln213">  }</a>
<a name="ln214">};</a>
<a name="ln215"> </a>
<a name="ln216">void CompactionJob::AggregateStatistics() {</a>
<a name="ln217">  for (SubcompactionState&amp; sc : compact_-&gt;sub_compact_states) {</a>
<a name="ln218">    compact_-&gt;total_bytes += sc.total_bytes;</a>
<a name="ln219">    compact_-&gt;num_input_records += sc.num_input_records;</a>
<a name="ln220">    compact_-&gt;num_output_records += sc.num_output_records;</a>
<a name="ln221">  }</a>
<a name="ln222">  if (compaction_job_stats_) {</a>
<a name="ln223">    for (SubcompactionState&amp; sc : compact_-&gt;sub_compact_states) {</a>
<a name="ln224">      compaction_job_stats_-&gt;Add(sc.compaction_job_stats);</a>
<a name="ln225">    }</a>
<a name="ln226">  }</a>
<a name="ln227">}</a>
<a name="ln228"> </a>
<a name="ln229">CompactionJob::CompactionJob(</a>
<a name="ln230">    int job_id, Compaction* compaction, const DBOptions&amp; db_options,</a>
<a name="ln231">    const EnvOptions&amp; env_options, VersionSet* versions,</a>
<a name="ln232">    std::atomic&lt;bool&gt;* shutting_down, LogBuffer* log_buffer,</a>
<a name="ln233">    Directory* db_directory, Directory* output_directory, Statistics* stats,</a>
<a name="ln234">    InstrumentedMutex* db_mutex, Status* db_bg_error,</a>
<a name="ln235">    std::vector&lt;SequenceNumber&gt; existing_snapshots,</a>
<a name="ln236">    SequenceNumber earliest_write_conflict_snapshot,</a>
<a name="ln237">    FileNumbersProvider* file_numbers_provider,</a>
<a name="ln238">    std::shared_ptr&lt;Cache&gt; table_cache, EventLogger* event_logger,</a>
<a name="ln239">    bool paranoid_file_checks, bool measure_io_stats, const std::string&amp; dbname,</a>
<a name="ln240">    CompactionJobStats* compaction_job_stats)</a>
<a name="ln241">    : job_id_(job_id),</a>
<a name="ln242">      compact_(new CompactionState(compaction)),</a>
<a name="ln243">      compaction_job_stats_(compaction_job_stats),</a>
<a name="ln244">      compaction_stats_(1),</a>
<a name="ln245">      dbname_(dbname),</a>
<a name="ln246">      db_options_(db_options),</a>
<a name="ln247">      env_options_(env_options),</a>
<a name="ln248">      env_(db_options.env),</a>
<a name="ln249">      versions_(versions),</a>
<a name="ln250">      shutting_down_(shutting_down),</a>
<a name="ln251">      log_buffer_(log_buffer),</a>
<a name="ln252">      db_directory_(db_directory),</a>
<a name="ln253">      output_directory_(output_directory),</a>
<a name="ln254">      stats_(stats),</a>
<a name="ln255">      db_mutex_(db_mutex),</a>
<a name="ln256">      db_bg_error_(db_bg_error),</a>
<a name="ln257">      existing_snapshots_(std::move(existing_snapshots)),</a>
<a name="ln258">      earliest_write_conflict_snapshot_(earliest_write_conflict_snapshot),</a>
<a name="ln259">      file_numbers_provider_(file_numbers_provider),</a>
<a name="ln260">      table_cache_(std::move(table_cache)),</a>
<a name="ln261">      event_logger_(event_logger),</a>
<a name="ln262">      paranoid_file_checks_(paranoid_file_checks),</a>
<a name="ln263">      measure_io_stats_(measure_io_stats) {</a>
<a name="ln264">  assert(log_buffer_ != nullptr);</a>
<a name="ln265">  const auto* cfd = compact_-&gt;compaction-&gt;column_family_data();</a>
<a name="ln266">  ThreadStatusUtil::SetColumnFamily(cfd, cfd-&gt;ioptions()-&gt;env,</a>
<a name="ln267">                                    cfd-&gt;options()-&gt;enable_thread_tracking);</a>
<a name="ln268">  ThreadStatusUtil::SetThreadOperation(ThreadStatus::OP_COMPACTION);</a>
<a name="ln269">  ReportStartedCompaction(compaction);</a>
<a name="ln270">}</a>
<a name="ln271"> </a>
<a name="ln272">CompactionJob::~CompactionJob() {</a>
<a name="ln273">  assert(compact_ == nullptr);</a>
<a name="ln274">  ThreadStatusUtil::ResetThreadStatus();</a>
<a name="ln275">}</a>
<a name="ln276"> </a>
<a name="ln277">void CompactionJob::ReportStartedCompaction(</a>
<a name="ln278">    Compaction* compaction) {</a>
<a name="ln279">  const auto* cfd = compact_-&gt;compaction-&gt;column_family_data();</a>
<a name="ln280">  ThreadStatusUtil::SetColumnFamily(cfd, cfd-&gt;ioptions()-&gt;env,</a>
<a name="ln281">                                    cfd-&gt;options()-&gt;enable_thread_tracking);</a>
<a name="ln282"> </a>
<a name="ln283">  ThreadStatusUtil::SetThreadOperationProperty(</a>
<a name="ln284">      ThreadStatus::COMPACTION_JOB_ID,</a>
<a name="ln285">      job_id_);</a>
<a name="ln286"> </a>
<a name="ln287">  ThreadStatusUtil::SetThreadOperationProperty(</a>
<a name="ln288">      ThreadStatus::COMPACTION_INPUT_OUTPUT_LEVEL,</a>
<a name="ln289">      (static_cast&lt;uint64_t&gt;(compact_-&gt;compaction-&gt;start_level()) &lt;&lt; 32) +</a>
<a name="ln290">          compact_-&gt;compaction-&gt;output_level());</a>
<a name="ln291"> </a>
<a name="ln292">  // In the current design, a CompactionJob is always created</a>
<a name="ln293">  // for non-trivial compaction.</a>
<a name="ln294">  assert(compaction-&gt;IsTrivialMove() == false ||</a>
<a name="ln295">         compaction-&gt;is_manual_compaction() == true);</a>
<a name="ln296"> </a>
<a name="ln297">  ThreadStatusUtil::SetThreadOperationProperty(</a>
<a name="ln298">      ThreadStatus::COMPACTION_PROP_FLAGS,</a>
<a name="ln299">      compaction-&gt;is_manual_compaction() +</a>
<a name="ln300">          (compaction-&gt;deletion_compaction() &lt;&lt; 1));</a>
<a name="ln301"> </a>
<a name="ln302">  ThreadStatusUtil::SetThreadOperationProperty(</a>
<a name="ln303">      ThreadStatus::COMPACTION_TOTAL_INPUT_BYTES,</a>
<a name="ln304">      compaction-&gt;CalculateTotalInputSize());</a>
<a name="ln305"> </a>
<a name="ln306">  IOSTATS_RESET(bytes_written);</a>
<a name="ln307">  IOSTATS_RESET(bytes_read);</a>
<a name="ln308">  ThreadStatusUtil::SetThreadOperationProperty(</a>
<a name="ln309">      ThreadStatus::COMPACTION_BYTES_WRITTEN, 0);</a>
<a name="ln310">  ThreadStatusUtil::SetThreadOperationProperty(</a>
<a name="ln311">      ThreadStatus::COMPACTION_BYTES_READ, 0);</a>
<a name="ln312"> </a>
<a name="ln313">  // Set the thread operation after operation properties</a>
<a name="ln314">  // to ensure GetThreadList() can always show them all together.</a>
<a name="ln315">  ThreadStatusUtil::SetThreadOperation(</a>
<a name="ln316">      ThreadStatus::OP_COMPACTION);</a>
<a name="ln317"> </a>
<a name="ln318">  if (compaction_job_stats_) {</a>
<a name="ln319">    compaction_job_stats_-&gt;is_manual_compaction =</a>
<a name="ln320">        compaction-&gt;is_manual_compaction();</a>
<a name="ln321">  }</a>
<a name="ln322">}</a>
<a name="ln323"> </a>
<a name="ln324">void CompactionJob::Prepare() {</a>
<a name="ln325">  AutoThreadOperationStageUpdater stage_updater(</a>
<a name="ln326">      ThreadStatus::STAGE_COMPACTION_PREPARE);</a>
<a name="ln327"> </a>
<a name="ln328">  // Generate file_levels_ for compaction berfore making Iterator</a>
<a name="ln329">  auto* c = compact_-&gt;compaction;</a>
<a name="ln330">  assert(c-&gt;column_family_data() != nullptr);</a>
<a name="ln331">  assert(c-&gt;column_family_data()-&gt;current()-&gt;storage_info()</a>
<a name="ln332">      -&gt;NumLevelFiles(compact_-&gt;compaction-&gt;level()) &gt; 0);</a>
<a name="ln333"> </a>
<a name="ln334">  // Is this compaction producing files at the bottommost level?</a>
<a name="ln335">  bottommost_level_ = c-&gt;bottommost_level();</a>
<a name="ln336"> </a>
<a name="ln337">  if (c-&gt;ShouldFormSubcompactions()) {</a>
<a name="ln338">    const uint64_t start_micros = env_-&gt;NowMicros();</a>
<a name="ln339">    GenSubcompactionBoundaries();</a>
<a name="ln340">    MeasureTime(stats_, SUBCOMPACTION_SETUP_TIME,</a>
<a name="ln341">                env_-&gt;NowMicros() - start_micros);</a>
<a name="ln342"> </a>
<a name="ln343">    assert(sizes_.size() == boundaries_.size() + 1);</a>
<a name="ln344"> </a>
<a name="ln345">    for (size_t i = 0; i &lt;= boundaries_.size(); i++) {</a>
<a name="ln346">      Slice* start = i == 0 ? nullptr : &amp;boundaries_[i - 1];</a>
<a name="ln347">      Slice* end = i == boundaries_.size() ? nullptr : &amp;boundaries_[i];</a>
<a name="ln348">      compact_-&gt;sub_compact_states.emplace_back(c, start, end, sizes_[i]);</a>
<a name="ln349">    }</a>
<a name="ln350">    MeasureTime(stats_, NUM_SUBCOMPACTIONS_SCHEDULED,</a>
<a name="ln351">                compact_-&gt;sub_compact_states.size());</a>
<a name="ln352">  } else {</a>
<a name="ln353">    compact_-&gt;sub_compact_states.emplace_back(c, nullptr, nullptr);</a>
<a name="ln354">  }</a>
<a name="ln355">}</a>
<a name="ln356"> </a>
<a name="ln357">struct RangeWithSize {</a>
<a name="ln358">  Range range;</a>
<a name="ln359">  uint64_t size;</a>
<a name="ln360"> </a>
<a name="ln361">  RangeWithSize(const Slice&amp; a, const Slice&amp; b, uint64_t s = 0)</a>
<a name="ln362">      : range(a, b), size(s) {}</a>
<a name="ln363">};</a>
<a name="ln364"> </a>
<a name="ln365">// Generates a histogram representing potential divisions of key ranges from</a>
<a name="ln366">// the input. It adds the starting and/or ending keys of certain input files</a>
<a name="ln367">// to the working set and then finds the approximate size of data in between</a>
<a name="ln368">// each consecutive pair of slices. Then it divides these ranges into</a>
<a name="ln369">// consecutive groups such that each group has a similar size.</a>
<a name="ln370">void CompactionJob::GenSubcompactionBoundaries() {</a>
<a name="ln371">  auto* c = compact_-&gt;compaction;</a>
<a name="ln372">  auto* cfd = c-&gt;column_family_data();</a>
<a name="ln373">  const Comparator* cfd_comparator = cfd-&gt;user_comparator();</a>
<a name="ln374">  std::vector&lt;Slice&gt; bounds;</a>
<a name="ln375">  int start_lvl = c-&gt;start_level();</a>
<a name="ln376">  int out_lvl = c-&gt;output_level();</a>
<a name="ln377"> </a>
<a name="ln378">  // Add the starting and/or ending key of certain input files as a potential</a>
<a name="ln379">  // boundary</a>
<a name="ln380">  for (size_t lvl_idx = 0; lvl_idx &lt; c-&gt;num_input_levels(); lvl_idx++) {</a>
<a name="ln381">    int lvl = c-&gt;level(lvl_idx);</a>
<a name="ln382">    if (lvl &gt;= start_lvl &amp;&amp; lvl &lt;= out_lvl) {</a>
<a name="ln383">      const LevelFilesBrief* flevel = c-&gt;input_levels(lvl_idx);</a>
<a name="ln384">      size_t num_files = flevel-&gt;num_files;</a>
<a name="ln385"> </a>
<a name="ln386">      if (num_files == 0) {</a>
<a name="ln387">        continue;</a>
<a name="ln388">      }</a>
<a name="ln389"> </a>
<a name="ln390">      if (lvl == 0) {</a>
<a name="ln391">        // For level 0 add the starting and ending key of each file since the</a>
<a name="ln392">        // files may have greatly differing key ranges (not range-partitioned)</a>
<a name="ln393">        for (size_t i = 0; i &lt; num_files; i++) {</a>
<a name="ln394">          bounds.emplace_back(flevel-&gt;files[i].smallest.key);</a>
<a name="ln395">          bounds.emplace_back(flevel-&gt;files[i].largest.key);</a>
<a name="ln396">        }</a>
<a name="ln397">      } else {</a>
<a name="ln398">        // For all other levels add the smallest/largest key in the level to</a>
<a name="ln399">        // encompass the range covered by that level</a>
<a name="ln400">        bounds.emplace_back(flevel-&gt;files[0].smallest.key);</a>
<a name="ln401">        bounds.emplace_back(flevel-&gt;files[num_files - 1].largest.key);</a>
<a name="ln402">        if (lvl == out_lvl) {</a>
<a name="ln403">          // For the last level include the starting keys of all files since</a>
<a name="ln404">          // the last level is the largest and probably has the widest key</a>
<a name="ln405">          // range. Since it's range partitioned, the ending key of one file</a>
<a name="ln406">          // and the starting key of the next are very close (or identical).</a>
<a name="ln407">          for (size_t i = 1; i &lt; num_files; i++) {</a>
<a name="ln408">            bounds.emplace_back(flevel-&gt;files[i].smallest.key);</a>
<a name="ln409">          }</a>
<a name="ln410">        }</a>
<a name="ln411">      }</a>
<a name="ln412">    }</a>
<a name="ln413">  }</a>
<a name="ln414"> </a>
<a name="ln415">  std::sort(bounds.begin(), bounds.end(),</a>
<a name="ln416">    [cfd_comparator] (const Slice&amp; a, const Slice&amp; b) -&gt; bool {</a>
<a name="ln417">      return cfd_comparator-&gt;Compare(ExtractUserKey(a), ExtractUserKey(b)) &lt; 0;</a>
<a name="ln418">    });</a>
<a name="ln419">  // Remove duplicated entries from bounds</a>
<a name="ln420">  bounds.erase(std::unique(bounds.begin(), bounds.end(),</a>
<a name="ln421">    [cfd_comparator] (const Slice&amp; a, const Slice&amp; b) -&gt; bool {</a>
<a name="ln422">      return cfd_comparator-&gt;Compare(ExtractUserKey(a), ExtractUserKey(b)) == 0;</a>
<a name="ln423">    }), bounds.end());</a>
<a name="ln424"> </a>
<a name="ln425">  // Combine consecutive pairs of boundaries into ranges with an approximate</a>
<a name="ln426">  // size of data covered by keys in that range</a>
<a name="ln427">  uint64_t sum = 0;</a>
<a name="ln428">  std::vector&lt;RangeWithSize&gt; ranges;</a>
<a name="ln429">  auto* v = cfd-&gt;current();</a>
<a name="ln430">  for (auto it = bounds.begin();;) {</a>
<a name="ln431">    const Slice a = *it;</a>
<a name="ln432">    it++;</a>
<a name="ln433"> </a>
<a name="ln434">    if (it == bounds.end()) {</a>
<a name="ln435">      break;</a>
<a name="ln436">    }</a>
<a name="ln437"> </a>
<a name="ln438">    const Slice b = *it;</a>
<a name="ln439">    uint64_t size = versions_-&gt;ApproximateSize(v, a, b, start_lvl, out_lvl + 1);</a>
<a name="ln440">    ranges.emplace_back(a, b, size);</a>
<a name="ln441">    sum += size;</a>
<a name="ln442">  }</a>
<a name="ln443"> </a>
<a name="ln444">  // Group the ranges into subcompactions</a>
<a name="ln445">  const double min_file_fill_percent = 4.0 / 5;</a>
<a name="ln446">  uint64_t max_output_files = static_cast&lt;uint64_t&gt;(std::ceil(</a>
<a name="ln447">      sum / min_file_fill_percent /</a>
<a name="ln448">      cfd-&gt;GetCurrentMutableCFOptions()-&gt;MaxFileSizeForLevel(out_lvl)));</a>
<a name="ln449">  uint64_t subcompactions =</a>
<a name="ln450">      std::min({static_cast&lt;uint64_t&gt;(ranges.size()),</a>
<a name="ln451">                static_cast&lt;uint64_t&gt;(db_options_.max_subcompactions),</a>
<a name="ln452">                max_output_files});</a>
<a name="ln453"> </a>
<a name="ln454">  double mean = subcompactions != 0 ? sum * 1.0 / subcompactions</a>
<a name="ln455">                                    : std::numeric_limits&lt;double&gt;::max();</a>
<a name="ln456"> </a>
<a name="ln457">  if (subcompactions &gt; 1) {</a>
<a name="ln458">    // Greedily add ranges to the subcompaction until the sum of the ranges'</a>
<a name="ln459">    // sizes becomes &gt;= the expected mean size of a subcompaction</a>
<a name="ln460">    sum = 0;</a>
<a name="ln461">    for (size_t i = 0; i &lt; ranges.size() - 1; i++) {</a>
<a name="ln462">      sum += ranges[i].size;</a>
<a name="ln463">      if (subcompactions == 1) {</a>
<a name="ln464">        // If there's only one left to schedule then it goes to the end so no</a>
<a name="ln465">        // need to put an end boundary</a>
<a name="ln466">        continue;</a>
<a name="ln467">      }</a>
<a name="ln468">      if (sum &gt;= mean) {</a>
<a name="ln469">        boundaries_.emplace_back(ExtractUserKey(ranges[i].range.limit));</a>
<a name="ln470">        sizes_.emplace_back(sum);</a>
<a name="ln471">        subcompactions--;</a>
<a name="ln472">        sum = 0;</a>
<a name="ln473">      }</a>
<a name="ln474">    }</a>
<a name="ln475">    sizes_.emplace_back(sum + ranges.back().size);</a>
<a name="ln476">  } else {</a>
<a name="ln477">    // Only one range so its size is the total sum of sizes computed above</a>
<a name="ln478">    sizes_.emplace_back(sum);</a>
<a name="ln479">  }</a>
<a name="ln480">}</a>
<a name="ln481"> </a>
<a name="ln482">Result&lt;FileNumbersHolder&gt; CompactionJob::Run() {</a>
<a name="ln483">  AutoThreadOperationStageUpdater stage_updater(</a>
<a name="ln484">      ThreadStatus::STAGE_COMPACTION_RUN);</a>
<a name="ln485">  TEST_SYNC_POINT(&quot;CompactionJob::Run():Start&quot;);</a>
<a name="ln486">  log_buffer_-&gt;FlushBufferToLog();</a>
<a name="ln487">  LogCompaction();</a>
<a name="ln488">  for (auto listener : db_options_.listeners) {</a>
<a name="ln489">    listener-&gt;OnCompactionStarted();</a>
<a name="ln490">  }</a>
<a name="ln491"> </a>
<a name="ln492">  const size_t num_threads = compact_-&gt;sub_compact_states.size();</a>
<a name="ln493">  assert(num_threads &gt; 0);</a>
<a name="ln494">  const uint64_t start_micros = env_-&gt;NowMicros();</a>
<a name="ln495"> </a>
<a name="ln496">  // Launch a thread for each of subcompactions 1...num_threads-1</a>
<a name="ln497">  std::vector&lt;std::thread&gt; thread_pool;</a>
<a name="ln498">  thread_pool.reserve(num_threads - 1);</a>
<a name="ln499">  FileNumbersHolder file_numbers_holder(file_numbers_provider_-&gt;CreateHolder());</a>
<a name="ln500">  file_numbers_holder.Reserve(num_threads);</a>
<a name="ln501">  for (size_t i = 1; i &lt; compact_-&gt;sub_compact_states.size(); i++) {</a>
<a name="ln502">    thread_pool.emplace_back(&amp;CompactionJob::ProcessKeyValueCompaction, this, &amp;file_numbers_holder,</a>
<a name="ln503">                             &amp;compact_-&gt;sub_compact_states[i]);</a>
<a name="ln504">  }</a>
<a name="ln505"> </a>
<a name="ln506">  // Always schedule the first subcompaction (whether or not there are also</a>
<a name="ln507">  // others) in the current thread to be efficient with resources</a>
<a name="ln508">  ProcessKeyValueCompaction(&amp;file_numbers_holder, &amp;compact_-&gt;sub_compact_states[0]);</a>
<a name="ln509"> </a>
<a name="ln510">  // Wait for all other threads (if there are any) to finish execution</a>
<a name="ln511">  for (auto&amp; thread : thread_pool) {</a>
<a name="ln512">    thread.join();</a>
<a name="ln513">  }</a>
<a name="ln514"> </a>
<a name="ln515">  if (output_directory_ &amp;&amp; !db_options_.disableDataSync) {</a>
<a name="ln516">    RETURN_NOT_OK(output_directory_-&gt;Fsync());</a>
<a name="ln517">  }</a>
<a name="ln518"> </a>
<a name="ln519">  compaction_stats_.micros = env_-&gt;NowMicros() - start_micros;</a>
<a name="ln520">  MeasureTime(stats_, COMPACTION_TIME, compaction_stats_.micros);</a>
<a name="ln521"> </a>
<a name="ln522">  // Check if any thread encountered an error during execution</a>
<a name="ln523">  Status status;</a>
<a name="ln524">  for (const auto&amp; state : compact_-&gt;sub_compact_states) {</a>
<a name="ln525">    if (!state.status.ok()) {</a>
<a name="ln526">      status = state.status;</a>
<a name="ln527">      break;</a>
<a name="ln528">    }</a>
<a name="ln529">  }</a>
<a name="ln530"> </a>
<a name="ln531">  TablePropertiesCollection tp;</a>
<a name="ln532">  for (const auto&amp; state : compact_-&gt;sub_compact_states) {</a>
<a name="ln533">    for (const auto&amp; output : state.outputs) {</a>
<a name="ln534">      auto fn = TableFileName(db_options_.db_paths, output.meta.fd.GetNumber(),</a>
<a name="ln535">                              output.meta.fd.GetPathId());</a>
<a name="ln536">      tp[fn] = output.table_properties;</a>
<a name="ln537">    }</a>
<a name="ln538">  }</a>
<a name="ln539">  compact_-&gt;compaction-&gt;SetOutputTableProperties(std::move(tp));</a>
<a name="ln540"> </a>
<a name="ln541">  // Finish up all book-keeping to unify the subcompaction results</a>
<a name="ln542">  AggregateStatistics();</a>
<a name="ln543">  UpdateCompactionStats();</a>
<a name="ln544">  RecordCompactionIOStats();</a>
<a name="ln545">  LogFlush(db_options_.info_log);</a>
<a name="ln546">  TEST_SYNC_POINT(&quot;CompactionJob::Run():End&quot;);</a>
<a name="ln547"> </a>
<a name="ln548">  compact_-&gt;status = status;</a>
<a name="ln549">  return file_numbers_holder;</a>
<a name="ln550">}</a>
<a name="ln551"> </a>
<a name="ln552">Status CompactionJob::Install(const MutableCFOptions&amp; mutable_cf_options) {</a>
<a name="ln553">  AutoThreadOperationStageUpdater stage_updater(</a>
<a name="ln554">      ThreadStatus::STAGE_COMPACTION_INSTALL);</a>
<a name="ln555">  db_mutex_-&gt;AssertHeld();</a>
<a name="ln556">  Status status = compact_-&gt;status;</a>
<a name="ln557">  ColumnFamilyData* cfd = compact_-&gt;compaction-&gt;column_family_data();</a>
<a name="ln558">  cfd-&gt;internal_stats()-&gt;AddCompactionStats(</a>
<a name="ln559">      compact_-&gt;compaction-&gt;output_level(), compaction_stats_);</a>
<a name="ln560"> </a>
<a name="ln561">  if (status.ok()) {</a>
<a name="ln562">    status = InstallCompactionResults(mutable_cf_options);</a>
<a name="ln563">  }</a>
<a name="ln564">  VersionStorageInfo::LevelSummaryStorage tmp;</a>
<a name="ln565">  auto vstorage = cfd-&gt;current()-&gt;storage_info();</a>
<a name="ln566">  const auto&amp; stats = compaction_stats_;</a>
<a name="ln567">  const auto micros = static_cast&lt;double&gt;(std::max&lt;uint64_t&gt;(stats.micros, 1));</a>
<a name="ln568">  const auto bytes_read_non_output_levels = static_cast&lt;double&gt;(</a>
<a name="ln569">      std::max&lt;uint64_t&gt;(stats.bytes_read_non_output_levels, 1));</a>
<a name="ln570">  LOG_TO_BUFFER(</a>
<a name="ln571">      log_buffer_,</a>
<a name="ln572">      &quot;[%s] compacted to: %s, MB/sec: %.1f rd, %.1f wr, level %d, &quot;</a>
<a name="ln573">      &quot;files in(%d, %d) out(%d) &quot;</a>
<a name="ln574">      &quot;MB in(%.1f, %.1f) out(%.1f), read-write-amplify(%.1f) &quot;</a>
<a name="ln575">      &quot;write-amplify(%.1f) %s, records in: %d, records dropped: %d\n&quot;,</a>
<a name="ln576">      cfd-&gt;GetName().c_str(), vstorage-&gt;LevelSummary(&amp;tmp),</a>
<a name="ln577">      (stats.bytes_read_non_output_levels + stats.bytes_read_output_level) /</a>
<a name="ln578">          static_cast&lt;double&gt;(stats.micros),</a>
<a name="ln579">      stats.bytes_written / static_cast&lt;double&gt;(stats.micros),</a>
<a name="ln580">      compact_-&gt;compaction-&gt;output_level(),</a>
<a name="ln581">      stats.num_input_files_in_non_output_levels,</a>
<a name="ln582">      stats.num_input_files_in_output_level,</a>
<a name="ln583">      stats.num_output_files,</a>
<a name="ln584">      stats.bytes_read_non_output_levels / 1048576.0,</a>
<a name="ln585">      stats.bytes_read_output_level / 1048576.0,</a>
<a name="ln586">      stats.bytes_written / 1048576.0,</a>
<a name="ln587">      (stats.bytes_written + stats.bytes_read_output_level + stats.bytes_read_non_output_levels) /</a>
<a name="ln588">          bytes_read_non_output_levels,</a>
<a name="ln589">      stats.bytes_written / bytes_read_non_output_levels,</a>
<a name="ln590">      status.ToString().c_str(), stats.num_input_records,</a>
<a name="ln591">      stats.num_dropped_records);</a>
<a name="ln592"> </a>
<a name="ln593">  UpdateCompactionJobStats(stats);</a>
<a name="ln594"> </a>
<a name="ln595">  auto stream = event_logger_-&gt;LogToBuffer(log_buffer_);</a>
<a name="ln596">  stream &lt;&lt; &quot;job&quot; &lt;&lt; job_id_</a>
<a name="ln597">         &lt;&lt; &quot;event&quot; &lt;&lt; &quot;compaction_finished&quot;</a>
<a name="ln598">         &lt;&lt; &quot;compaction_time_micros&quot; &lt;&lt; compaction_stats_.micros</a>
<a name="ln599">         &lt;&lt; &quot;output_level&quot; &lt;&lt; compact_-&gt;compaction-&gt;output_level()</a>
<a name="ln600">         &lt;&lt; &quot;num_output_files&quot; &lt;&lt; compact_-&gt;NumOutputFiles()</a>
<a name="ln601">         &lt;&lt; &quot;total_output_size&quot; &lt;&lt; compact_-&gt;total_bytes</a>
<a name="ln602">         &lt;&lt; &quot;num_input_records&quot; &lt;&lt; compact_-&gt;num_input_records</a>
<a name="ln603">         &lt;&lt; &quot;num_output_records&quot; &lt;&lt; compact_-&gt;num_output_records</a>
<a name="ln604">         &lt;&lt; &quot;num_subcompactions&quot; &lt;&lt; compact_-&gt;sub_compact_states.size();</a>
<a name="ln605"> </a>
<a name="ln606">  if (measure_io_stats_ &amp;&amp; compaction_job_stats_ != nullptr) {</a>
<a name="ln607">    stream &lt;&lt; &quot;file_write_nanos&quot; &lt;&lt; compaction_job_stats_-&gt;file_write_nanos;</a>
<a name="ln608">    stream &lt;&lt; &quot;file_range_sync_nanos&quot;</a>
<a name="ln609">           &lt;&lt; compaction_job_stats_-&gt;file_range_sync_nanos;</a>
<a name="ln610">    stream &lt;&lt; &quot;file_fsync_nanos&quot; &lt;&lt; compaction_job_stats_-&gt;file_fsync_nanos;</a>
<a name="ln611">    stream &lt;&lt; &quot;file_prepare_write_nanos&quot;</a>
<a name="ln612">           &lt;&lt; compaction_job_stats_-&gt;file_prepare_write_nanos;</a>
<a name="ln613">  }</a>
<a name="ln614"> </a>
<a name="ln615">  stream &lt;&lt; &quot;lsm_state&quot;;</a>
<a name="ln616">  stream.StartArray();</a>
<a name="ln617">  for (int level = 0; level &lt; vstorage-&gt;num_levels(); ++level) {</a>
<a name="ln618">    stream &lt;&lt; vstorage-&gt;NumLevelFiles(level);</a>
<a name="ln619">  }</a>
<a name="ln620">  stream.EndArray();</a>
<a name="ln621"> </a>
<a name="ln622">  CleanupCompaction();</a>
<a name="ln623">  return status;</a>
<a name="ln624">}</a>
<a name="ln625"> </a>
<a name="ln626">void CompactionJob::ProcessKeyValueCompaction(</a>
<a name="ln627">    FileNumbersHolder* holder, SubcompactionState* sub_compact) {</a>
<a name="ln628">  assert(sub_compact != nullptr);</a>
<a name="ln629">  std::unique_ptr&lt;InternalIterator&gt; input(</a>
<a name="ln630">      versions_-&gt;MakeInputIterator(sub_compact-&gt;compaction));</a>
<a name="ln631"> </a>
<a name="ln632">  AutoThreadOperationStageUpdater stage_updater(</a>
<a name="ln633">      ThreadStatus::STAGE_COMPACTION_PROCESS_KV);</a>
<a name="ln634"> </a>
<a name="ln635">  // I/O measurement variables</a>
<a name="ln636">  PerfLevel prev_perf_level = PerfLevel::kEnableTime;</a>
<a name="ln637">  const uint64_t kRecordStatsEvery = 1000;</a>
<a name="ln638">  uint64_t prev_write_nanos = 0;</a>
<a name="ln639">  uint64_t prev_fsync_nanos = 0;</a>
<a name="ln640">  uint64_t prev_range_sync_nanos = 0;</a>
<a name="ln641">  uint64_t prev_prepare_write_nanos = 0;</a>
<a name="ln642">  if (measure_io_stats_) {</a>
<a name="ln643">    prev_perf_level = GetPerfLevel();</a>
<a name="ln644">    SetPerfLevel(PerfLevel::kEnableTime);</a>
<a name="ln645">    prev_write_nanos = IOSTATS(write_nanos);</a>
<a name="ln646">    prev_fsync_nanos = IOSTATS(fsync_nanos);</a>
<a name="ln647">    prev_range_sync_nanos = IOSTATS(range_sync_nanos);</a>
<a name="ln648">    prev_prepare_write_nanos = IOSTATS(prepare_write_nanos);</a>
<a name="ln649">  }</a>
<a name="ln650"> </a>
<a name="ln651">  ColumnFamilyData* cfd = sub_compact-&gt;compaction-&gt;column_family_data();</a>
<a name="ln652">  auto compaction_filter = cfd-&gt;ioptions()-&gt;compaction_filter;</a>
<a name="ln653">  std::unique_ptr&lt;CompactionFilter&gt; compaction_filter_from_factory = nullptr;</a>
<a name="ln654">  if (compaction_filter == nullptr) {</a>
<a name="ln655">    compaction_filter_from_factory =</a>
<a name="ln656">        sub_compact-&gt;compaction-&gt;CreateCompactionFilter();</a>
<a name="ln657">    compaction_filter = compaction_filter_from_factory.get();</a>
<a name="ln658">  }</a>
<a name="ln659"> </a>
<a name="ln660">  if (compaction_filter) {</a>
<a name="ln661">    // This is used to persist the history cutoff hybrid time chosen for the DocDB compaction</a>
<a name="ln662">    // filter.</a>
<a name="ln663">    largest_user_frontier_ = compaction_filter-&gt;GetLargestUserFrontier();</a>
<a name="ln664">  }</a>
<a name="ln665"> </a>
<a name="ln666">  MergeHelper merge(</a>
<a name="ln667">      env_, cfd-&gt;user_comparator(), cfd-&gt;ioptions()-&gt;merge_operator,</a>
<a name="ln668">      compaction_filter, db_options_.info_log.get(),</a>
<a name="ln669">      cfd-&gt;ioptions()-&gt;min_partial_merge_operands,</a>
<a name="ln670">      false /* internal key corruption is expected */,</a>
<a name="ln671">      existing_snapshots_.empty() ? 0 : existing_snapshots_.back(),</a>
<a name="ln672">      compact_-&gt;compaction-&gt;level(), db_options_.statistics.get());</a>
<a name="ln673"> </a>
<a name="ln674">  TEST_SYNC_POINT(&quot;CompactionJob::Run():Inprogress&quot;);</a>
<a name="ln675"> </a>
<a name="ln676">  Slice* start = sub_compact-&gt;start;</a>
<a name="ln677">  Slice* end = sub_compact-&gt;end;</a>
<a name="ln678">  if (start != nullptr) {</a>
<a name="ln679">    IterKey start_iter;</a>
<a name="ln680">    start_iter.SetInternalKey(*start, kMaxSequenceNumber, kValueTypeForSeek);</a>
<a name="ln681">    input-&gt;Seek(start_iter.GetKey());</a>
<a name="ln682">  } else {</a>
<a name="ln683">    input-&gt;SeekToFirst();</a>
<a name="ln684">  }</a>
<a name="ln685"> </a>
<a name="ln686">  Status status;</a>
<a name="ln687">  sub_compact-&gt;c_iter.reset(new CompactionIterator(</a>
<a name="ln688">      input.get(), cfd-&gt;user_comparator(), &amp;merge, versions_-&gt;LastSequence(),</a>
<a name="ln689">      &amp;existing_snapshots_, earliest_write_conflict_snapshot_, false,</a>
<a name="ln690">      sub_compact-&gt;compaction, compaction_filter));</a>
<a name="ln691">  auto c_iter = sub_compact-&gt;c_iter.get();</a>
<a name="ln692">  c_iter-&gt;SeekToFirst();</a>
<a name="ln693">  const auto&amp; c_iter_stats = c_iter-&gt;iter_stats();</a>
<a name="ln694">  // TODO(noetzli): check whether we could check !shutting_down_-&gt;... only</a>
<a name="ln695">  // only occasionally (see diff D42687)</a>
<a name="ln696">  while (status.ok() &amp;&amp; !shutting_down_-&gt;load(std::memory_order_acquire) &amp;&amp;</a>
<a name="ln697">         !cfd-&gt;IsDropped() &amp;&amp; c_iter-&gt;Valid()) {</a>
<a name="ln698">    // Invariant: c_iter.status() is guaranteed to be OK if c_iter-&gt;Valid()</a>
<a name="ln699">    // returns true.</a>
<a name="ln700">    const Slice&amp; key = c_iter-&gt;key();</a>
<a name="ln701">    const Slice&amp; value = c_iter-&gt;value();</a>
<a name="ln702"> </a>
<a name="ln703">    // If an end key (exclusive) is specified, check if the current key is</a>
<a name="ln704">    // &gt;= than it and exit if it is because the iterator is out of its range</a>
<a name="ln705">    if (end != nullptr &amp;&amp;</a>
<a name="ln706">        cfd-&gt;user_comparator()-&gt;Compare(c_iter-&gt;user_key(), *end) &gt;= 0) {</a>
<a name="ln707">      break;</a>
<a name="ln708">    } else if (sub_compact-&gt;compaction-&gt;ShouldStopBefore(key) &amp;&amp;</a>
<a name="ln709">               sub_compact-&gt;builder != nullptr) {</a>
<a name="ln710">      status = FinishCompactionOutputFile(input-&gt;status(), sub_compact);</a>
<a name="ln711">      if (!status.ok()) {</a>
<a name="ln712">        break;</a>
<a name="ln713">      }</a>
<a name="ln714">    }</a>
<a name="ln715"> </a>
<a name="ln716">    if (c_iter_stats.num_input_records % kRecordStatsEvery ==</a>
<a name="ln717">        kRecordStatsEvery - 1) {</a>
<a name="ln718">      RecordDroppedKeys(c_iter_stats, &amp;sub_compact-&gt;compaction_job_stats);</a>
<a name="ln719">      c_iter-&gt;ResetRecordCounts();</a>
<a name="ln720">      RecordCompactionIOStats();</a>
<a name="ln721">    }</a>
<a name="ln722"> </a>
<a name="ln723">    // Open output file if necessary</a>
<a name="ln724">    if (sub_compact-&gt;builder == nullptr) {</a>
<a name="ln725">      status = OpenCompactionOutputFile(holder, sub_compact);</a>
<a name="ln726">      if (!status.ok()) {</a>
<a name="ln727">        break;</a>
<a name="ln728">      }</a>
<a name="ln729">    }</a>
<a name="ln730">    assert(sub_compact-&gt;builder != nullptr);</a>
<a name="ln731">    assert(sub_compact-&gt;current_output() != nullptr);</a>
<a name="ln732">    sub_compact-&gt;builder-&gt;Add(key, value);</a>
<a name="ln733">    auto boundaries = MakeFileBoundaryValues(db_options_.boundary_extractor.get(),</a>
<a name="ln734">                                             key,</a>
<a name="ln735">                                             value);</a>
<a name="ln736">    if (!boundaries) {</a>
<a name="ln737">      status = std::move(boundaries.status());</a>
<a name="ln738">      break;</a>
<a name="ln739">    }</a>
<a name="ln740">    auto&amp; boundary_values = *boundaries;</a>
<a name="ln741">    sub_compact-&gt;current_output()-&gt;meta.UpdateBoundaries(std::move(boundary_values.key),</a>
<a name="ln742">                                                         boundary_values);</a>
<a name="ln743">    sub_compact-&gt;num_output_records++;</a>
<a name="ln744"> </a>
<a name="ln745">    // Close output file if it is big enough</a>
<a name="ln746">    // TODO(aekmekji): determine if file should be closed earlier than this</a>
<a name="ln747">    // during subcompactions (i.e. if output size, estimated by input size, is</a>
<a name="ln748">    // going to be 1.2MB and max_output_file_size = 1MB, prefer to have 0.6MB</a>
<a name="ln749">    // and 0.6MB instead of 1MB and 0.2MB)</a>
<a name="ln750">    if (sub_compact-&gt;builder-&gt;TotalFileSize() &gt;=</a>
<a name="ln751">        sub_compact-&gt;compaction-&gt;max_output_file_size()) {</a>
<a name="ln752">      status = FinishCompactionOutputFile(input-&gt;status(), sub_compact);</a>
<a name="ln753">    }</a>
<a name="ln754"> </a>
<a name="ln755">    c_iter-&gt;Next();</a>
<a name="ln756">  }</a>
<a name="ln757"> </a>
<a name="ln758">  sub_compact-&gt;num_input_records = c_iter_stats.num_input_records;</a>
<a name="ln759">  sub_compact-&gt;compaction_job_stats.num_input_deletion_records =</a>
<a name="ln760">      c_iter_stats.num_input_deletion_records;</a>
<a name="ln761">  sub_compact-&gt;compaction_job_stats.num_corrupt_keys =</a>
<a name="ln762">      c_iter_stats.num_input_corrupt_records;</a>
<a name="ln763">  sub_compact-&gt;compaction_job_stats.total_input_raw_key_bytes +=</a>
<a name="ln764">      c_iter_stats.total_input_raw_key_bytes;</a>
<a name="ln765">  sub_compact-&gt;compaction_job_stats.total_input_raw_value_bytes +=</a>
<a name="ln766">      c_iter_stats.total_input_raw_value_bytes;</a>
<a name="ln767"> </a>
<a name="ln768">  RecordDroppedKeys(c_iter_stats, &amp;sub_compact-&gt;compaction_job_stats);</a>
<a name="ln769">  RecordCompactionIOStats();</a>
<a name="ln770"> </a>
<a name="ln771">  if (status.ok() &amp;&amp;</a>
<a name="ln772">      (shutting_down_-&gt;load(std::memory_order_acquire) || cfd-&gt;IsDropped())) {</a>
<a name="ln773">    status = STATUS(ShutdownInProgress,</a>
<a name="ln774">        &quot;Database shutdown or Column family drop during compaction&quot;);</a>
<a name="ln775">  }</a>
<a name="ln776">  if (status.ok() &amp;&amp; sub_compact-&gt;builder != nullptr) {</a>
<a name="ln777">    status = FinishCompactionOutputFile(input-&gt;status(), sub_compact);</a>
<a name="ln778">  }</a>
<a name="ln779">  if (status.ok()) {</a>
<a name="ln780">    status = input-&gt;status();</a>
<a name="ln781">  }</a>
<a name="ln782"> </a>
<a name="ln783">  if (measure_io_stats_) {</a>
<a name="ln784">    sub_compact-&gt;compaction_job_stats.file_write_nanos +=</a>
<a name="ln785">        IOSTATS(write_nanos) - prev_write_nanos;</a>
<a name="ln786">    sub_compact-&gt;compaction_job_stats.file_fsync_nanos +=</a>
<a name="ln787">        IOSTATS(fsync_nanos) - prev_fsync_nanos;</a>
<a name="ln788">    sub_compact-&gt;compaction_job_stats.file_range_sync_nanos +=</a>
<a name="ln789">        IOSTATS(range_sync_nanos) - prev_range_sync_nanos;</a>
<a name="ln790">    sub_compact-&gt;compaction_job_stats.file_prepare_write_nanos +=</a>
<a name="ln791">        IOSTATS(prepare_write_nanos) - prev_prepare_write_nanos;</a>
<a name="ln792">    if (prev_perf_level != PerfLevel::kEnableTime) {</a>
<a name="ln793">      SetPerfLevel(prev_perf_level);</a>
<a name="ln794">    }</a>
<a name="ln795">  }</a>
<a name="ln796"> </a>
<a name="ln797">  sub_compact-&gt;c_iter.reset();</a>
<a name="ln798">  input.reset();</a>
<a name="ln799">  sub_compact-&gt;status = status;</a>
<a name="ln800">}</a>
<a name="ln801"> </a>
<a name="ln802">void CompactionJob::RecordDroppedKeys(</a>
<a name="ln803">    const CompactionIteratorStats&amp; c_iter_stats,</a>
<a name="ln804">    CompactionJobStats* compaction_job_stats) {</a>
<a name="ln805">  if (c_iter_stats.num_record_drop_user &gt; 0) {</a>
<a name="ln806">    RecordTick(stats_, COMPACTION_KEY_DROP_USER,</a>
<a name="ln807">               c_iter_stats.num_record_drop_user);</a>
<a name="ln808">  }</a>
<a name="ln809">  if (c_iter_stats.num_record_drop_hidden &gt; 0) {</a>
<a name="ln810">    RecordTick(stats_, COMPACTION_KEY_DROP_NEWER_ENTRY,</a>
<a name="ln811">               c_iter_stats.num_record_drop_hidden);</a>
<a name="ln812">    if (compaction_job_stats) {</a>
<a name="ln813">      compaction_job_stats-&gt;num_records_replaced +=</a>
<a name="ln814">          c_iter_stats.num_record_drop_hidden;</a>
<a name="ln815">    }</a>
<a name="ln816">  }</a>
<a name="ln817">  if (c_iter_stats.num_record_drop_obsolete &gt; 0) {</a>
<a name="ln818">    RecordTick(stats_, COMPACTION_KEY_DROP_OBSOLETE,</a>
<a name="ln819">               c_iter_stats.num_record_drop_obsolete);</a>
<a name="ln820">    if (compaction_job_stats) {</a>
<a name="ln821">      compaction_job_stats-&gt;num_expired_deletion_records +=</a>
<a name="ln822">          c_iter_stats.num_record_drop_obsolete;</a>
<a name="ln823">    }</a>
<a name="ln824">  }</a>
<a name="ln825">}</a>
<a name="ln826"> </a>
<a name="ln827">void CompactionJob::CloseFile(Status* status, std::unique_ptr&lt;WritableFileWriter&gt;* writer) {</a>
<a name="ln828">  if (status-&gt;ok() &amp;&amp; !db_options_.disableDataSync) {</a>
<a name="ln829">    StopWatch sw(env_, stats_, COMPACTION_OUTFILE_SYNC_MICROS);</a>
<a name="ln830">    *status = (*writer)-&gt;Sync(db_options_.use_fsync);</a>
<a name="ln831">  }</a>
<a name="ln832">  if (status-&gt;ok()) {</a>
<a name="ln833">    *status = (*writer)-&gt;Close();</a>
<a name="ln834">  }</a>
<a name="ln835">  writer-&gt;reset();</a>
<a name="ln836"> </a>
<a name="ln837">}</a>
<a name="ln838"> </a>
<a name="ln839">Status CompactionJob::FinishCompactionOutputFile(</a>
<a name="ln840">    const Status&amp; input_status, SubcompactionState* sub_compact) {</a>
<a name="ln841">  AutoThreadOperationStageUpdater stage_updater(</a>
<a name="ln842">      ThreadStatus::STAGE_COMPACTION_SYNC_FILE);</a>
<a name="ln843">  assert(sub_compact != nullptr);</a>
<a name="ln844">  assert(sub_compact-&gt;base_outfile);</a>
<a name="ln845">  const bool is_split_sst = sub_compact-&gt;compaction-&gt;column_family_data()-&gt;ioptions()</a>
<a name="ln846">      -&gt;table_factory-&gt;IsSplitSstForWriteSupported();</a>
<a name="ln847">  assert((sub_compact-&gt;data_outfile != nullptr) == is_split_sst);</a>
<a name="ln848">  assert(sub_compact-&gt;builder != nullptr);</a>
<a name="ln849">  assert(sub_compact-&gt;current_output() != nullptr);</a>
<a name="ln850"> </a>
<a name="ln851">  uint64_t output_number = sub_compact-&gt;current_output()-&gt;meta.fd.GetNumber();</a>
<a name="ln852">  assert(output_number != 0);</a>
<a name="ln853"> </a>
<a name="ln854">  TableProperties table_properties;</a>
<a name="ln855">  // Check for iterator errors</a>
<a name="ln856">  Status s = input_status;</a>
<a name="ln857">  auto meta = &amp;sub_compact-&gt;current_output()-&gt;meta;</a>
<a name="ln858">  const uint64_t current_entries = sub_compact-&gt;builder-&gt;NumEntries();</a>
<a name="ln859">  meta-&gt;marked_for_compaction = sub_compact-&gt;builder-&gt;NeedCompact();</a>
<a name="ln860">  if (s.ok()) {</a>
<a name="ln861">    s = sub_compact-&gt;builder-&gt;Finish();</a>
<a name="ln862">  } else {</a>
<a name="ln863">    sub_compact-&gt;builder-&gt;Abandon();</a>
<a name="ln864">  }</a>
<a name="ln865"> </a>
<a name="ln866">  const uint64_t current_total_bytes = sub_compact-&gt;builder-&gt;TotalFileSize();</a>
<a name="ln867">  meta-&gt;fd.total_file_size = current_total_bytes;</a>
<a name="ln868">  meta-&gt;fd.base_file_size = sub_compact-&gt;builder-&gt;BaseFileSize();</a>
<a name="ln869">  sub_compact-&gt;current_output()-&gt;finished = true;</a>
<a name="ln870">  sub_compact-&gt;total_bytes += current_total_bytes;</a>
<a name="ln871"> </a>
<a name="ln872">  // Finish and check for file errors</a>
<a name="ln873">  if (sub_compact-&gt;data_outfile) {</a>
<a name="ln874">    CloseFile(&amp;s, &amp;sub_compact-&gt;data_outfile);</a>
<a name="ln875">  }</a>
<a name="ln876">  CloseFile(&amp;s, &amp;sub_compact-&gt;base_outfile);</a>
<a name="ln877"> </a>
<a name="ln878">  if (s.ok() &amp;&amp; current_entries &gt; 0) {</a>
<a name="ln879">    // Verify that the table is usable</a>
<a name="ln880">    ColumnFamilyData* cfd = sub_compact-&gt;compaction-&gt;column_family_data();</a>
<a name="ln881">    InternalIterator* iter = cfd-&gt;table_cache()-&gt;NewIterator(</a>
<a name="ln882">        ReadOptions(), env_options_, cfd-&gt;internal_comparator(), meta-&gt;fd, meta-&gt;UserFilter(),</a>
<a name="ln883">        nullptr, cfd-&gt;internal_stats()-&gt;GetFileReadHist(</a>
<a name="ln884">                     compact_-&gt;compaction-&gt;output_level()),</a>
<a name="ln885">        false);</a>
<a name="ln886">    s = iter-&gt;status();</a>
<a name="ln887"> </a>
<a name="ln888">    if (s.ok() &amp;&amp; paranoid_file_checks_) {</a>
<a name="ln889">      for (iter-&gt;SeekToFirst(); iter-&gt;Valid(); iter-&gt;Next()) {}</a>
<a name="ln890">      s = iter-&gt;status();</a>
<a name="ln891">    }</a>
<a name="ln892"> </a>
<a name="ln893">    delete iter;</a>
<a name="ln894">    if (s.ok()) {</a>
<a name="ln895">      auto tp = sub_compact-&gt;builder-&gt;GetTableProperties();</a>
<a name="ln896">      sub_compact-&gt;current_output()-&gt;table_properties =</a>
<a name="ln897">          std::make_shared&lt;TableProperties&gt;(tp);</a>
<a name="ln898">      TableFileCreationInfo info(std::move(tp));</a>
<a name="ln899">      info.db_name = dbname_;</a>
<a name="ln900">      info.cf_name = cfd-&gt;GetName();</a>
<a name="ln901">      info.file_path =</a>
<a name="ln902">          TableFileName(cfd-&gt;ioptions()-&gt;db_paths, meta-&gt;fd.GetNumber(),</a>
<a name="ln903">                        meta-&gt;fd.GetPathId());</a>
<a name="ln904">      info.file_size = meta-&gt;fd.GetTotalFileSize();</a>
<a name="ln905">      info.job_id = job_id_;</a>
<a name="ln906">      RLOG(InfoLogLevel::INFO_LEVEL, db_options_.info_log,</a>
<a name="ln907">          &quot;[%s] [JOB %d] Generated table #%&quot; PRIu64 &quot;: %&quot; PRIu64</a>
<a name="ln908">          &quot; keys, %&quot; PRIu64 &quot; bytes%s&quot;,</a>
<a name="ln909">          cfd-&gt;GetName().c_str(), job_id_, output_number, current_entries,</a>
<a name="ln910">          current_total_bytes,</a>
<a name="ln911">          meta-&gt;marked_for_compaction ? &quot; (need compaction)&quot; : &quot;&quot;);</a>
<a name="ln912">      EventHelpers::LogAndNotifyTableFileCreation(</a>
<a name="ln913">          event_logger_, cfd-&gt;ioptions()-&gt;listeners, meta-&gt;fd, info);</a>
<a name="ln914">    }</a>
<a name="ln915">  }</a>
<a name="ln916"> </a>
<a name="ln917">  // Report new file to SstFileManagerImpl</a>
<a name="ln918">  auto sfm =</a>
<a name="ln919">      static_cast&lt;SstFileManagerImpl*&gt;(db_options_.sst_file_manager.get());</a>
<a name="ln920">  if (sfm &amp;&amp; meta-&gt;fd.GetPathId() == 0) {</a>
<a name="ln921">    ColumnFamilyData* cfd = sub_compact-&gt;compaction-&gt;column_family_data();</a>
<a name="ln922">    auto fn = TableFileName(cfd-&gt;ioptions()-&gt;db_paths, meta-&gt;fd.GetNumber(),</a>
<a name="ln923">                            meta-&gt;fd.GetPathId());</a>
<a name="ln924">    RETURN_NOT_OK(sfm-&gt;OnAddFile(fn));</a>
<a name="ln925">    if (is_split_sst) {</a>
<a name="ln926">      RETURN_NOT_OK(sfm-&gt;OnAddFile(TableBaseToDataFileName(fn)));</a>
<a name="ln927">    }</a>
<a name="ln928">    if (sfm-&gt;IsMaxAllowedSpaceReached()) {</a>
<a name="ln929">      InstrumentedMutexLock l(db_mutex_);</a>
<a name="ln930">      if (db_bg_error_-&gt;ok()) {</a>
<a name="ln931">        s = STATUS(IOError, &quot;Max allowed space was reached&quot;);</a>
<a name="ln932">        *db_bg_error_ = s;</a>
<a name="ln933">        TEST_SYNC_POINT(</a>
<a name="ln934">            &quot;CompactionJob::FinishCompactionOutputFile:MaxAllowedSpaceReached&quot;);</a>
<a name="ln935">      }</a>
<a name="ln936">    }</a>
<a name="ln937">  }</a>
<a name="ln938"> </a>
<a name="ln939">  sub_compact-&gt;builder.reset();</a>
<a name="ln940">  return s;</a>
<a name="ln941">}</a>
<a name="ln942"> </a>
<a name="ln943">Status CompactionJob::InstallCompactionResults(</a>
<a name="ln944">    const MutableCFOptions&amp; mutable_cf_options) {</a>
<a name="ln945">  db_mutex_-&gt;AssertHeld();</a>
<a name="ln946"> </a>
<a name="ln947">  auto* compaction = compact_-&gt;compaction;</a>
<a name="ln948">  // paranoia: verify that the files that we started with</a>
<a name="ln949">  // still exist in the current version and in the same original level.</a>
<a name="ln950">  // This ensures that a concurrent compaction did not erroneously</a>
<a name="ln951">  // pick the same files to compact_.</a>
<a name="ln952">  if (!versions_-&gt;VerifyCompactionFileConsistency(compaction)) {</a>
<a name="ln953">    Compaction::InputLevelSummaryBuffer inputs_summary;</a>
<a name="ln954"> </a>
<a name="ln955">    RLOG(InfoLogLevel::ERROR_LEVEL, db_options_.info_log,</a>
<a name="ln956">        &quot;[%s] [JOB %d] Compaction %s aborted&quot;,</a>
<a name="ln957">        compaction-&gt;column_family_data()-&gt;GetName().c_str(), job_id_,</a>
<a name="ln958">        compaction-&gt;InputLevelSummary(&amp;inputs_summary));</a>
<a name="ln959">    return STATUS(Corruption, &quot;Compaction input files inconsistent&quot;);</a>
<a name="ln960">  }</a>
<a name="ln961"> </a>
<a name="ln962">  {</a>
<a name="ln963">    Compaction::InputLevelSummaryBuffer inputs_summary;</a>
<a name="ln964">    RLOG(InfoLogLevel::INFO_LEVEL, db_options_.info_log,</a>
<a name="ln965">        &quot;[%s] [JOB %d] Compacted %s =&gt; %&quot; PRIu64 &quot; bytes&quot;,</a>
<a name="ln966">        compaction-&gt;column_family_data()-&gt;GetName().c_str(), job_id_,</a>
<a name="ln967">        compaction-&gt;InputLevelSummary(&amp;inputs_summary), compact_-&gt;total_bytes);</a>
<a name="ln968">  }</a>
<a name="ln969"> </a>
<a name="ln970">  // Add compaction outputs</a>
<a name="ln971">  compaction-&gt;AddInputDeletions(compaction-&gt;edit());</a>
<a name="ln972"> </a>
<a name="ln973">  for (const auto&amp; sub_compact : compact_-&gt;sub_compact_states) {</a>
<a name="ln974">    for (const auto&amp; out : sub_compact.outputs) {</a>
<a name="ln975">      compaction-&gt;edit()-&gt;AddFile(compaction-&gt;output_level(), out.meta);</a>
<a name="ln976">    }</a>
<a name="ln977">  }</a>
<a name="ln978">  if (largest_user_frontier_) {</a>
<a name="ln979">    compaction-&gt;edit()-&gt;UpdateFlushedFrontier(largest_user_frontier_);</a>
<a name="ln980">  }</a>
<a name="ln981">  return versions_-&gt;LogAndApply(compaction-&gt;column_family_data(),</a>
<a name="ln982">                                mutable_cf_options, compaction-&gt;edit(),</a>
<a name="ln983">                                db_mutex_, db_directory_);</a>
<a name="ln984">}</a>
<a name="ln985"> </a>
<a name="ln986">void CompactionJob::RecordCompactionIOStats() {</a>
<a name="ln987">  RecordTick(stats_, COMPACT_READ_BYTES, IOSTATS(bytes_read));</a>
<a name="ln988">  ThreadStatusUtil::IncreaseThreadOperationProperty(</a>
<a name="ln989">      ThreadStatus::COMPACTION_BYTES_READ, IOSTATS(bytes_read));</a>
<a name="ln990">  IOSTATS_RESET(bytes_read);</a>
<a name="ln991">  RecordTick(stats_, COMPACT_WRITE_BYTES, IOSTATS(bytes_written));</a>
<a name="ln992">  ThreadStatusUtil::IncreaseThreadOperationProperty(</a>
<a name="ln993">      ThreadStatus::COMPACTION_BYTES_WRITTEN, IOSTATS(bytes_written));</a>
<a name="ln994">  IOSTATS_RESET(bytes_written);</a>
<a name="ln995">}</a>
<a name="ln996"> </a>
<a name="ln997">Status CompactionJob::OpenFile(const std::string table_name, uint64_t file_number,</a>
<a name="ln998">    const std::string file_type_label, const std::string fname,</a>
<a name="ln999">    std::unique_ptr&lt;WritableFile&gt;* writable_file) {</a>
<a name="ln1000">  Status s = NewWritableFile(env_, fname, writable_file, env_options_);</a>
<a name="ln1001">  if (!s.ok()) {</a>
<a name="ln1002">    RLOG(InfoLogLevel::ERROR_LEVEL, db_options_.info_log,</a>
<a name="ln1003">        &quot;[%s] [JOB %d] OpenCompactionOutputFiles for table #%&quot; PRIu64</a>
<a name="ln1004">        &quot; fails at NewWritableFile for %s file with status %s&quot;, table_name.c_str(),</a>
<a name="ln1005">        job_id_, file_number, file_type_label.c_str(), s.ToString().c_str());</a>
<a name="ln1006">    LogFlush(db_options_.info_log);</a>
<a name="ln1007">  }</a>
<a name="ln1008">  return s;</a>
<a name="ln1009">}</a>
<a name="ln1010"> </a>
<a name="ln1011">Status CompactionJob::OpenCompactionOutputFile(</a>
<a name="ln1012">    FileNumbersHolder* holder, SubcompactionState* sub_compact) {</a>
<a name="ln1013">  assert(sub_compact != nullptr);</a>
<a name="ln1014">  assert(sub_compact-&gt;builder == nullptr);</a>
<a name="ln1015">  FileNumber file_number = file_numbers_provider_-&gt;NewFileNumber(holder);</a>
<a name="ln1016"> </a>
<a name="ln1017">  // Make the output file</a>
<a name="ln1018">  unique_ptr&lt;WritableFile&gt; base_writable_file;</a>
<a name="ln1019">  unique_ptr&lt;WritableFile&gt; data_writable_file;</a>
<a name="ln1020">  const std::string base_fname = TableFileName(db_options_.db_paths, file_number,</a>
<a name="ln1021">                                    sub_compact-&gt;compaction-&gt;output_path_id());</a>
<a name="ln1022">  const std::string data_fname = TableBaseToDataFileName(base_fname);</a>
<a name="ln1023">  const std::string table_name = sub_compact-&gt;compaction-&gt;column_family_data()-&gt;GetName();</a>
<a name="ln1024">  RETURN_NOT_OK(OpenFile(table_name, file_number, &quot;base&quot;, base_fname, &amp;base_writable_file));</a>
<a name="ln1025">  RETURN_NOT_OK(OpenFile(table_name, file_number, &quot;data&quot;, data_fname, &amp;data_writable_file));</a>
<a name="ln1026"> </a>
<a name="ln1027">  SubcompactionState::Output out;</a>
<a name="ln1028">  out.meta.fd =</a>
<a name="ln1029">      FileDescriptor(file_number, sub_compact-&gt;compaction-&gt;output_path_id(), 0, 0);</a>
<a name="ln1030">  // Update sequence number boundaries for out.</a>
<a name="ln1031">  for (size_t level_idx = 0; level_idx &lt; compact_-&gt;compaction-&gt;num_input_levels(); level_idx++) {</a>
<a name="ln1032">    for (FileMetaData *fmd : *compact_-&gt;compaction-&gt;inputs(level_idx) ) {</a>
<a name="ln1033">      out.meta.UpdateBoundariesExceptKey(fmd-&gt;smallest, UpdateBoundariesType::kSmallest);</a>
<a name="ln1034">      out.meta.UpdateBoundariesExceptKey(fmd-&gt;largest, UpdateBoundariesType::kLargest);</a>
<a name="ln1035">    }</a>
<a name="ln1036">  }</a>
<a name="ln1037">  out.finished = false;</a>
<a name="ln1038"> </a>
<a name="ln1039">  sub_compact-&gt;outputs.push_back(out);</a>
<a name="ln1040"> </a>
<a name="ln1041">  ColumnFamilyData* cfd = sub_compact-&gt;compaction-&gt;column_family_data();</a>
<a name="ln1042"> </a>
<a name="ln1043">  {</a>
<a name="ln1044">    auto setup_outfile = [this, sub_compact] (</a>
<a name="ln1045">        size_t preallocation_block_size, std::unique_ptr&lt;WritableFile&gt;* writable_file,</a>
<a name="ln1046">        std::unique_ptr&lt;WritableFileWriter&gt;* writer) {</a>
<a name="ln1047">      (*writable_file)-&gt;SetIOPriority(Env::IO_LOW);</a>
<a name="ln1048">      if (preallocation_block_size &gt; 0) {</a>
<a name="ln1049">        (*writable_file)-&gt;SetPreallocationBlockSize(preallocation_block_size);</a>
<a name="ln1050">      }</a>
<a name="ln1051">      writer-&gt;reset(new WritableFileWriter(</a>
<a name="ln1052">          std::move(*writable_file), env_options_, sub_compact-&gt;compaction-&gt;suspender()));</a>
<a name="ln1053">    };</a>
<a name="ln1054"> </a>
<a name="ln1055">    const bool is_split_sst = cfd-&gt;ioptions()-&gt;table_factory-&gt;IsSplitSstForWriteSupported();</a>
<a name="ln1056">    const size_t preallocation_data_block_size = static_cast&lt;size_t&gt;(</a>
<a name="ln1057">        sub_compact-&gt;compaction-&gt;OutputFilePreallocationSize());</a>
<a name="ln1058">    // if we don't have separate data file - preallocate size for base file</a>
<a name="ln1059">    setup_outfile(</a>
<a name="ln1060">        is_split_sst ? 0 : preallocation_data_block_size, &amp;base_writable_file,</a>
<a name="ln1061">        &amp;sub_compact-&gt;base_outfile);</a>
<a name="ln1062">    if (is_split_sst) {</a>
<a name="ln1063">      setup_outfile(preallocation_data_block_size, &amp;data_writable_file, &amp;sub_compact-&gt;data_outfile);</a>
<a name="ln1064">    }</a>
<a name="ln1065">  }</a>
<a name="ln1066"> </a>
<a name="ln1067">  // If the Column family flag is to only optimize filters for hits,</a>
<a name="ln1068">  // we can skip creating filters if this is the bottommost_level where</a>
<a name="ln1069">  // data is going to be found</a>
<a name="ln1070">  bool skip_filters =</a>
<a name="ln1071">      cfd-&gt;ioptions()-&gt;optimize_filters_for_hits &amp;&amp; bottommost_level_;</a>
<a name="ln1072">  sub_compact-&gt;builder.reset(NewTableBuilder(</a>
<a name="ln1073">      *cfd-&gt;ioptions(), cfd-&gt;internal_comparator(),</a>
<a name="ln1074">      cfd-&gt;int_tbl_prop_collector_factories(), cfd-&gt;GetID(),</a>
<a name="ln1075">      sub_compact-&gt;base_outfile.get(), sub_compact-&gt;data_outfile.get(),</a>
<a name="ln1076">      sub_compact-&gt;compaction-&gt;output_compression(), cfd-&gt;ioptions()-&gt;compression_opts,</a>
<a name="ln1077">      skip_filters));</a>
<a name="ln1078">  LogFlush(db_options_.info_log);</a>
<a name="ln1079">  return Status::OK();</a>
<a name="ln1080">}</a>
<a name="ln1081"> </a>
<a name="ln1082">void CompactionJob::CleanupCompaction() {</a>
<a name="ln1083">  for (SubcompactionState&amp; sub_compact : compact_-&gt;sub_compact_states) {</a>
<a name="ln1084">    const auto&amp; sub_status = sub_compact.status;</a>
<a name="ln1085"> </a>
<a name="ln1086">    if (sub_compact.builder != nullptr) {</a>
<a name="ln1087">      // May happen if we get a shutdown call in the middle of compaction</a>
<a name="ln1088">      sub_compact.builder-&gt;Abandon();</a>
<a name="ln1089">      sub_compact.builder.reset();</a>
<a name="ln1090">    } else if (sub_status.ok() &amp;&amp;</a>
<a name="ln1091">        (sub_compact.base_outfile != nullptr || sub_compact.data_outfile != nullptr)) {</a>
<a name="ln1092">      std::string log_message;</a>
<a name="ln1093">      log_message.append(&quot;sub_status.ok(), but: sub_compact.base_outfile &quot;);</a>
<a name="ln1094">      log_message.append(sub_compact.base_outfile == nullptr ? &quot;==&quot; : &quot;!=&quot;);</a>
<a name="ln1095">      log_message.append(&quot; nullptr, sub_compact.data_outfile &quot;);</a>
<a name="ln1096">      log_message.append(sub_compact.data_outfile == nullptr ? &quot;==&quot; : &quot;!=&quot;);</a>
<a name="ln1097">      log_message.append(&quot; nullptr&quot;);</a>
<a name="ln1098">      RLOG(InfoLogLevel::FATAL_LEVEL, db_options_.info_log, log_message.c_str());</a>
<a name="ln1099">      assert(!&quot;If sub_status is OK, sub_compact.*_outfile should be nullptr&quot;);</a>
<a name="ln1100">    }</a>
<a name="ln1101">    for (const auto&amp; out : sub_compact.outputs) {</a>
<a name="ln1102">      // If this file was inserted into the table cache then remove</a>
<a name="ln1103">      // them here because this compaction was not committed.</a>
<a name="ln1104">      if (!sub_status.ok()) {</a>
<a name="ln1105">        TableCache::Evict(table_cache_.get(), out.meta.fd.GetNumber());</a>
<a name="ln1106">      }</a>
<a name="ln1107">    }</a>
<a name="ln1108">  }</a>
<a name="ln1109">  delete compact_;</a>
<a name="ln1110">  compact_ = nullptr;</a>
<a name="ln1111">}</a>
<a name="ln1112"> </a>
<a name="ln1113">#ifndef ROCKSDB_LITE</a>
<a name="ln1114">namespace {</a>
<a name="ln1115">void CopyPrefix(</a>
<a name="ln1116">    const Slice&amp; src, size_t prefix_length, std::string* dst) {</a>
<a name="ln1117">  assert(prefix_length &gt; 0);</a>
<a name="ln1118">  size_t length = src.size() &gt; prefix_length ? prefix_length : src.size();</a>
<a name="ln1119">  dst-&gt;assign(src.cdata(), length);</a>
<a name="ln1120">}</a>
<a name="ln1121">}  // namespace</a>
<a name="ln1122"> </a>
<a name="ln1123">#endif  // !ROCKSDB_LITE</a>
<a name="ln1124"> </a>
<a name="ln1125">void CompactionJob::UpdateCompactionStats() {</a>
<a name="ln1126">  Compaction* compaction = compact_-&gt;compaction;</a>
<a name="ln1127">  compaction_stats_.num_input_files_in_non_output_levels = 0;</a>
<a name="ln1128">  compaction_stats_.num_input_files_in_output_level = 0;</a>
<a name="ln1129">  for (int input_level = 0;</a>
<a name="ln1130">       input_level &lt; static_cast&lt;int&gt;(compaction-&gt;num_input_levels());</a>
<a name="ln1131">       ++input_level) {</a>
<a name="ln1132">    if (compaction-&gt;start_level() + input_level</a>
<a name="ln1133">        != compaction-&gt;output_level()) {</a>
<a name="ln1134">      UpdateCompactionInputStatsHelper(</a>
<a name="ln1135">          &amp;compaction_stats_.num_input_files_in_non_output_levels,</a>
<a name="ln1136">          &amp;compaction_stats_.bytes_read_non_output_levels,</a>
<a name="ln1137">          input_level);</a>
<a name="ln1138">    } else {</a>
<a name="ln1139">      UpdateCompactionInputStatsHelper(</a>
<a name="ln1140">          &amp;compaction_stats_.num_input_files_in_output_level,</a>
<a name="ln1141">          &amp;compaction_stats_.bytes_read_output_level,</a>
<a name="ln1142">          input_level);</a>
<a name="ln1143">    }</a>
<a name="ln1144">  }</a>
<a name="ln1145"> </a>
<a name="ln1146">  for (const auto&amp; sub_compact : compact_-&gt;sub_compact_states) {</a>
<a name="ln1147">    size_t num_output_files = sub_compact.outputs.size();</a>
<a name="ln1148">    if (sub_compact.builder != nullptr) {</a>
<a name="ln1149">      // An error occurred so ignore the last output.</a>
<a name="ln1150">      assert(num_output_files &gt; 0);</a>
<a name="ln1151">      --num_output_files;</a>
<a name="ln1152">    }</a>
<a name="ln1153">    compaction_stats_.num_output_files += static_cast&lt;int&gt;(num_output_files);</a>
<a name="ln1154"> </a>
<a name="ln1155">    for (const auto&amp; out : sub_compact.outputs) {</a>
<a name="ln1156">      compaction_stats_.bytes_written += out.meta.fd.total_file_size;</a>
<a name="ln1157">    }</a>
<a name="ln1158">    if (sub_compact.num_input_records &gt; sub_compact.num_output_records) {</a>
<a name="ln1159">      compaction_stats_.num_dropped_records +=</a>
<a name="ln1160">          sub_compact.num_input_records - sub_compact.num_output_records;</a>
<a name="ln1161">    }</a>
<a name="ln1162">  }</a>
<a name="ln1163">}</a>
<a name="ln1164"> </a>
<a name="ln1165">void CompactionJob::UpdateCompactionInputStatsHelper(</a>
<a name="ln1166">    int* num_files, uint64_t* bytes_read, int input_level) {</a>
<a name="ln1167">  const Compaction* compaction = compact_-&gt;compaction;</a>
<a name="ln1168">  auto num_input_files = compaction-&gt;num_input_files(input_level);</a>
<a name="ln1169">  *num_files += static_cast&lt;int&gt;(num_input_files);</a>
<a name="ln1170"> </a>
<a name="ln1171">  for (size_t i = 0; i &lt; num_input_files; ++i) {</a>
<a name="ln1172">    const auto* file_meta = compaction-&gt;input(input_level, i);</a>
<a name="ln1173">    *bytes_read += file_meta-&gt;fd.GetTotalFileSize();</a>
<a name="ln1174">    compaction_stats_.num_input_records +=</a>
<a name="ln1175">        static_cast&lt;uint64_t&gt;(file_meta-&gt;num_entries);</a>
<a name="ln1176">  }</a>
<a name="ln1177">}</a>
<a name="ln1178"> </a>
<a name="ln1179">void CompactionJob::UpdateCompactionJobStats(</a>
<a name="ln1180">    const InternalStats::CompactionStats&amp; stats) const {</a>
<a name="ln1181">#ifndef ROCKSDB_LITE</a>
<a name="ln1182">  if (compaction_job_stats_) {</a>
<a name="ln1183">    compaction_job_stats_-&gt;elapsed_micros = stats.micros;</a>
<a name="ln1184"> </a>
<a name="ln1185">    // input information</a>
<a name="ln1186">    compaction_job_stats_-&gt;total_input_bytes =</a>
<a name="ln1187">        stats.bytes_read_non_output_levels +</a>
<a name="ln1188">        stats.bytes_read_output_level;</a>
<a name="ln1189">    compaction_job_stats_-&gt;num_input_records =</a>
<a name="ln1190">        compact_-&gt;num_input_records;</a>
<a name="ln1191">    compaction_job_stats_-&gt;num_input_files =</a>
<a name="ln1192">        stats.num_input_files_in_non_output_levels +</a>
<a name="ln1193">        stats.num_input_files_in_output_level;</a>
<a name="ln1194">    compaction_job_stats_-&gt;num_input_files_at_output_level =</a>
<a name="ln1195">        stats.num_input_files_in_output_level;</a>
<a name="ln1196"> </a>
<a name="ln1197">    // output information</a>
<a name="ln1198">    compaction_job_stats_-&gt;total_output_bytes = stats.bytes_written;</a>
<a name="ln1199">    compaction_job_stats_-&gt;num_output_records =</a>
<a name="ln1200">        compact_-&gt;num_output_records;</a>
<a name="ln1201">    compaction_job_stats_-&gt;num_output_files = stats.num_output_files;</a>
<a name="ln1202"> </a>
<a name="ln1203">    if (compact_-&gt;NumOutputFiles() &gt; 0U) {</a>
<a name="ln1204">      CopyPrefix(</a>
<a name="ln1205">          compact_-&gt;SmallestUserKey(),</a>
<a name="ln1206">          CompactionJobStats::kMaxPrefixLength,</a>
<a name="ln1207">          &amp;compaction_job_stats_-&gt;smallest_output_key_prefix);</a>
<a name="ln1208">      CopyPrefix(</a>
<a name="ln1209">          compact_-&gt;LargestUserKey(),</a>
<a name="ln1210">          CompactionJobStats::kMaxPrefixLength,</a>
<a name="ln1211">          &amp;compaction_job_stats_-&gt;largest_output_key_prefix);</a>
<a name="ln1212">    }</a>
<a name="ln1213">  }</a>
<a name="ln1214">#endif  // !ROCKSDB_LITE</a>
<a name="ln1215">}</a>
<a name="ln1216"> </a>
<a name="ln1217">void CompactionJob::LogCompaction() {</a>
<a name="ln1218">  Compaction* compaction = compact_-&gt;compaction;</a>
<a name="ln1219">  ColumnFamilyData* cfd = compaction-&gt;column_family_data();</a>
<a name="ln1220"> </a>
<a name="ln1221">  // Let's check if anything will get logged. Don't prepare all the info if</a>
<a name="ln1222">  // we're not logging</a>
<a name="ln1223">  if (db_options_.info_log_level &lt;= InfoLogLevel::INFO_LEVEL) {</a>
<a name="ln1224">    Compaction::InputLevelSummaryBuffer inputs_summary;</a>
<a name="ln1225">    RLOG(InfoLogLevel::INFO_LEVEL, db_options_.info_log,</a>
<a name="ln1226">        &quot;[%s] [JOB %d] Compacting %s, score %.2f&quot;, cfd-&gt;GetName().c_str(),</a>
<a name="ln1227">        job_id_, compaction-&gt;InputLevelSummary(&amp;inputs_summary),</a>
<a name="ln1228">        compaction-&gt;score());</a>
<a name="ln1229">    char scratch[2345];</a>
<a name="ln1230">    compaction-&gt;Summary(scratch, sizeof(scratch));</a>
<a name="ln1231">    RLOG(InfoLogLevel::INFO_LEVEL, db_options_.info_log,</a>
<a name="ln1232">        &quot;[%s] Compaction start summary: %s\n&quot;, cfd-&gt;GetName().c_str(), scratch);</a>
<a name="ln1233">    // build event logger report</a>
<a name="ln1234">    auto stream = event_logger_-&gt;Log();</a>
<a name="ln1235">    stream &lt;&lt; &quot;job&quot; &lt;&lt; job_id_ &lt;&lt; &quot;event&quot;</a>
<a name="ln1236">           &lt;&lt; &quot;compaction_started&quot;;</a>
<a name="ln1237">    for (size_t i = 0; i &lt; compaction-&gt;num_input_levels(); ++i) {</a>
<a name="ln1238">      stream &lt;&lt; (&quot;files_L&quot; + ToString(compaction-&gt;level(i)));</a>
<a name="ln1239">      stream.StartArray();</a>
<a name="ln1240">      for (auto f : *compaction-&gt;inputs(i)) {</a>
<a name="ln1241">        stream &lt;&lt; f-&gt;fd.GetNumber();</a>
<a name="ln1242">      }</a>
<a name="ln1243">      stream.EndArray();</a>
<a name="ln1244">    }</a>
<a name="ln1245">    stream &lt;&lt; &quot;score&quot; &lt;&lt; compaction-&gt;score() &lt;&lt; &quot;input_data_size&quot;</a>
<a name="ln1246">           &lt;&lt; compaction-&gt;CalculateTotalInputSize();</a>
<a name="ln1247">  }</a>
<a name="ln1248">}</a>
<a name="ln1249"> </a>
<a name="ln1250">}  // namespace rocksdb</a>

</code></pre>
<div class="balloon" rel="81"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v690/" target="_blank">V690</a> The 'CompactionJob::SubcompactionState' class implements the move assignment operator, but lacks a move constructor. It is dangerous to use such a class.</p></div>
<div class="balloon" rel="595"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v1002/" target="_blank">V1002</a> The 'EventLoggerStream' class, containing pointers, constructor and destructor, is copied by the automatically generated copy constructor.</p></div>
<div class="balloon" rel="1234"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v1002/" target="_blank">V1002</a> The 'EventLoggerStream' class, containing pointers, constructor and destructor, is copied by the automatically generated copy constructor.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
