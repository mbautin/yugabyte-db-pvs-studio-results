
<html>
<head>

  <meta http-equiv="Content-Type" content="text/html; charset=US-ASCII" />
  <title>raft_consensus.cc</title>

  <link rel="stylesheet" href="../style.css"/>
  <script src="../jquery-3.2.1.min.js"></script>
</head>
<body>

<pre><code class = "cpp">
<a name="ln1">// Licensed to the Apache Software Foundation (ASF) under one</a>
<a name="ln2">// or more contributor license agreements.  See the NOTICE file</a>
<a name="ln3">// distributed with this work for additional information</a>
<a name="ln4">// regarding copyright ownership.  The ASF licenses this file</a>
<a name="ln5">// to you under the Apache License, Version 2.0 (the</a>
<a name="ln6">// &quot;License&quot;); you may not use this file except in compliance</a>
<a name="ln7">// with the License.  You may obtain a copy of the License at</a>
<a name="ln8">//</a>
<a name="ln9">//   http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln10">//</a>
<a name="ln11">// Unless required by applicable law or agreed to in writing,</a>
<a name="ln12">// software distributed under the License is distributed on an</a>
<a name="ln13">// &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</a>
<a name="ln14">// KIND, either express or implied.  See the License for the</a>
<a name="ln15">// specific language governing permissions and limitations</a>
<a name="ln16">// under the License.</a>
<a name="ln17">//</a>
<a name="ln18">// The following only applies to changes made to this file as part of YugaByte development.</a>
<a name="ln19">//</a>
<a name="ln20">// Portions Copyright (c) YugaByte, Inc.</a>
<a name="ln21">//</a>
<a name="ln22">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except</a>
<a name="ln23">// in compliance with the License.  You may obtain a copy of the License at</a>
<a name="ln24">//</a>
<a name="ln25">// http://www.apache.org/licenses/LICENSE-2.0</a>
<a name="ln26">//</a>
<a name="ln27">// Unless required by applicable law or agreed to in writing, software distributed under the License</a>
<a name="ln28">// is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express</a>
<a name="ln29">// or implied.  See the License for the specific language governing permissions and limitations</a>
<a name="ln30">// under the License.</a>
<a name="ln31">//</a>
<a name="ln32"> </a>
<a name="ln33">#include &quot;yb/consensus/raft_consensus.h&quot;</a>
<a name="ln34"> </a>
<a name="ln35">#include &lt;algorithm&gt;</a>
<a name="ln36">#include &lt;iostream&gt;</a>
<a name="ln37">#include &lt;memory&gt;</a>
<a name="ln38">#include &lt;mutex&gt;</a>
<a name="ln39"> </a>
<a name="ln40">#include &lt;boost/optional.hpp&gt;</a>
<a name="ln41">#include &lt;gflags/gflags.h&gt;</a>
<a name="ln42"> </a>
<a name="ln43">#include &quot;yb/common/wire_protocol.h&quot;</a>
<a name="ln44">#include &quot;yb/consensus/consensus.pb.h&quot;</a>
<a name="ln45">#include &quot;yb/consensus/consensus_context.h&quot;</a>
<a name="ln46">#include &quot;yb/consensus/consensus_peers.h&quot;</a>
<a name="ln47">#include &quot;yb/consensus/leader_election.h&quot;</a>
<a name="ln48">#include &quot;yb/consensus/log.h&quot;</a>
<a name="ln49">#include &quot;yb/consensus/peer_manager.h&quot;</a>
<a name="ln50">#include &quot;yb/consensus/quorum_util.h&quot;</a>
<a name="ln51">#include &quot;yb/consensus/replica_state.h&quot;</a>
<a name="ln52">#include &quot;yb/gutil/map-util.h&quot;</a>
<a name="ln53">#include &quot;yb/gutil/stl_util.h&quot;</a>
<a name="ln54">#include &quot;yb/gutil/stringprintf.h&quot;</a>
<a name="ln55">#include &quot;yb/gutil/strings/human_readable.h&quot;</a>
<a name="ln56">#include &quot;yb/rpc/periodic.h&quot;</a>
<a name="ln57">#include &quot;yb/server/clock.h&quot;</a>
<a name="ln58">#include &quot;yb/server/metadata.h&quot;</a>
<a name="ln59">#include &quot;yb/tserver/tserver.pb.h&quot;</a>
<a name="ln60"> </a>
<a name="ln61">#include &quot;yb/util/debug/trace_event.h&quot;</a>
<a name="ln62">#include &quot;yb/util/debug/long_operation_tracker.h&quot;</a>
<a name="ln63">#include &quot;yb/util/enums.h&quot;</a>
<a name="ln64">#include &quot;yb/util/flag_tags.h&quot;</a>
<a name="ln65">#include &quot;yb/util/logging.h&quot;</a>
<a name="ln66">#include &quot;yb/util/mem_tracker.h&quot;</a>
<a name="ln67">#include &quot;yb/util/metrics.h&quot;</a>
<a name="ln68">#include &quot;yb/util/net/dns_resolver.h&quot;</a>
<a name="ln69">#include &quot;yb/util/random.h&quot;</a>
<a name="ln70">#include &quot;yb/util/random_util.h&quot;</a>
<a name="ln71">#include &quot;yb/util/threadpool.h&quot;</a>
<a name="ln72">#include &quot;yb/util/tostring.h&quot;</a>
<a name="ln73">#include &quot;yb/util/trace.h&quot;</a>
<a name="ln74">#include &quot;yb/util/url-coding.h&quot;</a>
<a name="ln75">#include &quot;yb/util/format.h&quot;</a>
<a name="ln76">#include &quot;yb/util/tsan_util.h&quot;</a>
<a name="ln77"> </a>
<a name="ln78">using namespace std::literals;</a>
<a name="ln79">using namespace std::placeholders;</a>
<a name="ln80"> </a>
<a name="ln81">DEFINE_int32(raft_heartbeat_interval_ms, yb::NonTsanVsTsan(500, 1000),</a>
<a name="ln82">             &quot;The heartbeat interval for Raft replication. The leader produces heartbeats &quot;</a>
<a name="ln83">             &quot;to followers at this interval. The followers expect a heartbeat at this interval &quot;</a>
<a name="ln84">             &quot;and consider a leader to have failed if it misses several in a row.&quot;);</a>
<a name="ln85">TAG_FLAG(raft_heartbeat_interval_ms, advanced);</a>
<a name="ln86"> </a>
<a name="ln87">DEFINE_double(leader_failure_max_missed_heartbeat_periods, 6.0,</a>
<a name="ln88">              &quot;Maximum heartbeat periods that the leader can fail to heartbeat in before we &quot;</a>
<a name="ln89">              &quot;consider the leader to be failed. The total failure timeout in milliseconds is &quot;</a>
<a name="ln90">              &quot;raft_heartbeat_interval_ms times leader_failure_max_missed_heartbeat_periods. &quot;</a>
<a name="ln91">              &quot;The value passed to this flag may be fractional.&quot;);</a>
<a name="ln92">TAG_FLAG(leader_failure_max_missed_heartbeat_periods, advanced);</a>
<a name="ln93"> </a>
<a name="ln94">DEFINE_int32(leader_failure_exp_backoff_max_delta_ms, 20 * 1000,</a>
<a name="ln95">             &quot;Maximum time to sleep in between leader election retries, in addition to the &quot;</a>
<a name="ln96">             &quot;regular timeout. When leader election fails the interval in between retries &quot;</a>
<a name="ln97">             &quot;increases exponentially, up to this value.&quot;);</a>
<a name="ln98">TAG_FLAG(leader_failure_exp_backoff_max_delta_ms, experimental);</a>
<a name="ln99"> </a>
<a name="ln100">DEFINE_bool(enable_leader_failure_detection, true,</a>
<a name="ln101">            &quot;Whether to enable failure detection of tablet leaders. If enabled, attempts will be &quot;</a>
<a name="ln102">            &quot;made to elect a follower as a new leader when the leader is detected to have failed.&quot;);</a>
<a name="ln103">TAG_FLAG(enable_leader_failure_detection, unsafe);</a>
<a name="ln104"> </a>
<a name="ln105">DEFINE_test_flag(bool, do_not_start_election_test_only, false,</a>
<a name="ln106">                 &quot;Do not start election even if leader failure is detected. &quot;);</a>
<a name="ln107">TAG_FLAG(TEST_do_not_start_election_test_only, runtime);</a>
<a name="ln108"> </a>
<a name="ln109">DEFINE_bool(evict_failed_followers, true,</a>
<a name="ln110">            &quot;Whether to evict followers from the Raft config that have fallen &quot;</a>
<a name="ln111">            &quot;too far behind the leader's log to catch up normally or have been &quot;</a>
<a name="ln112">            &quot;unreachable by the leader for longer than &quot;</a>
<a name="ln113">            &quot;follower_unavailable_considered_failed_sec&quot;);</a>
<a name="ln114">TAG_FLAG(evict_failed_followers, advanced);</a>
<a name="ln115"> </a>
<a name="ln116">DEFINE_test_flag(bool, follower_reject_update_consensus_requests, false,</a>
<a name="ln117">                 &quot;Whether a follower will return an error for all UpdateConsensus() requests.&quot;);</a>
<a name="ln118"> </a>
<a name="ln119">DEFINE_test_flag(int32, follower_reject_update_consensus_requests_seconds, 0,</a>
<a name="ln120">                 &quot;Whether a follower will return an error for all UpdateConsensus() requests for &quot;</a>
<a name="ln121">                 &quot;the first TEST_follower_reject_update_consensus_requests_seconds seconds after &quot;</a>
<a name="ln122">                 &quot;the Consensus objet is created.&quot;);</a>
<a name="ln123"> </a>
<a name="ln124">DEFINE_test_flag(bool, follower_fail_all_prepare, false,</a>
<a name="ln125">                 &quot;Whether a follower will fail preparing all operations.&quot;);</a>
<a name="ln126"> </a>
<a name="ln127">DEFINE_int32(after_stepdown_delay_election_multiplier, 5,</a>
<a name="ln128">             &quot;After a peer steps down as a leader, the factor with which to multiply &quot;</a>
<a name="ln129">             &quot;leader_failure_max_missed_heartbeat_periods to get the delay time before starting a &quot;</a>
<a name="ln130">             &quot;new election.&quot;);</a>
<a name="ln131">TAG_FLAG(after_stepdown_delay_election_multiplier, advanced);</a>
<a name="ln132">TAG_FLAG(after_stepdown_delay_election_multiplier, hidden);</a>
<a name="ln133"> </a>
<a name="ln134">DECLARE_int32(memory_limit_warn_threshold_percentage);</a>
<a name="ln135"> </a>
<a name="ln136">DEFINE_test_flag(int32, inject_delay_leader_change_role_append_secs, 0,</a>
<a name="ln137">                 &quot;Amount of time to delay leader from sending replicate of change role.&quot;);</a>
<a name="ln138"> </a>
<a name="ln139">DEFINE_test_flag(double, return_error_on_change_config, 0.0,</a>
<a name="ln140">                 &quot;Fraction of the time when ChangeConfig will return an error.&quot;);</a>
<a name="ln141"> </a>
<a name="ln142">METRIC_DEFINE_counter(tablet, follower_memory_pressure_rejections,</a>
<a name="ln143">                      &quot;Follower Memory Pressure Rejections&quot;,</a>
<a name="ln144">                      yb::MetricUnit::kRequests,</a>
<a name="ln145">                      &quot;Number of RPC requests rejected due to &quot;</a>
<a name="ln146">                      &quot;memory pressure while FOLLOWER.&quot;);</a>
<a name="ln147">METRIC_DEFINE_gauge_int64(tablet, raft_term,</a>
<a name="ln148">                          &quot;Current Raft Consensus Term&quot;,</a>
<a name="ln149">                          yb::MetricUnit::kUnits,</a>
<a name="ln150">                          &quot;Current Term of the Raft Consensus algorithm. This number increments &quot;</a>
<a name="ln151">                          &quot;each time a leader election is started.&quot;);</a>
<a name="ln152"> </a>
<a name="ln153">METRIC_DEFINE_lag(tablet, follower_lag_ms,</a>
<a name="ln154">                  &quot;Follower lag from leader&quot;,</a>
<a name="ln155">                  &quot;The amount of time since the last UpdateConsensus request from the &quot;</a>
<a name="ln156">                  &quot;leader.&quot;);</a>
<a name="ln157"> </a>
<a name="ln158">METRIC_DEFINE_gauge_int64(tablet, is_raft_leader,</a>
<a name="ln159">                          &quot;Is tablet raft leader&quot;,</a>
<a name="ln160">                          yb::MetricUnit::kUnits,</a>
<a name="ln161">                          &quot;Keeps track whether tablet is raft leader&quot;</a>
<a name="ln162">                          &quot;1 indicates that the tablet is raft leader&quot;);</a>
<a name="ln163"> </a>
<a name="ln164">METRIC_DEFINE_histogram(</a>
<a name="ln165">  tablet, dns_resolve_latency_during_update_raft_config,</a>
<a name="ln166">  &quot;yb.consensus.RaftConsensus.UpdateRaftConfig DNS Resolve&quot;,</a>
<a name="ln167">  yb::MetricUnit::kMicroseconds,</a>
<a name="ln168">  &quot;Microseconds spent resolving DNS requests during RaftConsensus::UpdateRaftConfig&quot;,</a>
<a name="ln169">  60000000LU, 2);</a>
<a name="ln170"> </a>
<a name="ln171">DEFINE_int32(leader_lease_duration_ms, yb::consensus::kDefaultLeaderLeaseDurationMs,</a>
<a name="ln172">             &quot;Leader lease duration. A leader keeps establishing a new lease or extending the &quot;</a>
<a name="ln173">             &quot;existing one with every UpdateConsensus. A new server is not allowed to serve as a &quot;</a>
<a name="ln174">             &quot;leader (i.e. serve up-to-date read requests or acknowledge write requests) until a &quot;</a>
<a name="ln175">             &quot;lease of this duration has definitely expired on the old leader's side.&quot;);</a>
<a name="ln176"> </a>
<a name="ln177">DEFINE_int32(ht_lease_duration_ms, 2000,</a>
<a name="ln178">             &quot;Hybrid time leader lease duration. A leader keeps establishing a new lease or &quot;</a>
<a name="ln179">             &quot;extending the existing one with every UpdateConsensus. A new server is not allowed &quot;</a>
<a name="ln180">             &quot;to add entries to RAFT log until a lease of the old leader is expired. 0 to disable.&quot;</a>
<a name="ln181">             );</a>
<a name="ln182"> </a>
<a name="ln183">DEFINE_int32(min_leader_stepdown_retry_interval_ms,</a>
<a name="ln184">             20 * 1000,</a>
<a name="ln185">             &quot;Minimum amount of time between successive attempts to perform the leader stepdown &quot;</a>
<a name="ln186">             &quot;for the same combination of tablet and intended (target) leader. This is needed &quot;</a>
<a name="ln187">             &quot;to avoid infinite leader stepdown loops when the current leader never has a chance &quot;</a>
<a name="ln188">             &quot;to update the intended leader with its latest records.&quot;);</a>
<a name="ln189"> </a>
<a name="ln190">DEFINE_bool(use_preelection, true, &quot;Whether to use pre election, before doing actual election.&quot;);</a>
<a name="ln191"> </a>
<a name="ln192">DEFINE_int32(temporary_disable_preelections_timeout_ms, 10 * 60 * 1000,</a>
<a name="ln193">             &quot;If some of nodes does not support preelections, then we disable them for this &quot;</a>
<a name="ln194">             &quot;amount of time.&quot;);</a>
<a name="ln195"> </a>
<a name="ln196">DEFINE_test_flag(bool, pause_update_replica, false,</a>
<a name="ln197">                 &quot;Pause RaftConsensus::UpdateReplica processing before snoozing failure detector.&quot;);</a>
<a name="ln198"> </a>
<a name="ln199">DEFINE_test_flag(bool, pause_update_majority_replicated, false,</a>
<a name="ln200">                 &quot;Pause RaftConsensus::UpdateMajorityReplicated.&quot;);</a>
<a name="ln201"> </a>
<a name="ln202">DEFINE_test_flag(int32, log_change_config_every_n, 1,</a>
<a name="ln203">                 &quot;How often to log change config information. &quot;</a>
<a name="ln204">                 &quot;Used to reduce the number of lines being printed for change config requests &quot;</a>
<a name="ln205">                 &quot;when a test simulates a failure that would generate a log of these requests.&quot;);</a>
<a name="ln206"> </a>
<a name="ln207">DEFINE_bool(enable_lease_revocation, true, &quot;Enables lease revocation mechanism&quot;);</a>
<a name="ln208"> </a>
<a name="ln209">DEFINE_bool(quick_leader_election_on_create, true, &quot;Do we trigger quick leader elections on table &quot;</a>
<a name="ln210">                                                   &quot;creation.&quot;);</a>
<a name="ln211">TAG_FLAG(quick_leader_election_on_create, advanced);</a>
<a name="ln212">TAG_FLAG(quick_leader_election_on_create, hidden);</a>
<a name="ln213"> </a>
<a name="ln214">DEFINE_bool(</a>
<a name="ln215">    stepdown_disable_graceful_transition, false,</a>
<a name="ln216">    &quot;During a leader stepdown, disable graceful leadership transfer &quot;</a>
<a name="ln217">    &quot;to an up to date peer&quot;);</a>
<a name="ln218"> </a>
<a name="ln219">namespace yb {</a>
<a name="ln220">namespace consensus {</a>
<a name="ln221"> </a>
<a name="ln222">using log::LogEntryBatch;</a>
<a name="ln223">using rpc::PeriodicTimer;</a>
<a name="ln224">using std::shared_ptr;</a>
<a name="ln225">using std::unique_ptr;</a>
<a name="ln226">using std::weak_ptr;</a>
<a name="ln227">using strings::Substitute;</a>
<a name="ln228">using tserver::TabletServerErrorPB;</a>
<a name="ln229"> </a>
<a name="ln230">struct RaftConsensus::LeaderRequest {</a>
<a name="ln231">  std::string leader_uuid;</a>
<a name="ln232">  yb::OpId preceding_op_id;</a>
<a name="ln233">  yb::OpId committed_op_id;</a>
<a name="ln234">  ReplicateMsgs messages;</a>
<a name="ln235">  // The positional index of the first message selected to be appended, in the</a>
<a name="ln236">  // original leader's request message sequence.</a>
<a name="ln237">  int64_t first_message_idx;</a>
<a name="ln238"> </a>
<a name="ln239">  std::string OpsRangeString() const;</a>
<a name="ln240">};</a>
<a name="ln241"> </a>
<a name="ln242">shared_ptr&lt;RaftConsensus&gt; RaftConsensus::Create(</a>
<a name="ln243">    const ConsensusOptions&amp; options,</a>
<a name="ln244">    std::unique_ptr&lt;ConsensusMetadata&gt; cmeta,</a>
<a name="ln245">    const RaftPeerPB&amp; local_peer_pb,</a>
<a name="ln246">    const scoped_refptr&lt;MetricEntity&gt;&amp; metric_entity,</a>
<a name="ln247">    const scoped_refptr&lt;server::Clock&gt;&amp; clock,</a>
<a name="ln248">    ConsensusContext* consensus_context,</a>
<a name="ln249">    rpc::Messenger* messenger,</a>
<a name="ln250">    rpc::ProxyCache* proxy_cache,</a>
<a name="ln251">    const scoped_refptr&lt;log::Log&gt;&amp; log,</a>
<a name="ln252">    const shared_ptr&lt;MemTracker&gt;&amp; server_mem_tracker,</a>
<a name="ln253">    const shared_ptr&lt;MemTracker&gt;&amp; parent_mem_tracker,</a>
<a name="ln254">    const Callback&lt;void(std::shared_ptr&lt;StateChangeContext&gt; context)&gt; mark_dirty_clbk,</a>
<a name="ln255">    TableType table_type,</a>
<a name="ln256">    ThreadPool* raft_pool,</a>
<a name="ln257">    RetryableRequests* retryable_requests,</a>
<a name="ln258">    const yb::OpId&amp; split_op_id) {</a>
<a name="ln259">  auto rpc_factory = std::make_unique&lt;RpcPeerProxyFactory&gt;(</a>
<a name="ln260">      messenger, proxy_cache, local_peer_pb.cloud_info());</a>
<a name="ln261"> </a>
<a name="ln262">  // The message queue that keeps track of which operations need to be replicated</a>
<a name="ln263">  // where.</a>
<a name="ln264">  auto queue = std::make_unique&lt;PeerMessageQueue&gt;(</a>
<a name="ln265">      metric_entity,</a>
<a name="ln266">      log,</a>
<a name="ln267">      server_mem_tracker,</a>
<a name="ln268">      parent_mem_tracker,</a>
<a name="ln269">      local_peer_pb,</a>
<a name="ln270">      options.tablet_id,</a>
<a name="ln271">      clock,</a>
<a name="ln272">      consensus_context,</a>
<a name="ln273">      raft_pool-&gt;NewToken(ThreadPool::ExecutionMode::SERIAL));</a>
<a name="ln274"> </a>
<a name="ln275">  DCHECK(local_peer_pb.has_permanent_uuid());</a>
<a name="ln276">  const string&amp; peer_uuid = local_peer_pb.permanent_uuid();</a>
<a name="ln277"> </a>
<a name="ln278">  // A single Raft thread pool token is shared between RaftConsensus and</a>
<a name="ln279">  // PeerManager. Because PeerManager is owned by RaftConsensus, it receives a</a>
<a name="ln280">  // raw pointer to the token, to emphasize that RaftConsensus is responsible</a>
<a name="ln281">  // for destroying the token.</a>
<a name="ln282">  unique_ptr&lt;ThreadPoolToken&gt; raft_pool_token(raft_pool-&gt;NewToken(</a>
<a name="ln283">      ThreadPool::ExecutionMode::CONCURRENT));</a>
<a name="ln284"> </a>
<a name="ln285">  // A manager for the set of peers that actually send the operations both remotely</a>
<a name="ln286">  // and to the local wal.</a>
<a name="ln287">  auto peer_manager = std::make_unique&lt;PeerManager&gt;(</a>
<a name="ln288">      options.tablet_id,</a>
<a name="ln289">      peer_uuid,</a>
<a name="ln290">      rpc_factory.get(),</a>
<a name="ln291">      queue.get(),</a>
<a name="ln292">      raft_pool_token.get(),</a>
<a name="ln293">      log);</a>
<a name="ln294"> </a>
<a name="ln295">  return std::make_shared&lt;RaftConsensus&gt;(</a>
<a name="ln296">      options,</a>
<a name="ln297">      std::move(cmeta),</a>
<a name="ln298">      std::move(rpc_factory),</a>
<a name="ln299">      std::move(queue),</a>
<a name="ln300">      std::move(peer_manager),</a>
<a name="ln301">      std::move(raft_pool_token),</a>
<a name="ln302">      metric_entity,</a>
<a name="ln303">      peer_uuid,</a>
<a name="ln304">      clock,</a>
<a name="ln305">      consensus_context,</a>
<a name="ln306">      log,</a>
<a name="ln307">      parent_mem_tracker,</a>
<a name="ln308">      mark_dirty_clbk,</a>
<a name="ln309">      table_type,</a>
<a name="ln310">      retryable_requests,</a>
<a name="ln311">      split_op_id);</a>
<a name="ln312">}</a>
<a name="ln313"> </a>
<a name="ln314">RaftConsensus::RaftConsensus(</a>
<a name="ln315">    const ConsensusOptions&amp; options, std::unique_ptr&lt;ConsensusMetadata&gt; cmeta,</a>
<a name="ln316">    std::unique_ptr&lt;PeerProxyFactory&gt; proxy_factory,</a>
<a name="ln317">    std::unique_ptr&lt;PeerMessageQueue&gt; queue,</a>
<a name="ln318">    std::unique_ptr&lt;PeerManager&gt; peer_manager,</a>
<a name="ln319">    std::unique_ptr&lt;ThreadPoolToken&gt; raft_pool_token,</a>
<a name="ln320">    const scoped_refptr&lt;MetricEntity&gt;&amp; metric_entity,</a>
<a name="ln321">    const std::string&amp; peer_uuid, const scoped_refptr&lt;server::Clock&gt;&amp; clock,</a>
<a name="ln322">    ConsensusContext* consensus_context, const scoped_refptr&lt;log::Log&gt;&amp; log,</a>
<a name="ln323">    shared_ptr&lt;MemTracker&gt; parent_mem_tracker,</a>
<a name="ln324">    Callback&lt;void(std::shared_ptr&lt;StateChangeContext&gt; context)&gt; mark_dirty_clbk,</a>
<a name="ln325">    TableType table_type,</a>
<a name="ln326">    RetryableRequests* retryable_requests,</a>
<a name="ln327">    const yb::OpId&amp; split_op_id)</a>
<a name="ln328">    : raft_pool_token_(std::move(raft_pool_token)),</a>
<a name="ln329">      log_(log),</a>
<a name="ln330">      clock_(clock),</a>
<a name="ln331">      peer_proxy_factory_(std::move(proxy_factory)),</a>
<a name="ln332">      peer_manager_(std::move(peer_manager)),</a>
<a name="ln333">      queue_(std::move(queue)),</a>
<a name="ln334">      rng_(GetRandomSeed32()),</a>
<a name="ln335">      withhold_votes_until_(MonoTime::Min()),</a>
<a name="ln336">      mark_dirty_clbk_(std::move(mark_dirty_clbk)),</a>
<a name="ln337">      shutdown_(false),</a>
<a name="ln338">      follower_memory_pressure_rejections_(metric_entity-&gt;FindOrCreateCounter(</a>
<a name="ln339">          &amp;METRIC_follower_memory_pressure_rejections)),</a>
<a name="ln340">      term_metric_(metric_entity-&gt;FindOrCreateGauge(&amp;METRIC_raft_term,</a>
<a name="ln341">                                                    cmeta-&gt;current_term())),</a>
<a name="ln342">      follower_last_update_time_ms_metric_(</a>
<a name="ln343">          metric_entity-&gt;FindOrCreateAtomicMillisLag(&amp;METRIC_follower_lag_ms, clock_)),</a>
<a name="ln344">      is_raft_leader_metric_(metric_entity-&gt;FindOrCreateGauge(&amp;METRIC_is_raft_leader,</a>
<a name="ln345">                                                              static_cast&lt;int64_t&gt;(0))),</a>
<a name="ln346">      parent_mem_tracker_(std::move(parent_mem_tracker)),</a>
<a name="ln347">      table_type_(table_type),</a>
<a name="ln348">      update_raft_config_dns_latency_(</a>
<a name="ln349">          METRIC_dns_resolve_latency_during_update_raft_config.Instantiate(metric_entity)) {</a>
<a name="ln350">  DCHECK_NOTNULL(log_.get());</a>
<a name="ln351"> </a>
<a name="ln352">  if (PREDICT_FALSE(FLAGS_TEST_follower_reject_update_consensus_requests_seconds &gt; 0)) {</a>
<a name="ln353">    withold_replica_updates_until_ = MonoTime::Now() +</a>
<a name="ln354">        MonoDelta::FromSeconds(FLAGS_TEST_follower_reject_update_consensus_requests_seconds);</a>
<a name="ln355">  }</a>
<a name="ln356"> </a>
<a name="ln357">  state_ = std::make_unique&lt;ReplicaState&gt;(</a>
<a name="ln358">      options,</a>
<a name="ln359">      peer_uuid,</a>
<a name="ln360">      std::move(cmeta),</a>
<a name="ln361">      DCHECK_NOTNULL(consensus_context),</a>
<a name="ln362">      this,</a>
<a name="ln363">      retryable_requests,</a>
<a name="ln364">      split_op_id,</a>
<a name="ln365">      std::bind(&amp;PeerMessageQueue::TrackOperationsMemory, queue_.get(), _1));</a>
<a name="ln366"> </a>
<a name="ln367">  peer_manager_-&gt;SetConsensus(this);</a>
<a name="ln368">}</a>
<a name="ln369"> </a>
<a name="ln370">RaftConsensus::~RaftConsensus() {</a>
<a name="ln371">  Shutdown();</a>
<a name="ln372">}</a>
<a name="ln373"> </a>
<a name="ln374">Status RaftConsensus::Start(const ConsensusBootstrapInfo&amp; info) {</a>
<a name="ln375">  RETURN_NOT_OK(ExecuteHook(PRE_START));</a>
<a name="ln376"> </a>
<a name="ln377">  // Capture a weak_ptr reference into the functor so it can safely handle</a>
<a name="ln378">  // outliving the consensus instance.</a>
<a name="ln379">  std::weak_ptr&lt;RaftConsensus&gt; w = shared_from_this();</a>
<a name="ln380">  failure_detector_ = PeriodicTimer::Create(</a>
<a name="ln381">      peer_proxy_factory_-&gt;messenger(),</a>
<a name="ln382">      [w]() {</a>
<a name="ln383">        if (auto consensus = w.lock()) {</a>
<a name="ln384">          consensus-&gt;ReportFailureDetected();</a>
<a name="ln385">        }</a>
<a name="ln386">      },</a>
<a name="ln387">      MinimumElectionTimeout());</a>
<a name="ln388"> </a>
<a name="ln389">  {</a>
<a name="ln390">    ReplicaState::UniqueLock lock;</a>
<a name="ln391">    RETURN_NOT_OK(state_-&gt;LockForStart(&amp;lock));</a>
<a name="ln392">    state_-&gt;ClearLeaderUnlocked();</a>
<a name="ln393"> </a>
<a name="ln394">    RETURN_NOT_OK_PREPEND(state_-&gt;StartUnlocked(info.last_id),</a>
<a name="ln395">                          &quot;Unable to start RAFT ReplicaState&quot;);</a>
<a name="ln396"> </a>
<a name="ln397">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Replica starting. Triggering &quot;</a>
<a name="ln398">                          &lt;&lt; info.orphaned_replicates.size()</a>
<a name="ln399">                          &lt;&lt; &quot; pending operations. Active config: &quot;</a>
<a name="ln400">                          &lt;&lt; state_-&gt;GetActiveConfigUnlocked().ShortDebugString();</a>
<a name="ln401">    for (const auto&amp; replicate : info.orphaned_replicates) {</a>
<a name="ln402">      ReplicateMsgPtr replicate_ptr = std::make_shared&lt;ReplicateMsg&gt;(*replicate);</a>
<a name="ln403">      RETURN_NOT_OK(StartReplicaOperationUnlocked(replicate_ptr, HybridTime::kInvalid));</a>
<a name="ln404">    }</a>
<a name="ln405"> </a>
<a name="ln406">    RETURN_NOT_OK(state_-&gt;InitCommittedOpIdUnlocked(yb::OpId::FromPB(info.last_committed_id)));</a>
<a name="ln407"> </a>
<a name="ln408">    queue_-&gt;Init(state_-&gt;GetLastReceivedOpIdUnlocked().ToPB&lt;OpIdPB&gt;());</a>
<a name="ln409">  }</a>
<a name="ln410"> </a>
<a name="ln411">  {</a>
<a name="ln412">    ReplicaState::UniqueLock lock;</a>
<a name="ln413">    RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;lock));</a>
<a name="ln414"> </a>
<a name="ln415">    // If this is the first term expire the FD immediately so that we have a fast first</a>
<a name="ln416">    // election, otherwise we just let the timer expire normally.</a>
<a name="ln417">    MonoDelta initial_delta = MonoDelta();</a>
<a name="ln418">    if (state_-&gt;GetCurrentTermUnlocked() == 0) {</a>
<a name="ln419">      // The failure detector is initialized to a low value to trigger an early election</a>
<a name="ln420">      // (unless someone else requested a vote from us first, which resets the</a>
<a name="ln421">      // election timer). We do it this way instead of immediately running an</a>
<a name="ln422">      // election to get a higher likelihood of enough servers being available</a>
<a name="ln423">      // when the first one attempts an election to avoid multiple election</a>
<a name="ln424">      // cycles on startup, while keeping that &quot;waiting period&quot; random. If there is only one peer,</a>
<a name="ln425">      // trigger an election right away.</a>
<a name="ln426">      if (PREDICT_TRUE(FLAGS_enable_leader_failure_detection)) {</a>
<a name="ln427">        LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Consensus starting up: Expiring fail detector timer &quot;</a>
<a name="ln428">                                 &quot;to make a prompt election more likely&quot;;</a>
<a name="ln429">        // Gating quick leader elections on table creation since prompter leader elections are</a>
<a name="ln430">        // more likely to fail due to uninitialized peers or conflicting elections, which could</a>
<a name="ln431">        // have unforseen consequences.</a>
<a name="ln432">        if (FLAGS_quick_leader_election_on_create) {</a>
<a name="ln433">          initial_delta = (state_-&gt;GetCommittedConfigUnlocked().peers_size() == 1) ?</a>
<a name="ln434">              MonoDelta::kZero :</a>
<a name="ln435">              MonoDelta::FromMilliseconds(rng_.Uniform(FLAGS_raft_heartbeat_interval_ms));</a>
<a name="ln436">        }</a>
<a name="ln437">      }</a>
<a name="ln438">    }</a>
<a name="ln439">    RETURN_NOT_OK(BecomeReplicaUnlocked(std::string(), initial_delta));</a>
<a name="ln440">  }</a>
<a name="ln441"> </a>
<a name="ln442">  RETURN_NOT_OK(ExecuteHook(POST_START));</a>
<a name="ln443"> </a>
<a name="ln444">  // The context tracks that the current caller does not hold the lock for consensus state.</a>
<a name="ln445">  // So mark dirty callback, e.g., consensus-&gt;ConsensusState() for master consensus callback of</a>
<a name="ln446">  // SysCatalogStateChanged, can get the lock when needed.</a>
<a name="ln447">  auto context = std::make_shared&lt;StateChangeContext&gt;(StateChangeReason::CONSENSUS_STARTED, false);</a>
<a name="ln448">  // Report become visible to the Master.</a>
<a name="ln449">  MarkDirty(context);</a>
<a name="ln450"> </a>
<a name="ln451">  return Status::OK();</a>
<a name="ln452">}</a>
<a name="ln453"> </a>
<a name="ln454">bool RaftConsensus::IsRunning() const {</a>
<a name="ln455">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln456">  return state_-&gt;state() == ReplicaState::kRunning;</a>
<a name="ln457">}</a>
<a name="ln458"> </a>
<a name="ln459">Status RaftConsensus::EmulateElection() {</a>
<a name="ln460">  ReplicaState::UniqueLock lock;</a>
<a name="ln461">  RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;lock));</a>
<a name="ln462"> </a>
<a name="ln463">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Emulating election...&quot;;</a>
<a name="ln464"> </a>
<a name="ln465">  // Assume leadership of new term.</a>
<a name="ln466">  RETURN_NOT_OK(IncrementTermUnlocked());</a>
<a name="ln467">  SetLeaderUuidUnlocked(state_-&gt;GetPeerUuid());</a>
<a name="ln468">  return BecomeLeaderUnlocked();</a>
<a name="ln469">}</a>
<a name="ln470"> </a>
<a name="ln471">Status RaftConsensus::DoStartElection(const LeaderElectionData&amp; data, PreElected preelected) {</a>
<a name="ln472">  TRACE_EVENT2(&quot;consensus&quot;, &quot;RaftConsensus::StartElection&quot;,</a>
<a name="ln473">               &quot;peer&quot;, peer_uuid(),</a>
<a name="ln474">               &quot;tablet&quot;, tablet_id());</a>
<a name="ln475">  VLOG(1) &lt;&lt; &quot;RaftConsensus::StartElection for tablet id &quot; &lt;&lt; tablet_id() &lt;&lt; &quot; &quot; &lt;&lt; data.ToString();</a>
<a name="ln476">  if (FLAGS_TEST_do_not_start_election_test_only) {</a>
<a name="ln477">    LOG(INFO) &lt;&lt; &quot;Election start skipped as TEST_do_not_start_election_test_only flag &quot;</a>
<a name="ln478">                 &quot;is set to true.&quot;;</a>
<a name="ln479">    return Status::OK();</a>
<a name="ln480">  }</a>
<a name="ln481"> </a>
<a name="ln482">  // If pre-elections disabled or we already won pre-election then start regular election,</a>
<a name="ln483">  // otherwise pre-election is started.</a>
<a name="ln484">  // Pre-elections could be disable via flag, or temporarily if some nodes do not support them.</a>
<a name="ln485">  auto preelection = ANNOTATE_UNPROTECTED_READ(FLAGS_use_preelection) &amp;&amp; !preelected &amp;&amp;</a>
<a name="ln486">                     disable_pre_elections_until_ &lt; CoarseMonoClock::now();</a>
<a name="ln487">  const char* election_name = preelection ? &quot;pre-election&quot; : &quot;election&quot;;</a>
<a name="ln488"> </a>
<a name="ln489">  LeaderElectionPtr election;</a>
<a name="ln490">  {</a>
<a name="ln491">    ReplicaState::UniqueLock lock;</a>
<a name="ln492">    RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;lock));</a>
<a name="ln493"> </a>
<a name="ln494">    RaftPeerPB::Role active_role = state_-&gt;GetActiveRoleUnlocked();</a>
<a name="ln495">    if (active_role == RaftPeerPB::LEADER) {</a>
<a name="ln496">      LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Not starting &quot; &lt;&lt; election_name &lt;&lt; &quot; -- already leader&quot;;</a>
<a name="ln497">      return Status::OK();</a>
<a name="ln498">    }</a>
<a name="ln499">    if (active_role == RaftPeerPB::LEARNER || active_role == RaftPeerPB::READ_REPLICA) {</a>
<a name="ln500">      LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Not starting &quot; &lt;&lt; election_name &lt;&lt; &quot; -- role is &quot; &lt;&lt; active_role</a>
<a name="ln501">                            &lt;&lt; &quot;, pending = &quot; &lt;&lt; state_-&gt;IsConfigChangePendingUnlocked()</a>
<a name="ln502">                            &lt;&lt; &quot;, active_role=&quot; &lt;&lt; active_role;</a>
<a name="ln503">      return Status::OK();</a>
<a name="ln504">    }</a>
<a name="ln505">    if (PREDICT_FALSE(active_role == RaftPeerPB::NON_PARTICIPANT)) {</a>
<a name="ln506">      // Avoid excessive election noise while in this state.</a>
<a name="ln507">      SnoozeFailureDetector(DO_NOT_LOG);</a>
<a name="ln508">      return STATUS_FORMAT(</a>
<a name="ln509">          IllegalState,</a>
<a name="ln510">          &quot;Not starting $0: Node is currently a non-participant in the raft config: $1&quot;,</a>
<a name="ln511">          election_name, state_-&gt;GetActiveConfigUnlocked());</a>
<a name="ln512">    }</a>
<a name="ln513"> </a>
<a name="ln514">    // Default is to start the election now. But if we are starting a pending election, see if</a>
<a name="ln515">    // there is an op id pending upon indeed and if it has been committed to the log. The op id</a>
<a name="ln516">    // could have been cleared if the pending election has already been started or another peer</a>
<a name="ln517">    // has jumped before we can start.</a>
<a name="ln518">    bool start_now = true;</a>
<a name="ln519">    if (data.pending_commit) {</a>
<a name="ln520">      const auto required_id =</a>
<a name="ln521">          data.must_be_committed_opid.IsInitialized() ? data.must_be_committed_opid</a>
<a name="ln522">                                                      : state_-&gt;GetPendingElectionOpIdUnlocked();</a>
<a name="ln523">      const Status advance_committed_index_status = ResultToStatus(</a>
<a name="ln524">          state_-&gt;AdvanceCommittedOpIdUnlocked(yb::OpId::FromPB(required_id), CouldStop::kFalse));</a>
<a name="ln525">      if (!advance_committed_index_status.ok()) {</a>
<a name="ln526">        LOG(WARNING) &lt;&lt; &quot;Starting an &quot; &lt;&lt; election_name &lt;&lt; &quot; but the latest committed OpId is not &quot;</a>
<a name="ln527">                        &quot;present in this peer's log: &quot;</a>
<a name="ln528">                     &lt;&lt; required_id.ShortDebugString() &lt;&lt; &quot;. &quot;</a>
<a name="ln529">                     &lt;&lt; &quot;Status: &quot; &lt;&lt; advance_committed_index_status.ToString();</a>
<a name="ln530">      }</a>
<a name="ln531">      start_now = required_id.index() &lt;= state_-&gt;GetCommittedOpIdUnlocked().index;</a>
<a name="ln532">    }</a>
<a name="ln533"> </a>
<a name="ln534">    if (start_now) {</a>
<a name="ln535">      if (state_-&gt;HasLeaderUnlocked()) {</a>
<a name="ln536">        LOG_WITH_PREFIX(INFO)</a>
<a name="ln537">            &lt;&lt; &quot;Fail of leader &quot; &lt;&lt; state_-&gt;GetLeaderUuidUnlocked()</a>
<a name="ln538">            &lt;&lt; &quot; detected. Triggering leader &quot; &lt;&lt; election_name &lt;&lt; &quot;, mode=&quot; &lt;&lt; data.mode;</a>
<a name="ln539">      } else {</a>
<a name="ln540">        LOG_WITH_PREFIX(INFO)</a>
<a name="ln541">            &lt;&lt; &quot;Triggering leader &quot; &lt;&lt; election_name &lt;&lt; &quot;, mode=&quot; &lt;&lt; data.mode;</a>
<a name="ln542">      }</a>
<a name="ln543"> </a>
<a name="ln544">      // Snooze to avoid the election timer firing again as much as possible.</a>
<a name="ln545">      // We do not disable the election timer while running an election.</a>
<a name="ln546">      MonoDelta timeout = LeaderElectionExpBackoffDeltaUnlocked();</a>
<a name="ln547">      SnoozeFailureDetector(ALLOW_LOGGING, timeout);</a>
<a name="ln548"> </a>
<a name="ln549">      election = VERIFY_RESULT(CreateElectionUnlocked(</a>
<a name="ln550">          data, timeout, PreElection(preelection)));</a>
<a name="ln551">    } else if (data.pending_commit &amp;&amp; data.must_be_committed_opid.IsInitialized()) {</a>
<a name="ln552">      // Queue up the pending op id if specified.</a>
<a name="ln553">      state_-&gt;SetPendingElectionOpIdUnlocked(data.must_be_committed_opid);</a>
<a name="ln554">      LOG(INFO) &lt;&lt; &quot;Leader &quot; &lt;&lt; election_name &lt;&lt; &quot; is pending upon log commitment of OpId &quot;</a>
<a name="ln555">                &lt;&lt; data.must_be_committed_opid.ShortDebugString();</a>
<a name="ln556">    }</a>
<a name="ln557">  }</a>
<a name="ln558"> </a>
<a name="ln559">  // Start the election outside the lock.</a>
<a name="ln560">  if (election) {</a>
<a name="ln561">    election-&gt;Run();</a>
<a name="ln562">  }</a>
<a name="ln563"> </a>
<a name="ln564">  return Status::OK();</a>
<a name="ln565">}</a>
<a name="ln566"> </a>
<a name="ln567">Result&lt;LeaderElectionPtr&gt; RaftConsensus::CreateElectionUnlocked(</a>
<a name="ln568">    const LeaderElectionData&amp; data, MonoDelta timeout, PreElection preelection) {</a>
<a name="ln569">  int64_t new_term;</a>
<a name="ln570">  if (preelection) {</a>
<a name="ln571">    new_term = state_-&gt;GetCurrentTermUnlocked() + 1;</a>
<a name="ln572">  } else {</a>
<a name="ln573">    // Increment the term.</a>
<a name="ln574">    RETURN_NOT_OK(IncrementTermUnlocked());</a>
<a name="ln575">    new_term = state_-&gt;GetCurrentTermUnlocked();</a>
<a name="ln576">  }</a>
<a name="ln577"> </a>
<a name="ln578">  const RaftConfigPB&amp; active_config = state_-&gt;GetActiveConfigUnlocked();</a>
<a name="ln579">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Starting &quot; &lt;&lt; (preelection ? &quot;pre-&quot; : &quot;&quot;) &lt;&lt; &quot;election with config: &quot;</a>
<a name="ln580">                        &lt;&lt; active_config.ShortDebugString();</a>
<a name="ln581"> </a>
<a name="ln582">  // Initialize the VoteCounter.</a>
<a name="ln583">  int num_voters = CountVoters(active_config);</a>
<a name="ln584">  int majority_size = MajoritySize(num_voters);</a>
<a name="ln585"> </a>
<a name="ln586">  // Vote for ourselves.</a>
<a name="ln587">  if (!preelection) {</a>
<a name="ln588">    // TODO: Consider using a separate Mutex for voting, which must sync to disk.</a>
<a name="ln589">    RETURN_NOT_OK(state_-&gt;SetVotedForCurrentTermUnlocked(state_-&gt;GetPeerUuid()));</a>
<a name="ln590">  }</a>
<a name="ln591"> </a>
<a name="ln592">  auto counter = std::make_unique&lt;VoteCounter&gt;(num_voters, majority_size);</a>
<a name="ln593">  bool duplicate;</a>
<a name="ln594">  RETURN_NOT_OK(counter-&gt;RegisterVote(state_-&gt;GetPeerUuid(), ElectionVote::kGranted, &amp;duplicate));</a>
<a name="ln595">  CHECK(!duplicate) &lt;&lt; state_-&gt;LogPrefix()</a>
<a name="ln596">                    &lt;&lt; &quot;Inexplicable duplicate self-vote for term &quot;</a>
<a name="ln597">                    &lt;&lt; state_-&gt;GetCurrentTermUnlocked();</a>
<a name="ln598"> </a>
<a name="ln599">  VoteRequestPB request;</a>
<a name="ln600">  request.set_ignore_live_leader(data.mode == ElectionMode::ELECT_EVEN_IF_LEADER_IS_ALIVE);</a>
<a name="ln601">  request.set_candidate_uuid(state_-&gt;GetPeerUuid());</a>
<a name="ln602">  request.set_candidate_term(new_term);</a>
<a name="ln603">  request.set_tablet_id(state_-&gt;GetOptions().tablet_id);</a>
<a name="ln604">  request.set_preelection(preelection);</a>
<a name="ln605">  state_-&gt;GetLastReceivedOpIdUnlocked().ToPB(</a>
<a name="ln606">      request.mutable_candidate_status()-&gt;mutable_last_received());</a>
<a name="ln607"> </a>
<a name="ln608">  LeaderElectionPtr result(new LeaderElection(</a>
<a name="ln609">      active_config,</a>
<a name="ln610">      peer_proxy_factory_.get(),</a>
<a name="ln611">      request,</a>
<a name="ln612">      std::move(counter),</a>
<a name="ln613">      timeout,</a>
<a name="ln614">      preelection,</a>
<a name="ln615">      data.suppress_vote_request,</a>
<a name="ln616">      std::bind(&amp;RaftConsensus::ElectionCallback, shared_from_this(), data, _1)));</a>
<a name="ln617"> </a>
<a name="ln618">  if (!preelection) {</a>
<a name="ln619">    // Clear the pending election op id so that we won't start the same pending election again.</a>
<a name="ln620">    // Pre-election does not change state, so should not do it in this case.</a>
<a name="ln621">    state_-&gt;ClearPendingElectionOpIdUnlocked();</a>
<a name="ln622">  }</a>
<a name="ln623"> </a>
<a name="ln624">  return result;</a>
<a name="ln625">}</a>
<a name="ln626"> </a>
<a name="ln627">Status RaftConsensus::WaitUntilLeaderForTests(const MonoDelta&amp; timeout) {</a>
<a name="ln628">  MonoTime deadline = MonoTime::Now();</a>
<a name="ln629">  deadline.AddDelta(timeout);</a>
<a name="ln630">  while (MonoTime::Now().ComesBefore(deadline)) {</a>
<a name="ln631">    if (GetLeaderStatus() == LeaderStatus::LEADER_AND_READY) {</a>
<a name="ln632">      return Status::OK();</a>
<a name="ln633">    }</a>
<a name="ln634">    SleepFor(MonoDelta::FromMilliseconds(10));</a>
<a name="ln635">  }</a>
<a name="ln636"> </a>
<a name="ln637">  return STATUS(TimedOut, Substitute(&quot;Peer $0 is not leader of tablet $1 after $2. Role: $3&quot;,</a>
<a name="ln638">                                     peer_uuid(), tablet_id(), timeout.ToString(), role()));</a>
<a name="ln639">}</a>
<a name="ln640"> </a>
<a name="ln641">string RaftConsensus::ServersInTransitionMessage() {</a>
<a name="ln642">  string err_msg;</a>
<a name="ln643">  const RaftConfigPB&amp; active_config = state_-&gt;GetActiveConfigUnlocked();</a>
<a name="ln644">  const RaftConfigPB&amp; committed_config = state_-&gt;GetCommittedConfigUnlocked();</a>
<a name="ln645">  int servers_in_transition = CountServersInTransition(active_config);</a>
<a name="ln646">  int committed_servers_in_transition = CountServersInTransition(committed_config);</a>
<a name="ln647">  LOG(INFO) &lt;&lt; Substitute(&quot;Active config has $0 and committed has $1 servers in transition.&quot;,</a>
<a name="ln648">                          servers_in_transition, committed_servers_in_transition);</a>
<a name="ln649">  if (servers_in_transition != 0 || committed_servers_in_transition != 0) {</a>
<a name="ln650">    err_msg = Substitute(&quot;Leader not ready to step down as there are $0 active config peers&quot;</a>
<a name="ln651">                         &quot; in transition, $1 in committed. Configs:\nactive=$2\ncommit=$3&quot;,</a>
<a name="ln652">                         servers_in_transition, committed_servers_in_transition,</a>
<a name="ln653">                         active_config.ShortDebugString(), committed_config.ShortDebugString());</a>
<a name="ln654">    LOG(INFO) &lt;&lt; err_msg;</a>
<a name="ln655">  }</a>
<a name="ln656">  return err_msg;</a>
<a name="ln657">}</a>
<a name="ln658"> </a>
<a name="ln659">Status RaftConsensus::StepDown(const LeaderStepDownRequestPB* req, LeaderStepDownResponsePB* resp) {</a>
<a name="ln660">  TRACE_EVENT0(&quot;consensus&quot;, &quot;RaftConsensus::StepDown&quot;);</a>
<a name="ln661">  ReplicaState::UniqueLock lock;</a>
<a name="ln662">  RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;lock));</a>
<a name="ln663"> </a>
<a name="ln664">  // A sanity check that this request was routed to the correct RaftConsensus.</a>
<a name="ln665">  const auto&amp; tablet_id = req-&gt;tablet_id();</a>
<a name="ln666">  if (tablet_id != this-&gt;tablet_id()) {</a>
<a name="ln667">    resp-&gt;mutable_error()-&gt;set_code(TabletServerErrorPB::UNKNOWN_ERROR);</a>
<a name="ln668">    const auto msg = Format(</a>
<a name="ln669">        &quot;Received a leader stepdown operation for wrong tablet id: $0, must be: $1&quot;,</a>
<a name="ln670">        tablet_id, this-&gt;tablet_id());</a>
<a name="ln671">    LOG_WITH_PREFIX(ERROR) &lt;&lt; msg;</a>
<a name="ln672">    StatusToPB(STATUS(IllegalState, msg), resp-&gt;mutable_error()-&gt;mutable_status());</a>
<a name="ln673">    return Status::OK();</a>
<a name="ln674">  }</a>
<a name="ln675"> </a>
<a name="ln676">  if (state_-&gt;GetActiveRoleUnlocked() != RaftPeerPB::LEADER) {</a>
<a name="ln677">    resp-&gt;mutable_error()-&gt;set_code(TabletServerErrorPB::NOT_THE_LEADER);</a>
<a name="ln678">    StatusToPB(STATUS(IllegalState, &quot;Not currently leader&quot;),</a>
<a name="ln679">               resp-&gt;mutable_error()-&gt;mutable_status());</a>
<a name="ln680">    // We return OK so that the tablet service won't overwrite the error code.</a>
<a name="ln681">    return Status::OK();</a>
<a name="ln682">  }</a>
<a name="ln683"> </a>
<a name="ln684">  // The leader needs to be ready to perform a step down. There should be no PRE_VOTER in both</a>
<a name="ln685">  // active and committed configs - ENG-557.</a>
<a name="ln686">  const string err_msg = ServersInTransitionMessage();</a>
<a name="ln687">  if (!err_msg.empty()) {</a>
<a name="ln688">    resp-&gt;mutable_error()-&gt;set_code(TabletServerErrorPB::LEADER_NOT_READY_TO_STEP_DOWN);</a>
<a name="ln689">    StatusToPB(STATUS(IllegalState, err_msg), resp-&gt;mutable_error()-&gt;mutable_status());</a>
<a name="ln690">    return Status::OK();</a>
<a name="ln691">  }</a>
<a name="ln692"> </a>
<a name="ln693">  std::string new_leader_uuid;</a>
<a name="ln694">  // If a new leader is nominated, find it among peers to send RunLeaderElection request.</a>
<a name="ln695">  // See https://ramcloud.stanford.edu/~ongaro/thesis.pdf, section 3.10 for this mechanism</a>
<a name="ln696">  // to transfer the leadership.</a>
<a name="ln697">  const bool forced = (req-&gt;has_force_step_down() &amp;&amp; req-&gt;force_step_down());</a>
<a name="ln698">  if (req-&gt;has_new_leader_uuid()) {</a>
<a name="ln699">    new_leader_uuid = req-&gt;new_leader_uuid();</a>
<a name="ln700">    if (!forced &amp;&amp; !queue_-&gt;CanPeerBecomeLeader(new_leader_uuid)) {</a>
<a name="ln701">      resp-&gt;mutable_error()-&gt;set_code(TabletServerErrorPB::LEADER_NOT_READY_TO_STEP_DOWN);</a>
<a name="ln702">      StatusToPB(</a>
<a name="ln703">          STATUS(IllegalState, &quot;Suggested peer is not caught up yet&quot;),</a>
<a name="ln704">          resp-&gt;mutable_error()-&gt;mutable_status());</a>
<a name="ln705">      // We return OK so that the tablet service won't overwrite the error code.</a>
<a name="ln706">      return Status::OK();</a>
<a name="ln707">    }</a>
<a name="ln708">  }</a>
<a name="ln709"> </a>
<a name="ln710">  bool graceful_stepdown = false;</a>
<a name="ln711">  if (new_leader_uuid.empty() &amp;&amp; !FLAGS_stepdown_disable_graceful_transition &amp;&amp;</a>
<a name="ln712">      !(req-&gt;has_disable_graceful_transition() &amp;&amp; req-&gt;disable_graceful_transition())) {</a>
<a name="ln713">    new_leader_uuid = queue_-&gt;GetUpToDatePeer();</a>
<a name="ln714">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Selected up to date candidate protege leader [&quot; &lt;&lt; new_leader_uuid</a>
<a name="ln715">                          &lt;&lt; &quot;]&quot;;</a>
<a name="ln716">    graceful_stepdown = true;</a>
<a name="ln717">  }</a>
<a name="ln718"> </a>
<a name="ln719">  const auto&amp; local_peer_uuid = state_-&gt;GetPeerUuid();</a>
<a name="ln720">  if (!new_leader_uuid.empty()) {</a>
<a name="ln721">    const auto leadership_transfer_description =</a>
<a name="ln722">        Format(&quot;tablet $0 from $1 to $2&quot;, tablet_id, local_peer_uuid, new_leader_uuid);</a>
<a name="ln723">    if (!forced &amp;&amp; new_leader_uuid == protege_leader_uuid_ &amp;&amp; election_lost_by_protege_at_) {</a>
<a name="ln724">      const MonoDelta time_since_election_loss_by_protege =</a>
<a name="ln725">          MonoTime::Now() - election_lost_by_protege_at_;</a>
<a name="ln726">      if (time_since_election_loss_by_protege.ToMilliseconds() &lt;</a>
<a name="ln727">              FLAGS_min_leader_stepdown_retry_interval_ms) {</a>
<a name="ln728">        LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Unable to execute leadership transfer for &quot;</a>
<a name="ln729">                              &lt;&lt; leadership_transfer_description</a>
<a name="ln730">                              &lt;&lt; &quot; because the intended leader already lost an election only &quot;</a>
<a name="ln731">                              &lt;&lt; ToString(time_since_election_loss_by_protege) &lt;&lt; &quot; ago (within &quot;</a>
<a name="ln732">                              &lt;&lt; FLAGS_min_leader_stepdown_retry_interval_ms &lt;&lt; &quot; ms).&quot;;</a>
<a name="ln733">        if (req-&gt;has_new_leader_uuid()) {</a>
<a name="ln734">          LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Rejecting leader stepdown request for &quot;</a>
<a name="ln735">                                &lt;&lt; leadership_transfer_description;</a>
<a name="ln736">          resp-&gt;mutable_error()-&gt;set_code(TabletServerErrorPB::LEADER_NOT_READY_TO_STEP_DOWN);</a>
<a name="ln737">          resp-&gt;set_time_since_election_failure_ms(</a>
<a name="ln738">              time_since_election_loss_by_protege.ToMilliseconds());</a>
<a name="ln739">          StatusToPB(</a>
<a name="ln740">              STATUS(IllegalState, &quot;Suggested peer lost an election recently&quot;),</a>
<a name="ln741">              resp-&gt;mutable_error()-&gt;mutable_status());</a>
<a name="ln742">          // We return OK so that the tablet service won't overwrite the error code.</a>
<a name="ln743">          return Status::OK();</a>
<a name="ln744">        } else {</a>
<a name="ln745">          // we were attempting a graceful transfer of our own choice</a>
<a name="ln746">          // which is no longer possible</a>
<a name="ln747">          new_leader_uuid.clear();</a>
<a name="ln748">        }</a>
<a name="ln749">      }</a>
<a name="ln750">      election_lost_by_protege_at_ = MonoTime();</a>
<a name="ln751">    }</a>
<a name="ln752">  }</a>
<a name="ln753"> </a>
<a name="ln754">  if (!new_leader_uuid.empty()) {</a>
<a name="ln755">    bool new_leader_found = false;</a>
<a name="ln756">    const RaftConfigPB&amp; active_config = state_-&gt;GetActiveConfigUnlocked();</a>
<a name="ln757">    for (const RaftPeerPB&amp; peer : active_config.peers()) {</a>
<a name="ln758">      if (peer.member_type() == RaftPeerPB::VOTER &amp;&amp;</a>
<a name="ln759">          peer.permanent_uuid() == new_leader_uuid) {</a>
<a name="ln760">        auto election_state = std::make_shared&lt;RunLeaderElectionState&gt;();</a>
<a name="ln761">        // TODO(sergei) Currently we preserved synchronous DNS resolution in this case.</a>
<a name="ln762">        // It is possible that it should be changed to async in future.</a>
<a name="ln763">        // But it looks like it is not a problem to leave synchronous variant here.</a>
<a name="ln764">        election_state-&gt;proxy = peer_proxy_factory_-&gt;NewProxy(peer);</a>
<a name="ln765">        election_state-&gt;req.set_originator_uuid(req-&gt;dest_uuid());</a>
<a name="ln766">        election_state-&gt;req.set_dest_uuid(new_leader_uuid);</a>
<a name="ln767">        election_state-&gt;req.set_tablet_id(tablet_id);</a>
<a name="ln768">        election_state-&gt;rpc.set_invoke_callback_mode(rpc::InvokeCallbackMode::kThreadPoolHigh);</a>
<a name="ln769">        state_-&gt;GetCommittedOpIdUnlocked().ToPB(election_state-&gt;req.mutable_committed_index());</a>
<a name="ln770">        election_state-&gt;proxy-&gt;RunLeaderElectionAsync(</a>
<a name="ln771">            &amp;election_state-&gt;req, &amp;election_state-&gt;resp, &amp;election_state-&gt;rpc,</a>
<a name="ln772">            std::bind(&amp;RaftConsensus::RunLeaderElectionResponseRpcCallback, this,</a>
<a name="ln773">                election_state));</a>
<a name="ln774">        new_leader_found = true;</a>
<a name="ln775">        const auto leadership_transfer_description =</a>
<a name="ln776">            Format(&quot;tablet $0 from $1 to $2&quot;, tablet_id, local_peer_uuid, new_leader_uuid);</a>
<a name="ln777">        LOG(INFO) &lt;&lt; &quot;Transferring leadership of &quot; &lt;&lt; leadership_transfer_description;</a>
<a name="ln778">        break;</a>
<a name="ln779">      }</a>
<a name="ln780">    }</a>
<a name="ln781">    if (!new_leader_found) {</a>
<a name="ln782">      LOG(WARNING) &lt;&lt; &quot;New leader &quot; &lt;&lt; new_leader_uuid &lt;&lt; &quot; not found among &quot; &lt;&lt; tablet_id</a>
<a name="ln783">                   &lt;&lt; &quot; tablet peers.&quot;;</a>
<a name="ln784">      if (req-&gt;has_new_leader_uuid()) {</a>
<a name="ln785">        resp-&gt;mutable_error()-&gt;set_code(TabletServerErrorPB::LEADER_NOT_READY_TO_STEP_DOWN);</a>
<a name="ln786">        StatusToPB(</a>
<a name="ln787">            STATUS(IllegalState, &quot;New leader not found among peers&quot;),</a>
<a name="ln788">            resp-&gt;mutable_error()-&gt;mutable_status());</a>
<a name="ln789">        // We return OK so that the tablet service won't overwrite the error code.</a>
<a name="ln790">        return Status::OK();</a>
<a name="ln791">      } else {</a>
<a name="ln792">        // we were attempting a graceful transfer of our own choice</a>
<a name="ln793">        // which is no longer possible</a>
<a name="ln794">        new_leader_uuid.clear();</a>
<a name="ln795">      }</a>
<a name="ln796">    }</a>
<a name="ln797">  }</a>
<a name="ln798"> </a>
<a name="ln799">  RETURN_NOT_OK(BecomeReplicaUnlocked(new_leader_uuid, MonoDelta(), graceful_stepdown));</a>
<a name="ln800"> </a>
<a name="ln801">  return Status::OK();</a>
<a name="ln802">}</a>
<a name="ln803"> </a>
<a name="ln804">Status RaftConsensus::ElectionLostByProtege(const std::string&amp; election_lost_by_uuid) {</a>
<a name="ln805">  if (election_lost_by_uuid.empty()) {</a>
<a name="ln806">    return STATUS(InvalidArgument, &quot;election_lost_by_uuid could not be empty&quot;);</a>
<a name="ln807">  }</a>
<a name="ln808"> </a>
<a name="ln809">  auto start_election = false;</a>
<a name="ln810">  {</a>
<a name="ln811">    ReplicaState::UniqueLock lock;</a>
<a name="ln812">    RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;lock));</a>
<a name="ln813">    if (graceful_stepdown_) {</a>
<a name="ln814">      return Status::OK();</a>
<a name="ln815">    }</a>
<a name="ln816">    if (election_lost_by_uuid == protege_leader_uuid_) {</a>
<a name="ln817">      LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Our protege &quot; &lt;&lt; election_lost_by_uuid</a>
<a name="ln818">                            &lt;&lt; &quot;, lost election. Has leader: &quot;</a>
<a name="ln819">                            &lt;&lt; state_-&gt;HasLeaderUnlocked();</a>
<a name="ln820">      withhold_election_start_until_.store(MonoTime::Min(), std::memory_order_relaxed);</a>
<a name="ln821">      election_lost_by_protege_at_ = MonoTime::Now();</a>
<a name="ln822"> </a>
<a name="ln823">      start_election = !state_-&gt;HasLeaderUnlocked();</a>
<a name="ln824">    }</a>
<a name="ln825">  }</a>
<a name="ln826"> </a>
<a name="ln827">  if (start_election) {</a>
<a name="ln828">    return StartElection({ElectionMode::NORMAL_ELECTION});</a>
<a name="ln829">  }</a>
<a name="ln830"> </a>
<a name="ln831">  return Status::OK();</a>
<a name="ln832">}</a>
<a name="ln833"> </a>
<a name="ln834">void RaftConsensus::WithholdElectionAfterStepDown(</a>
<a name="ln835">    const std::string&amp; protege_uuid, bool graceful_stepdown) {</a>
<a name="ln836">  DCHECK(state_-&gt;IsLocked());</a>
<a name="ln837">  protege_leader_uuid_ = protege_uuid;</a>
<a name="ln838">  graceful_stepdown_ = graceful_stepdown;</a>
<a name="ln839">  auto timeout = MonoDelta::FromMilliseconds(</a>
<a name="ln840">      FLAGS_leader_failure_max_missed_heartbeat_periods *</a>
<a name="ln841">      FLAGS_raft_heartbeat_interval_ms);</a>
<a name="ln842">  if (!protege_uuid.empty()) {</a>
<a name="ln843">    // Actually we have 2 kinds of step downs.</a>
<a name="ln844">    // 1) We step down in favor of some protege.</a>
<a name="ln845">    // 2) We step down because term was advanced or just started.</a>
<a name="ln846">    // In second case we should not withhold election for a long period of time.</a>
<a name="ln847">    timeout *= FLAGS_after_stepdown_delay_election_multiplier;</a>
<a name="ln848">  }</a>
<a name="ln849">  auto deadline = MonoTime::Now() + timeout;</a>
<a name="ln850">  VLOG(2) &lt;&lt; &quot;Withholding election for &quot; &lt;&lt; timeout;</a>
<a name="ln851">  withhold_election_start_until_.store(deadline, std::memory_order_release);</a>
<a name="ln852">  election_lost_by_protege_at_ = MonoTime();</a>
<a name="ln853">}</a>
<a name="ln854"> </a>
<a name="ln855">void RaftConsensus::RunLeaderElectionResponseRpcCallback(</a>
<a name="ln856">    shared_ptr&lt;RunLeaderElectionState&gt; election_state) {</a>
<a name="ln857">  // Check for RPC errors.</a>
<a name="ln858">  if (!election_state-&gt;rpc.status().ok()) {</a>
<a name="ln859">    LOG(WARNING) &lt;&lt; &quot;RPC error from RunLeaderElection() call to peer &quot;</a>
<a name="ln860">                 &lt;&lt; election_state-&gt;req.dest_uuid() &lt;&lt; &quot;: &quot;</a>
<a name="ln861">                 &lt;&lt; election_state-&gt;rpc.status().ToString();</a>
<a name="ln862">  // Check for tablet errors.</a>
<a name="ln863">  } else if (election_state-&gt;resp.has_error()) {</a>
<a name="ln864">    LOG(WARNING) &lt;&lt; &quot;Tablet error from RunLeaderElection() call to peer &quot;</a>
<a name="ln865">                 &lt;&lt; election_state-&gt;req.dest_uuid() &lt;&lt; &quot;: &quot;</a>
<a name="ln866">                 &lt;&lt; StatusFromPB(election_state-&gt;resp.error().status()).ToString();</a>
<a name="ln867">  }</a>
<a name="ln868">}</a>
<a name="ln869"> </a>
<a name="ln870">void RaftConsensus::ReportFailureDetectedTask() {</a>
<a name="ln871">  MonoTime now;</a>
<a name="ln872">  for (;;) {</a>
<a name="ln873">    // Do not start election for an extended period of time if we were recently stepped down.</a>
<a name="ln874">    auto old_value = withhold_election_start_until_.load(std::memory_order_acquire);</a>
<a name="ln875"> </a>
<a name="ln876">    if (old_value == MonoTime::Min()) {</a>
<a name="ln877">      break;</a>
<a name="ln878">    }</a>
<a name="ln879"> </a>
<a name="ln880">    if (!now.Initialized()) {</a>
<a name="ln881">      now = MonoTime::Now();</a>
<a name="ln882">    }</a>
<a name="ln883"> </a>
<a name="ln884">    if (now &lt; old_value) {</a>
<a name="ln885">      VLOG(1) &lt;&lt; &quot;Skipping election due to delayed timeout for &quot; &lt;&lt; (old_value - now);</a>
<a name="ln886">      return;</a>
<a name="ln887">    }</a>
<a name="ln888"> </a>
<a name="ln889">    // If we ever stepped down and then delayed election start did get scheduled, reset that we</a>
<a name="ln890">    // are out of that extra delay mode.</a>
<a name="ln891">    if (withhold_election_start_until_.compare_exchange_weak(</a>
<a name="ln892">        old_value, MonoTime::Min(), std::memory_order_release)) {</a>
<a name="ln893">      break;</a>
<a name="ln894">    }</a>
<a name="ln895">  }</a>
<a name="ln896"> </a>
<a name="ln897">  // Start an election.</a>
<a name="ln898">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;ReportFailDetected: Starting NORMAL_ELECTION...&quot;;</a>
<a name="ln899">  Status s = StartElection({ElectionMode::NORMAL_ELECTION});</a>
<a name="ln900">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln901">    LOG_WITH_PREFIX(WARNING) &lt;&lt; &quot;Failed to trigger leader election: &quot; &lt;&lt; s.ToString();</a>
<a name="ln902">  }</a>
<a name="ln903">}</a>
<a name="ln904"> </a>
<a name="ln905">void RaftConsensus::ReportFailureDetected() {</a>
<a name="ln906">  // We're running on a timer thread; start an election on a different thread pool.</a>
<a name="ln907">  WARN_NOT_OK(raft_pool_token_-&gt;SubmitFunc(std::bind(&amp;RaftConsensus::ReportFailureDetectedTask,</a>
<a name="ln908">                                                     shared_from_this())),</a>
<a name="ln909">              &quot;Failed to submit failure detected task&quot;);</a>
<a name="ln910">}</a>
<a name="ln911"> </a>
<a name="ln912">Status RaftConsensus::BecomeLeaderUnlocked() {</a>
<a name="ln913">  DCHECK(state_-&gt;IsLocked());</a>
<a name="ln914">  TRACE_EVENT2(&quot;consensus&quot;, &quot;RaftConsensus::BecomeLeaderUnlocked&quot;,</a>
<a name="ln915">               &quot;peer&quot;, peer_uuid(),</a>
<a name="ln916">               &quot;tablet&quot;, tablet_id());</a>
<a name="ln917">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Becoming Leader. State: &quot; &lt;&lt; state_-&gt;ToStringUnlocked();</a>
<a name="ln918"> </a>
<a name="ln919">  // Disable FD while we are leader.</a>
<a name="ln920">  DisableFailureDetector();</a>
<a name="ln921"> </a>
<a name="ln922">  // Don't vote for anyone if we're a leader.</a>
<a name="ln923">  withhold_votes_until_.store(MonoTime::Max(), std::memory_order_release);</a>
<a name="ln924"> </a>
<a name="ln925">  queue_-&gt;RegisterObserver(this);</a>
<a name="ln926"> </a>
<a name="ln927">  // Refresh queue and peers before initiating NO_OP.</a>
<a name="ln928">  RefreshConsensusQueueAndPeersUnlocked();</a>
<a name="ln929"> </a>
<a name="ln930">  // Initiate a NO_OP operation that is sent at the beginning of every term</a>
<a name="ln931">  // change in raft.</a>
<a name="ln932">  auto replicate = std::make_shared&lt;ReplicateMsg&gt;();</a>
<a name="ln933">  replicate-&gt;set_op_type(NO_OP);</a>
<a name="ln934">  replicate-&gt;mutable_noop_request(); // Define the no-op request field.</a>
<a name="ln935">  LOG(INFO) &lt;&lt; &quot;Sending NO_OP at op &quot; &lt;&lt; state_-&gt;GetCommittedOpIdUnlocked();</a>
<a name="ln936">  // This committed OpId is used for tablet bootstrap for RocksDB-backed tables.</a>
<a name="ln937">  state_-&gt;GetCommittedOpIdUnlocked().ToPB(replicate-&gt;mutable_committed_op_id());</a>
<a name="ln938"> </a>
<a name="ln939">  // TODO: We should have no-ops (?) and config changes be COMMIT_WAIT</a>
<a name="ln940">  // operations. See KUDU-798.</a>
<a name="ln941">  // Note: This hybrid_time has no meaning from a serialization perspective</a>
<a name="ln942">  // because this method is not executed on the TabletPeer's prepare thread.</a>
<a name="ln943">  replicate-&gt;set_hybrid_time(clock_-&gt;Now().ToUint64());</a>
<a name="ln944"> </a>
<a name="ln945">  scoped_refptr&lt;ConsensusRound&gt; round(new ConsensusRound(this, replicate));</a>
<a name="ln946">  round-&gt;SetConsensusReplicatedCallback(std::bind(&amp;RaftConsensus::NonTxRoundReplicationFinished,</a>
<a name="ln947">                                             this,</a>
<a name="ln948">                                             round.get(),</a>
<a name="ln949">                                             &amp;DoNothingStatusCB, std::placeholders::_1));</a>
<a name="ln950">  RETURN_NOT_OK(AppendNewRoundToQueueUnlocked(round));</a>
<a name="ln951"> </a>
<a name="ln952">  peer_manager_-&gt;SignalRequest(RequestTriggerMode::kNonEmptyOnly);</a>
<a name="ln953"> </a>
<a name="ln954">  // Set the timestamp to max uint64_t so that every time this metric is queried, the returned</a>
<a name="ln955">  // lag is 0. We will need to restore the timestamp once this peer steps down.</a>
<a name="ln956">  follower_last_update_time_ms_metric_-&gt;UpdateTimestampInMilliseconds(</a>
<a name="ln957">      std::numeric_limits&lt;uint64_t&gt;::max());</a>
<a name="ln958">  is_raft_leader_metric_-&gt;set_value(1);</a>
<a name="ln959"> </a>
<a name="ln960">  return Status::OK();</a>
<a name="ln961">}</a>
<a name="ln962"> </a>
<a name="ln963">Status RaftConsensus::BecomeReplicaUnlocked(</a>
<a name="ln964">    const std::string&amp; new_leader_uuid, MonoDelta initial_fd_wait, bool graceful_stepdown) {</a>
<a name="ln965">  LOG_WITH_PREFIX(INFO)</a>
<a name="ln966">      &lt;&lt; &quot;Becoming Follower/Learner. State: &quot; &lt;&lt; state_-&gt;ToStringUnlocked()</a>
<a name="ln967">      &lt;&lt; &quot;, new leader: &quot; &lt;&lt; new_leader_uuid &lt;&lt; &quot;, initial_fd_wait: &quot; &lt;&lt; initial_fd_wait;</a>
<a name="ln968"> </a>
<a name="ln969">  if (state_-&gt;GetActiveRoleUnlocked() == RaftPeerPB::LEADER) {</a>
<a name="ln970">    WithholdElectionAfterStepDown(new_leader_uuid, graceful_stepdown);</a>
<a name="ln971">  }</a>
<a name="ln972"> </a>
<a name="ln973">  state_-&gt;ClearLeaderUnlocked();</a>
<a name="ln974"> </a>
<a name="ln975">  // FD should be running while we are a follower.</a>
<a name="ln976">  EnableFailureDetector(initial_fd_wait);</a>
<a name="ln977"> </a>
<a name="ln978">  // Now that we're a replica, we can allow voting for other nodes.</a>
<a name="ln979">  withhold_votes_until_.store(MonoTime::Min(), std::memory_order_release);</a>
<a name="ln980"> </a>
<a name="ln981">  const Status unregister_observer_status = queue_-&gt;UnRegisterObserver(this);</a>
<a name="ln982">  if (!unregister_observer_status.IsNotFound()) {</a>
<a name="ln983">    RETURN_NOT_OK(unregister_observer_status);</a>
<a name="ln984">  }</a>
<a name="ln985">  // Deregister ourselves from the queue. We don't care what get's replicated, since</a>
<a name="ln986">  // we're stepping down.</a>
<a name="ln987">  queue_-&gt;SetNonLeaderMode();</a>
<a name="ln988"> </a>
<a name="ln989">  peer_manager_-&gt;Close();</a>
<a name="ln990"> </a>
<a name="ln991">  // TODO: https://github.com/yugabyte/yugabyte-db/issues/5522. Add unit tests for this metric.</a>
<a name="ln992">  // We update the follower lag metric timestamp here because it's possible that a leader</a>
<a name="ln993">  // that step downs could get partitioned before it receives any replicate message. If we</a>
<a name="ln994">  // don't update the timestamp here, and the above scenario happens, the metric will keep the</a>
<a name="ln995">  // uint64_t max value, which would make the metric return a 0 lag every time it is queried,</a>
<a name="ln996">  // even though that's not the case.</a>
<a name="ln997">  follower_last_update_time_ms_metric_-&gt;UpdateTimestampInMilliseconds(</a>
<a name="ln998">      clock_-&gt;Now().GetPhysicalValueMicros() / 1000);</a>
<a name="ln999">  is_raft_leader_metric_-&gt;set_value(0);</a>
<a name="ln1000"> </a>
<a name="ln1001">  return Status::OK();</a>
<a name="ln1002">}</a>
<a name="ln1003"> </a>
<a name="ln1004">Status RaftConsensus::TEST_Replicate(const ConsensusRoundPtr&amp; round) {</a>
<a name="ln1005">  ConsensusRounds rounds = { round };</a>
<a name="ln1006">  return ReplicateBatch(&amp;rounds);</a>
<a name="ln1007">}</a>
<a name="ln1008"> </a>
<a name="ln1009">Status RaftConsensus::ReplicateBatch(ConsensusRounds* rounds) {</a>
<a name="ln1010">  RETURN_NOT_OK(ExecuteHook(PRE_REPLICATE));</a>
<a name="ln1011">  {</a>
<a name="ln1012">    ReplicaState::UniqueLock lock;</a>
<a name="ln1013">#ifndef NDEBUG</a>
<a name="ln1014">    for (const auto&amp; round : *rounds) {</a>
<a name="ln1015">      DCHECK(!round-&gt;replicate_msg()-&gt;has_id()) &lt;&lt; &quot;Should not have an OpId yet: &quot;</a>
<a name="ln1016">                                                &lt;&lt; round-&gt;replicate_msg()-&gt;DebugString();</a>
<a name="ln1017">    }</a>
<a name="ln1018">#endif</a>
<a name="ln1019">    RETURN_NOT_OK(state_-&gt;LockForReplicate(&amp;lock));</a>
<a name="ln1020">    auto current_term = state_-&gt;GetCurrentTermUnlocked();</a>
<a name="ln1021"> </a>
<a name="ln1022">    for (const auto&amp; round : *rounds) {</a>
<a name="ln1023">      RETURN_NOT_OK(round-&gt;CheckBoundTerm(current_term));</a>
<a name="ln1024">    }</a>
<a name="ln1025">    RETURN_NOT_OK(AppendNewRoundsToQueueUnlocked(*rounds));</a>
<a name="ln1026">  }</a>
<a name="ln1027"> </a>
<a name="ln1028">  peer_manager_-&gt;SignalRequest(RequestTriggerMode::kNonEmptyOnly);</a>
<a name="ln1029">  RETURN_NOT_OK(ExecuteHook(POST_REPLICATE));</a>
<a name="ln1030">  return Status::OK();</a>
<a name="ln1031">}</a>
<a name="ln1032"> </a>
<a name="ln1033">Status RaftConsensus::AppendNewRoundToQueueUnlocked(const scoped_refptr&lt;ConsensusRound&gt;&amp; round) {</a>
<a name="ln1034">  return AppendNewRoundsToQueueUnlocked({ round });</a>
<a name="ln1035">}</a>
<a name="ln1036"> </a>
<a name="ln1037">Status RaftConsensus::AppendNewRoundsToQueueUnlocked(</a>
<a name="ln1038">    const std::vector&lt;scoped_refptr&lt;ConsensusRound&gt;&gt;&amp; rounds) {</a>
<a name="ln1039"> </a>
<a name="ln1040">  std::vector&lt;ReplicateMsgPtr&gt; replicate_msgs;</a>
<a name="ln1041">  replicate_msgs.reserve(rounds.size());</a>
<a name="ln1042">  const yb::OpId&amp; committed_op_id = state_-&gt;GetCommittedOpIdUnlocked();</a>
<a name="ln1043"> </a>
<a name="ln1044">  for (auto iter = rounds.begin(); iter != rounds.end(); ++iter) {</a>
<a name="ln1045">    const ConsensusRoundPtr&amp; round = *iter;</a>
<a name="ln1046"> </a>
<a name="ln1047">    yb::OpId op_id = state_-&gt;NewIdUnlocked();</a>
<a name="ln1048"> </a>
<a name="ln1049">    // We use this callback to transform write operations by substituting the hybrid_time into</a>
<a name="ln1050">    // the write batch inside the write operation.</a>
<a name="ln1051">    //</a>
<a name="ln1052">    // TODO: we could allocate multiple HybridTimes in batch, only reading system clock once.</a>
<a name="ln1053">    auto* const append_cb = round-&gt;append_callback();</a>
<a name="ln1054">    if (append_cb) {</a>
<a name="ln1055">      append_cb-&gt;HandleConsensusAppend(op_id, committed_op_id);</a>
<a name="ln1056">    } else {</a>
<a name="ln1057">      // No op operation</a>
<a name="ln1058">      op_id.ToPB(round-&gt;replicate_msg()-&gt;mutable_id());</a>
<a name="ln1059">      committed_op_id.ToPB(round-&gt;replicate_msg()-&gt;mutable_committed_op_id());</a>
<a name="ln1060">    }</a>
<a name="ln1061"> </a>
<a name="ln1062">    Status s = state_-&gt;AddPendingOperation(round);</a>
<a name="ln1063">    if (!s.ok()) {</a>
<a name="ln1064">      RollbackIdAndDeleteOpId(round-&gt;replicate_msg(), false /* should_exists */);</a>
<a name="ln1065">      // If it was duplicate request, cancel only it.</a>
<a name="ln1066">      if (s.IsAlreadyPresent()) {</a>
<a name="ln1067">        continue;</a>
<a name="ln1068">      }</a>
<a name="ln1069"> </a>
<a name="ln1070">      // Iterate rounds in the reverse order and release ids.</a>
<a name="ln1071">      while (!replicate_msgs.empty()) {</a>
<a name="ln1072">        RollbackIdAndDeleteOpId(replicate_msgs.back(), true /* should_exists */);</a>
<a name="ln1073">        replicate_msgs.pop_back();</a>
<a name="ln1074">      }</a>
<a name="ln1075">      return s;</a>
<a name="ln1076">    }</a>
<a name="ln1077"> </a>
<a name="ln1078">    replicate_msgs.push_back(round-&gt;replicate_msg());</a>
<a name="ln1079">  }</a>
<a name="ln1080"> </a>
<a name="ln1081">  if (replicate_msgs.empty()) {</a>
<a name="ln1082">    return Status::OK();</a>
<a name="ln1083">  }</a>
<a name="ln1084"> </a>
<a name="ln1085">  Status s = queue_-&gt;AppendOperations(</a>
<a name="ln1086">      replicate_msgs, state_-&gt;GetCommittedOpIdUnlocked(), state_-&gt;Clock().Now());</a>
<a name="ln1087"> </a>
<a name="ln1088">  // Handle Status::ServiceUnavailable(), which means the queue is full.</a>
<a name="ln1089">  // TODO: what are we doing about other errors here? Should we also release OpIds in those cases?</a>
<a name="ln1090">  if (PREDICT_FALSE(s.IsServiceUnavailable())) {</a>
<a name="ln1091">    for (auto iter = replicate_msgs.rbegin(); iter != replicate_msgs.rend(); ++iter) {</a>
<a name="ln1092">      RollbackIdAndDeleteOpId(*iter, true /* should_exists */);</a>
<a name="ln1093">      LOG_WITH_PREFIX(WARNING) &lt;&lt; &quot;: Could not append replicate request &quot;</a>
<a name="ln1094">                   &lt;&lt; &quot;to the queue. Queue is Full. &quot;</a>
<a name="ln1095">                   &lt;&lt; &quot;Queue metrics: &quot; &lt;&lt; queue_-&gt;ToString();</a>
<a name="ln1096"> </a>
<a name="ln1097">      // TODO Possibly evict a dangling peer from the configuration here.</a>
<a name="ln1098">      // TODO count of number of ops failed due to consensus queue overflow.</a>
<a name="ln1099">    }</a>
<a name="ln1100">  }</a>
<a name="ln1101"> </a>
<a name="ln1102">  RETURN_NOT_OK_PREPEND(s, &quot;Unable to append operations to consensus queue&quot;);</a>
<a name="ln1103">  state_-&gt;UpdateLastReceivedOpIdUnlocked(replicate_msgs.back()-&gt;id());</a>
<a name="ln1104">  return Status::OK();</a>
<a name="ln1105">}</a>
<a name="ln1106"> </a>
<a name="ln1107">void RaftConsensus::MajorityReplicatedNumSSTFilesChanged(</a>
<a name="ln1108">    uint64_t majority_replicated_num_sst_files) {</a>
<a name="ln1109">  majority_num_sst_files_.store(majority_replicated_num_sst_files, std::memory_order_release);</a>
<a name="ln1110">}</a>
<a name="ln1111"> </a>
<a name="ln1112">void RaftConsensus::UpdateMajorityReplicated(</a>
<a name="ln1113">    const MajorityReplicatedData&amp; majority_replicated_data, OpIdPB* committed_op_id) {</a>
<a name="ln1114">  TEST_PAUSE_IF_FLAG(TEST_pause_update_majority_replicated);</a>
<a name="ln1115">  ReplicaState::UniqueLock lock;</a>
<a name="ln1116">  Status s = state_-&gt;LockForMajorityReplicatedIndexUpdate(&amp;lock);</a>
<a name="ln1117">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln1118">    LOG_WITH_PREFIX(WARNING)</a>
<a name="ln1119">        &lt;&lt; &quot;Unable to take state lock to update committed index: &quot;</a>
<a name="ln1120">        &lt;&lt; s.ToString();</a>
<a name="ln1121">    return;</a>
<a name="ln1122">  }</a>
<a name="ln1123"> </a>
<a name="ln1124">  EnumBitSet&lt;SetMajorityReplicatedLeaseExpirationFlag&gt; flags;</a>
<a name="ln1125">  if (GetAtomicFlag(&amp;FLAGS_enable_lease_revocation)) {</a>
<a name="ln1126">    if (!state_-&gt;old_leader_lease().holder_uuid.empty() &amp;&amp;</a>
<a name="ln1127">        queue_-&gt;PeerAcceptedOurLease(state_-&gt;old_leader_lease().holder_uuid)) {</a>
<a name="ln1128">      flags.Set(SetMajorityReplicatedLeaseExpirationFlag::kResetOldLeaderLease);</a>
<a name="ln1129">    }</a>
<a name="ln1130"> </a>
<a name="ln1131">    if (!state_-&gt;old_leader_ht_lease().holder_uuid.empty() &amp;&amp;</a>
<a name="ln1132">        queue_-&gt;PeerAcceptedOurLease(state_-&gt;old_leader_ht_lease().holder_uuid)) {</a>
<a name="ln1133">      flags.Set(SetMajorityReplicatedLeaseExpirationFlag::kResetOldLeaderHtLease);</a>
<a name="ln1134">    }</a>
<a name="ln1135">  }</a>
<a name="ln1136"> </a>
<a name="ln1137">  state_-&gt;SetMajorityReplicatedLeaseExpirationUnlocked(majority_replicated_data, flags);</a>
<a name="ln1138">  leader_lease_wait_cond_.notify_all();</a>
<a name="ln1139"> </a>
<a name="ln1140">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Marking majority replicated up to &quot;</a>
<a name="ln1141">      &lt;&lt; majority_replicated_data.ToString();</a>
<a name="ln1142">  TRACE(&quot;Marking majority replicated up to $0&quot;, majority_replicated_data.op_id.ShortDebugString());</a>
<a name="ln1143">  bool committed_index_changed = false;</a>
<a name="ln1144">  s = state_-&gt;UpdateMajorityReplicatedUnlocked(</a>
<a name="ln1145">      majority_replicated_data.op_id, committed_op_id, &amp;committed_index_changed);</a>
<a name="ln1146">  auto leader_state = state_-&gt;GetLeaderStateUnlocked();</a>
<a name="ln1147">  if (leader_state.ok() &amp;&amp; leader_state.status == LeaderStatus::LEADER_AND_READY) {</a>
<a name="ln1148">    state_-&gt;context()-&gt;MajorityReplicated();</a>
<a name="ln1149">  }</a>
<a name="ln1150">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln1151">    string msg = Substitute(&quot;Unable to mark committed up to $0: $1&quot;,</a>
<a name="ln1152">                            majority_replicated_data.op_id.ShortDebugString(),</a>
<a name="ln1153">                            s.ToString());</a>
<a name="ln1154">    TRACE(msg);</a>
<a name="ln1155">    LOG_WITH_PREFIX(WARNING) &lt;&lt; msg;</a>
<a name="ln1156">    return;</a>
<a name="ln1157">  }</a>
<a name="ln1158"> </a>
<a name="ln1159">  majority_num_sst_files_.store(majority_replicated_data.num_sst_files, std::memory_order_release);</a>
<a name="ln1160"> </a>
<a name="ln1161">  if (committed_index_changed &amp;&amp;</a>
<a name="ln1162">      state_-&gt;GetActiveRoleUnlocked() == RaftPeerPB::LEADER) {</a>
<a name="ln1163">    // If all operations were just committed, and we don't have pending operations, then</a>
<a name="ln1164">    // we write an empty batch that contains committed index.</a>
<a name="ln1165">    // This affects only our local log, because followers have different logic in this scenario.</a>
<a name="ln1166">    if (yb::OpId::FromPB(*committed_op_id) == state_-&gt;GetLastReceivedOpIdUnlocked()) {</a>
<a name="ln1167">      auto status = queue_-&gt;AppendOperations(</a>
<a name="ln1168">          {}, yb::OpId::FromPB(*committed_op_id), state_-&gt;Clock().Now());</a>
<a name="ln1169">      LOG_IF_WITH_PREFIX(DFATAL, !status.ok() &amp;&amp; !status.IsServiceUnavailable())</a>
<a name="ln1170">          &lt;&lt; &quot;Failed to append empty batch: &quot; &lt;&lt; status;</a>
<a name="ln1171">    }</a>
<a name="ln1172"> </a>
<a name="ln1173">    lock.unlock();</a>
<a name="ln1174">    // No need to hold the lock while calling SignalRequest.</a>
<a name="ln1175">    peer_manager_-&gt;SignalRequest(RequestTriggerMode::kNonEmptyOnly);</a>
<a name="ln1176">  }</a>
<a name="ln1177">}</a>
<a name="ln1178"> </a>
<a name="ln1179">void RaftConsensus::AppendEmptyBatchToLeaderLog() {</a>
<a name="ln1180">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln1181">  auto committed_op_id = state_-&gt;GetCommittedOpIdUnlocked();</a>
<a name="ln1182">  if (committed_op_id == state_-&gt;GetLastReceivedOpIdUnlocked()) {</a>
<a name="ln1183">    auto status = queue_-&gt;AppendOperations({}, committed_op_id, state_-&gt;Clock().Now());</a>
<a name="ln1184">    LOG_IF_WITH_PREFIX(DFATAL, !status.ok()) &lt;&lt; &quot;Failed to append empty batch: &quot; &lt;&lt; status;</a>
<a name="ln1185">  }</a>
<a name="ln1186">}</a>
<a name="ln1187"> </a>
<a name="ln1188">void RaftConsensus::NotifyTermChange(int64_t term) {</a>
<a name="ln1189">  ReplicaState::UniqueLock lock;</a>
<a name="ln1190">  Status s = state_-&gt;LockForConfigChange(&amp;lock);</a>
<a name="ln1191">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln1192">    LOG_WITH_PREFIX(WARNING) &lt;&lt; &quot;Unable to lock ReplicaState for config change&quot;</a>
<a name="ln1193">                             &lt;&lt; &quot; when notified of new term &quot; &lt;&lt; term &lt;&lt; &quot;: &quot; &lt;&lt; s;</a>
<a name="ln1194">    return;</a>
<a name="ln1195">  }</a>
<a name="ln1196">  WARN_NOT_OK(HandleTermAdvanceUnlocked(term), &quot;Couldn't advance consensus term.&quot;);</a>
<a name="ln1197">}</a>
<a name="ln1198"> </a>
<a name="ln1199">void RaftConsensus::NotifyFailedFollower(const string&amp; uuid,</a>
<a name="ln1200">                                         int64_t term,</a>
<a name="ln1201">                                         const std::string&amp; reason) {</a>
<a name="ln1202">  // Common info used in all of the log messages within this method.</a>
<a name="ln1203">  string fail_msg = Substitute(&quot;Processing failure of peer $0 in term $1 ($2): &quot;,</a>
<a name="ln1204">                               uuid, term, reason);</a>
<a name="ln1205"> </a>
<a name="ln1206">  if (!FLAGS_evict_failed_followers) {</a>
<a name="ln1207">    LOG_WITH_PREFIX(INFO) &lt;&lt; fail_msg &lt;&lt; &quot;Eviction of failed followers is disabled. Doing nothing.&quot;;</a>
<a name="ln1208">    return;</a>
<a name="ln1209">  }</a>
<a name="ln1210"> </a>
<a name="ln1211">  RaftConfigPB committed_config;</a>
<a name="ln1212">  {</a>
<a name="ln1213">    auto lock = state_-&gt;LockForRead();</a>
<a name="ln1214"> </a>
<a name="ln1215">    int64_t current_term = state_-&gt;GetCurrentTermUnlocked();</a>
<a name="ln1216">    if (current_term != term) {</a>
<a name="ln1217">      LOG_WITH_PREFIX(INFO) &lt;&lt; fail_msg &lt;&lt; &quot;Notified about a follower failure in &quot;</a>
<a name="ln1218">                            &lt;&lt; &quot;previous term &quot; &lt;&lt; term &lt;&lt; &quot;, but a leader election &quot;</a>
<a name="ln1219">                            &lt;&lt; &quot;likely occurred since the failure was detected. &quot;</a>
<a name="ln1220">                            &lt;&lt; &quot;Doing nothing.&quot;;</a>
<a name="ln1221">      return;</a>
<a name="ln1222">    }</a>
<a name="ln1223"> </a>
<a name="ln1224">    if (state_-&gt;IsConfigChangePendingUnlocked()) {</a>
<a name="ln1225">      LOG_WITH_PREFIX(INFO) &lt;&lt; fail_msg &lt;&lt; &quot;There is already a config change operation &quot;</a>
<a name="ln1226">                            &lt;&lt; &quot;in progress. Unable to evict follower until it completes. &quot;</a>
<a name="ln1227">                            &lt;&lt; &quot;Doing nothing.&quot;;</a>
<a name="ln1228">      return;</a>
<a name="ln1229">    }</a>
<a name="ln1230">    committed_config = state_-&gt;GetCommittedConfigUnlocked();</a>
<a name="ln1231">  }</a>
<a name="ln1232"> </a>
<a name="ln1233">  // Run config change on thread pool after dropping ReplicaState lock.</a>
<a name="ln1234">  WARN_NOT_OK(raft_pool_token_-&gt;SubmitFunc(std::bind(&amp;RaftConsensus::TryRemoveFollowerTask,</a>
<a name="ln1235">                                               shared_from_this(), uuid, committed_config, reason)),</a>
<a name="ln1236">              state_-&gt;LogPrefix() + &quot;Unable to start RemoteFollowerTask&quot;);</a>
<a name="ln1237">}</a>
<a name="ln1238"> </a>
<a name="ln1239">void RaftConsensus::TryRemoveFollowerTask(const string&amp; uuid,</a>
<a name="ln1240">                                          const RaftConfigPB&amp; committed_config,</a>
<a name="ln1241">                                          const std::string&amp; reason) {</a>
<a name="ln1242">  ChangeConfigRequestPB req;</a>
<a name="ln1243">  req.set_tablet_id(tablet_id());</a>
<a name="ln1244">  req.mutable_server()-&gt;set_permanent_uuid(uuid);</a>
<a name="ln1245">  req.set_type(REMOVE_SERVER);</a>
<a name="ln1246">  req.set_cas_config_opid_index(committed_config.opid_index());</a>
<a name="ln1247">  LOG_WITH_PREFIX(INFO)</a>
<a name="ln1248">      &lt;&lt; &quot;Attempting to remove follower &quot; &lt;&lt; uuid &lt;&lt; &quot; from the Raft config at commit index &quot;</a>
<a name="ln1249">      &lt;&lt; committed_config.opid_index() &lt;&lt; &quot;. Reason: &quot; &lt;&lt; reason;</a>
<a name="ln1250">  boost::optional&lt;TabletServerErrorPB::Code&gt; error_code;</a>
<a name="ln1251">  WARN_NOT_OK(ChangeConfig(req, &amp;DoNothingStatusCB, &amp;error_code),</a>
<a name="ln1252">              state_-&gt;LogPrefix() + &quot;Unable to remove follower &quot; + uuid);</a>
<a name="ln1253">}</a>
<a name="ln1254"> </a>
<a name="ln1255">Status RaftConsensus::Update(ConsensusRequestPB* request,</a>
<a name="ln1256">                             ConsensusResponsePB* response,</a>
<a name="ln1257">                             CoarseTimePoint deadline) {</a>
<a name="ln1258">  if (PREDICT_FALSE(FLAGS_TEST_follower_reject_update_consensus_requests)) {</a>
<a name="ln1259">    return STATUS(IllegalState, &quot;Rejected: --TEST_follower_reject_update_consensus_requests &quot;</a>
<a name="ln1260">                                &quot;is set to true.&quot;);</a>
<a name="ln1261">  }</a>
<a name="ln1262"> </a>
<a name="ln1263">  auto reject_mode = reject_mode_.load(std::memory_order_acquire);</a>
<a name="ln1264">  if (reject_mode != RejectMode::kNone) {</a>
<a name="ln1265">    if (reject_mode == RejectMode::kAll ||</a>
<a name="ln1266">        (reject_mode == RejectMode::kNonEmpty &amp;&amp; !request-&gt;ops().empty())) {</a>
<a name="ln1267">      auto result = STATUS_FORMAT(IllegalState, &quot;Rejected because of reject mode: $0&quot;,</a>
<a name="ln1268">                                  ToString(reject_mode));</a>
<a name="ln1269">      LOG_WITH_PREFIX(INFO) &lt;&lt; result;</a>
<a name="ln1270">      return result;</a>
<a name="ln1271">    }</a>
<a name="ln1272">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Accepted: &quot; &lt;&lt; request-&gt;ShortDebugString();</a>
<a name="ln1273">  }</a>
<a name="ln1274"> </a>
<a name="ln1275">  if (PREDICT_FALSE(FLAGS_TEST_follower_reject_update_consensus_requests_seconds &gt; 0)) {</a>
<a name="ln1276">    if (MonoTime::Now() &lt; withold_replica_updates_until_) {</a>
<a name="ln1277">      LOG(INFO) &lt;&lt; &quot;Rejecting Update for tablet: &quot; &lt;&lt; tablet_id()</a>
<a name="ln1278">                &lt;&lt; &quot; tserver uuid: &quot; &lt;&lt; peer_uuid();</a>
<a name="ln1279">      return STATUS_SUBSTITUTE(IllegalState,</a>
<a name="ln1280">          &quot;Rejected: --TEST_follower_reject_update_consensus_requests_seconds is set to $0&quot;,</a>
<a name="ln1281">          FLAGS_TEST_follower_reject_update_consensus_requests_seconds);</a>
<a name="ln1282">    }</a>
<a name="ln1283">  }</a>
<a name="ln1284"> </a>
<a name="ln1285">  RETURN_NOT_OK(ExecuteHook(PRE_UPDATE));</a>
<a name="ln1286">  response-&gt;set_responder_uuid(state_-&gt;GetPeerUuid());</a>
<a name="ln1287"> </a>
<a name="ln1288">  VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Replica received request: &quot; &lt;&lt; request-&gt;ShortDebugString();</a>
<a name="ln1289"> </a>
<a name="ln1290">  UpdateReplicaResult result;</a>
<a name="ln1291">  {</a>
<a name="ln1292">    // see var declaration</a>
<a name="ln1293">    auto wait_start = CoarseMonoClock::now();</a>
<a name="ln1294">    auto wait_duration = deadline != CoarseTimePoint::max() ? deadline - wait_start</a>
<a name="ln1295">                                                            : CoarseDuration::max();</a>
<a name="ln1296">    auto lock = LockMutex(&amp;update_mutex_, wait_duration);</a>
<a name="ln1297">    if (!lock.owns_lock()) {</a>
<a name="ln1298">      return STATUS_FORMAT(TimedOut, &quot;Unable to lock update mutex for $0&quot;, wait_duration);</a>
<a name="ln1299">    }</a>
<a name="ln1300"> </a>
<a name="ln1301">    LongOperationTracker operation_tracker(&quot;UpdateReplica&quot;, 1s);</a>
<a name="ln1302">    result = VERIFY_RESULT(UpdateReplica(request, response));</a>
<a name="ln1303"> </a>
<a name="ln1304">    auto delay = TEST_delay_update_.load(std::memory_order_acquire);</a>
<a name="ln1305">    if (delay != MonoDelta::kZero) {</a>
<a name="ln1306">      std::this_thread::sleep_for(delay.ToSteadyDuration());</a>
<a name="ln1307">    }</a>
<a name="ln1308">  }</a>
<a name="ln1309"> </a>
<a name="ln1310">  // Release the lock while we wait for the log append to finish so that commits can go through.</a>
<a name="ln1311">  if (result.wait_for_op_id) {</a>
<a name="ln1312">    RETURN_NOT_OK(WaitForWrites(result.wait_for_op_id));</a>
<a name="ln1313">  }</a>
<a name="ln1314"> </a>
<a name="ln1315">  if (PREDICT_FALSE(VLOG_IS_ON(2))) {</a>
<a name="ln1316">    VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Replica updated. &quot;</a>
<a name="ln1317">        &lt;&lt; state_-&gt;ToString() &lt;&lt; &quot; Request: &quot; &lt;&lt; request-&gt;ShortDebugString();</a>
<a name="ln1318">  }</a>
<a name="ln1319"> </a>
<a name="ln1320">  // If an election pending on a specific op id and it has just been committed, start it now.</a>
<a name="ln1321">  // StartElection will ensure the pending election will be started just once only even if</a>
<a name="ln1322">  // UpdateReplica happens in multiple threads in parallel.</a>
<a name="ln1323">  if (result.start_election) {</a>
<a name="ln1324">    RETURN_NOT_OK(StartElection(</a>
<a name="ln1325">        {consensus::ElectionMode::ELECT_EVEN_IF_LEADER_IS_ALIVE, true /* pending_commit */}));</a>
<a name="ln1326">  }</a>
<a name="ln1327"> </a>
<a name="ln1328">  RETURN_NOT_OK(ExecuteHook(POST_UPDATE));</a>
<a name="ln1329">  return Status::OK();</a>
<a name="ln1330">}</a>
<a name="ln1331"> </a>
<a name="ln1332">// Helper function to check if the op is a non-Operation op.</a>
<a name="ln1333">static bool IsConsensusOnlyOperation(OperationType op_type) {</a>
<a name="ln1334">  return op_type == NO_OP || op_type == CHANGE_CONFIG_OP;</a>
<a name="ln1335">}</a>
<a name="ln1336"> </a>
<a name="ln1337">// Helper to check if the op is Change Config op.</a>
<a name="ln1338">static bool IsChangeConfigOperation(OperationType op_type) {</a>
<a name="ln1339">  return op_type == CHANGE_CONFIG_OP;</a>
<a name="ln1340">}</a>
<a name="ln1341"> </a>
<a name="ln1342">Status RaftConsensus::StartReplicaOperationUnlocked(</a>
<a name="ln1343">    const ReplicateMsgPtr&amp; msg, HybridTime propagated_safe_time) {</a>
<a name="ln1344">  if (IsConsensusOnlyOperation(msg-&gt;op_type())) {</a>
<a name="ln1345">    return StartConsensusOnlyRoundUnlocked(msg);</a>
<a name="ln1346">  }</a>
<a name="ln1347"> </a>
<a name="ln1348">  if (PREDICT_FALSE(FLAGS_TEST_follower_fail_all_prepare)) {</a>
<a name="ln1349">    return STATUS(IllegalState, &quot;Rejected: --TEST_follower_fail_all_prepare &quot;</a>
<a name="ln1350">                                &quot;is set to true.&quot;);</a>
<a name="ln1351">  }</a>
<a name="ln1352"> </a>
<a name="ln1353">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Starting operation: &quot; &lt;&lt; msg-&gt;id().ShortDebugString();</a>
<a name="ln1354">  scoped_refptr&lt;ConsensusRound&gt; round(new ConsensusRound(this, msg));</a>
<a name="ln1355">  ConsensusRound* round_ptr = round.get();</a>
<a name="ln1356">  RETURN_NOT_OK(state_-&gt;context()-&gt;StartReplicaOperation(round, propagated_safe_time));</a>
<a name="ln1357">  return state_-&gt;AddPendingOperation(round_ptr);</a>
<a name="ln1358">}</a>
<a name="ln1359"> </a>
<a name="ln1360">std::string RaftConsensus::LeaderRequest::OpsRangeString() const {</a>
<a name="ln1361">  std::string ret;</a>
<a name="ln1362">  ret.reserve(100);</a>
<a name="ln1363">  ret.push_back('[');</a>
<a name="ln1364">  if (!messages.empty()) {</a>
<a name="ln1365">    const OpIdPB&amp; first_op = (*messages.begin())-&gt;id();</a>
<a name="ln1366">    const OpIdPB&amp; last_op = (*messages.rbegin())-&gt;id();</a>
<a name="ln1367">    strings::SubstituteAndAppend(&amp;ret, &quot;$0.$1-$2.$3&quot;,</a>
<a name="ln1368">                                 first_op.term(), first_op.index(),</a>
<a name="ln1369">                                 last_op.term(), last_op.index());</a>
<a name="ln1370">  }</a>
<a name="ln1371">  ret.push_back(']');</a>
<a name="ln1372">  return ret;</a>
<a name="ln1373">}</a>
<a name="ln1374"> </a>
<a name="ln1375">Status RaftConsensus::DeduplicateLeaderRequestUnlocked(ConsensusRequestPB* rpc_req,</a>
<a name="ln1376">                                                       LeaderRequest* deduplicated_req) {</a>
<a name="ln1377">  const auto&amp; last_committed = state_-&gt;GetCommittedOpIdUnlocked();</a>
<a name="ln1378"> </a>
<a name="ln1379">  // The leader's preceding id.</a>
<a name="ln1380">  deduplicated_req-&gt;preceding_op_id = yb::OpId::FromPB(rpc_req-&gt;preceding_id());</a>
<a name="ln1381"> </a>
<a name="ln1382">  int64_t dedup_up_to_index = state_-&gt;GetLastReceivedOpIdUnlocked().index;</a>
<a name="ln1383"> </a>
<a name="ln1384">  deduplicated_req-&gt;first_message_idx = -1;</a>
<a name="ln1385"> </a>
<a name="ln1386">  // In this loop we discard duplicates and advance the leader's preceding id</a>
<a name="ln1387">  // accordingly.</a>
<a name="ln1388">  for (int i = 0; i &lt; rpc_req-&gt;ops_size(); i++) {</a>
<a name="ln1389">    ReplicateMsg* leader_msg = rpc_req-&gt;mutable_ops(i);</a>
<a name="ln1390"> </a>
<a name="ln1391">    if (leader_msg-&gt;id().index() &lt;= last_committed.index) {</a>
<a name="ln1392">      VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Skipping op id &quot; &lt;&lt; leader_msg-&gt;id()</a>
<a name="ln1393">                          &lt;&lt; &quot; (already committed)&quot;;</a>
<a name="ln1394">      deduplicated_req-&gt;preceding_op_id = yb::OpId::FromPB(leader_msg-&gt;id());</a>
<a name="ln1395">      continue;</a>
<a name="ln1396">    }</a>
<a name="ln1397"> </a>
<a name="ln1398">    if (leader_msg-&gt;id().index() &lt;= dedup_up_to_index) {</a>
<a name="ln1399">      // If the index is uncommitted and below our match index, then it must be in the</a>
<a name="ln1400">      // pendings set.</a>
<a name="ln1401">      scoped_refptr&lt;ConsensusRound&gt; round =</a>
<a name="ln1402">          state_-&gt;GetPendingOpByIndexOrNullUnlocked(leader_msg-&gt;id().index());</a>
<a name="ln1403">      if (!round) {</a>
<a name="ln1404">        // Could happen if we received outdated leader request. So should just reject it.</a>
<a name="ln1405">        return STATUS_FORMAT(IllegalState, &quot;Round not found for index: $0&quot;,</a>
<a name="ln1406">                             leader_msg-&gt;id().index());</a>
<a name="ln1407">      }</a>
<a name="ln1408"> </a>
<a name="ln1409">      // If the OpIds match, i.e. if they have the same term and id, then this is just</a>
<a name="ln1410">      // duplicate, we skip...</a>
<a name="ln1411">      if (OpIdEquals(round-&gt;replicate_msg()-&gt;id(), leader_msg-&gt;id())) {</a>
<a name="ln1412">        VLOG_WITH_PREFIX(2) &lt;&lt; &quot;Skipping op id &quot; &lt;&lt; leader_msg-&gt;id()</a>
<a name="ln1413">                            &lt;&lt; &quot; (already replicated)&quot;;</a>
<a name="ln1414">        deduplicated_req-&gt;preceding_op_id = yb::OpId::FromPB(leader_msg-&gt;id());</a>
<a name="ln1415">        continue;</a>
<a name="ln1416">      }</a>
<a name="ln1417"> </a>
<a name="ln1418">      // ... otherwise we must adjust our match index, i.e. all messages from now on</a>
<a name="ln1419">      // are &quot;new&quot;</a>
<a name="ln1420">      dedup_up_to_index = leader_msg-&gt;id().index();</a>
<a name="ln1421">    }</a>
<a name="ln1422"> </a>
<a name="ln1423">    if (deduplicated_req-&gt;first_message_idx == -1) {</a>
<a name="ln1424">      deduplicated_req-&gt;first_message_idx = i;</a>
<a name="ln1425">    }</a>
<a name="ln1426">    deduplicated_req-&gt;messages.emplace_back(leader_msg);</a>
<a name="ln1427">  }</a>
<a name="ln1428"> </a>
<a name="ln1429">  if (deduplicated_req-&gt;messages.size() != rpc_req-&gt;ops_size()) {</a>
<a name="ln1430">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Deduplicated request from leader. Original: &quot;</a>
<a name="ln1431">                          &lt;&lt; rpc_req-&gt;preceding_id() &lt;&lt; &quot;-&gt;&quot; &lt;&lt; OpsRangeString(*rpc_req)</a>
<a name="ln1432">                          &lt;&lt; &quot;   Dedup: &quot; &lt;&lt; deduplicated_req-&gt;preceding_op_id &lt;&lt; &quot;-&gt;&quot;</a>
<a name="ln1433">                          &lt;&lt; deduplicated_req-&gt;OpsRangeString();</a>
<a name="ln1434">  }</a>
<a name="ln1435"> </a>
<a name="ln1436">  return Status::OK();</a>
<a name="ln1437">}</a>
<a name="ln1438"> </a>
<a name="ln1439">Status RaftConsensus::HandleLeaderRequestTermUnlocked(const ConsensusRequestPB* request,</a>
<a name="ln1440">                                                      ConsensusResponsePB* response) {</a>
<a name="ln1441">  // Do term checks first:</a>
<a name="ln1442">  if (PREDICT_FALSE(request-&gt;caller_term() != state_-&gt;GetCurrentTermUnlocked())) {</a>
<a name="ln1443"> </a>
<a name="ln1444">    // If less, reject.</a>
<a name="ln1445">    if (request-&gt;caller_term() &lt; state_-&gt;GetCurrentTermUnlocked()) {</a>
<a name="ln1446">      string msg = Substitute(&quot;Rejecting Update request from peer $0 for earlier term $1. &quot;</a>
<a name="ln1447">                              &quot;Current term is $2. Ops: $3&quot;,</a>
<a name="ln1448"> </a>
<a name="ln1449">                              request-&gt;caller_uuid(),</a>
<a name="ln1450">                              request-&gt;caller_term(),</a>
<a name="ln1451">                              state_-&gt;GetCurrentTermUnlocked(),</a>
<a name="ln1452">                              OpsRangeString(*request));</a>
<a name="ln1453">      LOG_WITH_PREFIX(INFO) &lt;&lt; msg;</a>
<a name="ln1454">      FillConsensusResponseError(response,</a>
<a name="ln1455">                                 ConsensusErrorPB::INVALID_TERM,</a>
<a name="ln1456">                                 STATUS(IllegalState, msg));</a>
<a name="ln1457">      return Status::OK();</a>
<a name="ln1458">    } else {</a>
<a name="ln1459">      RETURN_NOT_OK(HandleTermAdvanceUnlocked(request-&gt;caller_term()));</a>
<a name="ln1460">    }</a>
<a name="ln1461">  }</a>
<a name="ln1462">  return Status::OK();</a>
<a name="ln1463">}</a>
<a name="ln1464"> </a>
<a name="ln1465">Status RaftConsensus::EnforceLogMatchingPropertyMatchesUnlocked(const LeaderRequest&amp; req,</a>
<a name="ln1466">                                                                ConsensusResponsePB* response) {</a>
<a name="ln1467"> </a>
<a name="ln1468">  bool term_mismatch;</a>
<a name="ln1469">  if (state_-&gt;IsOpCommittedOrPending(req.preceding_op_id, &amp;term_mismatch)) {</a>
<a name="ln1470">    return Status::OK();</a>
<a name="ln1471">  }</a>
<a name="ln1472"> </a>
<a name="ln1473">  string error_msg = Format(</a>
<a name="ln1474">    &quot;Log matching property violated.&quot;</a>
<a name="ln1475">    &quot; Preceding OpId in replica: $0. Preceding OpId from leader: $1. ($2 mismatch)&quot;,</a>
<a name="ln1476">    state_-&gt;GetLastReceivedOpIdUnlocked(), req.preceding_op_id, term_mismatch ? &quot;term&quot; : &quot;index&quot;);</a>
<a name="ln1477"> </a>
<a name="ln1478">  FillConsensusResponseError(response,</a>
<a name="ln1479">                             ConsensusErrorPB::PRECEDING_ENTRY_DIDNT_MATCH,</a>
<a name="ln1480">                             STATUS(IllegalState, error_msg));</a>
<a name="ln1481"> </a>
<a name="ln1482">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Refusing update from remote peer &quot;</a>
<a name="ln1483">                        &lt;&lt; req.leader_uuid &lt;&lt; &quot;: &quot; &lt;&lt; error_msg;</a>
<a name="ln1484"> </a>
<a name="ln1485">  // If the terms mismatch we abort down to the index before the leader's preceding,</a>
<a name="ln1486">  // since we know that is the last opid that has a chance of not being overwritten.</a>
<a name="ln1487">  // Aborting preemptively here avoids us reporting a last received index that is</a>
<a name="ln1488">  // possibly higher than the leader's causing an avoidable cache miss on the leader's</a>
<a name="ln1489">  // queue.</a>
<a name="ln1490">  //</a>
<a name="ln1491">  // TODO: this isn't just an optimization! if we comment this out, we get</a>
<a name="ln1492">  // failures on raft_consensus-itest a couple percent of the time! Should investigate</a>
<a name="ln1493">  // why this is actually critical to do here, as opposed to just on requests that</a>
<a name="ln1494">  // append some ops.</a>
<a name="ln1495">  if (term_mismatch) {</a>
<a name="ln1496">    return state_-&gt;AbortOpsAfterUnlocked(req.preceding_op_id.index - 1);</a>
<a name="ln1497">  }</a>
<a name="ln1498"> </a>
<a name="ln1499">  return Status::OK();</a>
<a name="ln1500">}</a>
<a name="ln1501"> </a>
<a name="ln1502">Status RaftConsensus::CheckLeaderRequestOpIdSequence(</a>
<a name="ln1503">    const LeaderRequest&amp; deduped_req,</a>
<a name="ln1504">    ConsensusRequestPB* request) {</a>
<a name="ln1505">  Status sequence_check_status;</a>
<a name="ln1506">  yb::OpId prev = deduped_req.preceding_op_id;</a>
<a name="ln1507">  for (const auto&amp; message : deduped_req.messages) {</a>
<a name="ln1508">    auto current = yb::OpId::FromPB(message-&gt;id());</a>
<a name="ln1509">    sequence_check_status = ReplicaState::CheckOpInSequence(prev, current);</a>
<a name="ln1510">    if (PREDICT_FALSE(!sequence_check_status.ok())) {</a>
<a name="ln1511">      LOG(ERROR) &lt;&lt; &quot;Leader request contained out-of-sequence messages. Status: &quot;</a>
<a name="ln1512">          &lt;&lt; sequence_check_status.ToString() &lt;&lt; &quot;. Leader Request: &quot;</a>
<a name="ln1513">          &lt;&lt; request-&gt;ShortDebugString();</a>
<a name="ln1514">      break;</a>
<a name="ln1515">    }</a>
<a name="ln1516">    prev = current;</a>
<a name="ln1517">  }</a>
<a name="ln1518"> </a>
<a name="ln1519">  // We only release the messages from the request after the above check so that that we can print</a>
<a name="ln1520">  // the original request, if it fails.</a>
<a name="ln1521">  if (!deduped_req.messages.empty()) {</a>
<a name="ln1522">    // We take ownership of the deduped ops.</a>
<a name="ln1523">    DCHECK_GE(deduped_req.first_message_idx, 0);</a>
<a name="ln1524">    request-&gt;mutable_ops()-&gt;ExtractSubrange(</a>
<a name="ln1525">        deduped_req.first_message_idx,</a>
<a name="ln1526">        deduped_req.messages.size(),</a>
<a name="ln1527">        nullptr);</a>
<a name="ln1528">  }</a>
<a name="ln1529"> </a>
<a name="ln1530">  // We don't need request-&gt;ops() anymore, so could release them to avoid unnecessary memory</a>
<a name="ln1531">  // consumption.</a>
<a name="ln1532">  request-&gt;mutable_ops()-&gt;Clear();</a>
<a name="ln1533"> </a>
<a name="ln1534">  return sequence_check_status;</a>
<a name="ln1535">}</a>
<a name="ln1536"> </a>
<a name="ln1537">Status RaftConsensus::CheckLeaderRequestUnlocked(ConsensusRequestPB* request,</a>
<a name="ln1538">                                                 ConsensusResponsePB* response,</a>
<a name="ln1539">                                                 LeaderRequest* deduped_req) {</a>
<a name="ln1540">  RETURN_NOT_OK(DeduplicateLeaderRequestUnlocked(request, deduped_req));</a>
<a name="ln1541"> </a>
<a name="ln1542">  // This is an additional check for KUDU-639 that makes sure the message's index</a>
<a name="ln1543">  // and term are in the right sequence in the request, after we've deduplicated</a>
<a name="ln1544">  // them. We do this before we change any of the internal state.</a>
<a name="ln1545">  //</a>
<a name="ln1546">  // TODO move this to raft_consensus-state or whatever we transform that into.</a>
<a name="ln1547">  // We should be able to do this check for each append, but right now the way</a>
<a name="ln1548">  // we initialize raft_consensus-state is preventing us from doing so.</a>
<a name="ln1549">  RETURN_NOT_OK(CheckLeaderRequestOpIdSequence(*deduped_req, request));</a>
<a name="ln1550"> </a>
<a name="ln1551">  RETURN_NOT_OK(HandleLeaderRequestTermUnlocked(request, response));</a>
<a name="ln1552"> </a>
<a name="ln1553">  if (response-&gt;status().has_error()) {</a>
<a name="ln1554">    return Status::OK();</a>
<a name="ln1555">  }</a>
<a name="ln1556"> </a>
<a name="ln1557">  RETURN_NOT_OK(EnforceLogMatchingPropertyMatchesUnlocked(*deduped_req, response));</a>
<a name="ln1558"> </a>
<a name="ln1559">  if (response-&gt;status().has_error()) {</a>
<a name="ln1560">    return Status::OK();</a>
<a name="ln1561">  }</a>
<a name="ln1562"> </a>
<a name="ln1563">  // If the first of the messages to apply is not in our log, either it follows the last</a>
<a name="ln1564">  // received message or it replaces some in-flight.</a>
<a name="ln1565">  if (!deduped_req-&gt;messages.empty()) {</a>
<a name="ln1566">    auto first_id = yb::OpId::FromPB(deduped_req-&gt;messages[0]-&gt;id());</a>
<a name="ln1567">    bool term_mismatch;</a>
<a name="ln1568">    if (state_-&gt;IsOpCommittedOrPending(first_id, &amp;term_mismatch)) {</a>
<a name="ln1569">      return STATUS_FORMAT(IllegalState,</a>
<a name="ln1570">                           &quot;First deduped message $0 is committed or pending&quot;,</a>
<a name="ln1571">                           first_id);</a>
<a name="ln1572">    }</a>
<a name="ln1573"> </a>
<a name="ln1574">    // If the index is in our log but the terms are not the same abort down to the leader's</a>
<a name="ln1575">    // preceding id.</a>
<a name="ln1576">    if (term_mismatch) {</a>
<a name="ln1577">      RETURN_NOT_OK(state_-&gt;AbortOpsAfterUnlocked(deduped_req-&gt;preceding_op_id.index));</a>
<a name="ln1578">    }</a>
<a name="ln1579">  }</a>
<a name="ln1580"> </a>
<a name="ln1581">  // If all of the above logic was successful then we can consider this to be</a>
<a name="ln1582">  // the effective leader of the configuration. If they are not currently marked as</a>
<a name="ln1583">  // the leader locally, mark them as leader now.</a>
<a name="ln1584">  const string&amp; caller_uuid = request-&gt;caller_uuid();</a>
<a name="ln1585">  if (PREDICT_FALSE(state_-&gt;HasLeaderUnlocked() &amp;&amp;</a>
<a name="ln1586">                    state_-&gt;GetLeaderUuidUnlocked() != caller_uuid)) {</a>
<a name="ln1587">    LOG_WITH_PREFIX(FATAL)</a>
<a name="ln1588">        &lt;&lt; &quot;Unexpected new leader in same term! &quot;</a>
<a name="ln1589">        &lt;&lt; &quot;Existing leader UUID: &quot; &lt;&lt; state_-&gt;GetLeaderUuidUnlocked() &lt;&lt; &quot;, &quot;</a>
<a name="ln1590">        &lt;&lt; &quot;new leader UUID: &quot; &lt;&lt; caller_uuid;</a>
<a name="ln1591">  }</a>
<a name="ln1592">  if (PREDICT_FALSE(!state_-&gt;HasLeaderUnlocked())) {</a>
<a name="ln1593">    SetLeaderUuidUnlocked(caller_uuid);</a>
<a name="ln1594">  }</a>
<a name="ln1595"> </a>
<a name="ln1596">  return Status::OK();</a>
<a name="ln1597">}</a>
<a name="ln1598"> </a>
<a name="ln1599">Result&lt;RaftConsensus::UpdateReplicaResult&gt; RaftConsensus::UpdateReplica(</a>
<a name="ln1600">    ConsensusRequestPB* request, ConsensusResponsePB* response) {</a>
<a name="ln1601">  TRACE_EVENT2(&quot;consensus&quot;, &quot;RaftConsensus::UpdateReplica&quot;,</a>
<a name="ln1602">               &quot;peer&quot;, peer_uuid(),</a>
<a name="ln1603">               &quot;tablet&quot;, tablet_id());</a>
<a name="ln1604"> </a>
<a name="ln1605">  if (request-&gt;has_propagated_hybrid_time()) {</a>
<a name="ln1606">    clock_-&gt;Update(HybridTime(request-&gt;propagated_hybrid_time()));</a>
<a name="ln1607">  }</a>
<a name="ln1608"> </a>
<a name="ln1609">  // The ordering of the following operations is crucial, read on for details.</a>
<a name="ln1610">  //</a>
<a name="ln1611">  // The main requirements explained in more detail below are:</a>
<a name="ln1612">  //</a>
<a name="ln1613">  //   1) We must enqueue the prepares before we write to our local log.</a>
<a name="ln1614">  //   2) If we were able to enqueue a prepare then we must be able to log it.</a>
<a name="ln1615">  //   3) If we fail to enqueue a prepare, we must not attempt to enqueue any</a>
<a name="ln1616">  //      later-indexed prepare or apply.</a>
<a name="ln1617">  //</a>
<a name="ln1618">  // See below for detailed rationale.</a>
<a name="ln1619">  //</a>
<a name="ln1620">  // The steps are:</a>
<a name="ln1621">  //</a>
<a name="ln1622">  // 0 - Dedup</a>
<a name="ln1623">  //</a>
<a name="ln1624">  // We make sure that we don't do anything on Replicate operations we've already received in a</a>
<a name="ln1625">  // previous call. This essentially makes this method idempotent.</a>
<a name="ln1626">  //</a>
<a name="ln1627">  // 1 - We mark as many pending operations as committed as we can.</a>
<a name="ln1628">  //</a>
<a name="ln1629">  // We may have some pending operations that, according to the leader, are now</a>
<a name="ln1630">  // committed. We Apply them early, because:</a>
<a name="ln1631">  // - Soon (step 2) we may reject the call due to excessive memory pressure. One</a>
<a name="ln1632">  //   way to relieve the pressure is by flushing the MRS, and applying these</a>
<a name="ln1633">  //   operations may unblock an in-flight Flush().</a>
<a name="ln1634">  // - The Apply and subsequent Prepares (step 2) can take place concurrently.</a>
<a name="ln1635">  //</a>
<a name="ln1636">  // 2 - We enqueue the Prepare of the operations.</a>
<a name="ln1637">  //</a>
<a name="ln1638">  // The actual prepares are enqueued in order but happen asynchronously so we don't</a>
<a name="ln1639">  // have decoding/acquiring locks on the critical path.</a>
<a name="ln1640">  //</a>
<a name="ln1641">  // We need to do this now for a number of reasons:</a>
<a name="ln1642">  // - Prepares, by themselves, are inconsequential, i.e. they do not mutate the</a>
<a name="ln1643">  //   state machine so, were we to crash afterwards, having the prepares in-flight</a>
<a name="ln1644">  //   won't hurt.</a>
<a name="ln1645">  // - Prepares depend on factors external to consensus (the operation drivers and</a>
<a name="ln1646">  //   the tablet peer) so if for some reason they cannot be enqueued we must know</a>
<a name="ln1647">  //   before we try write them to the WAL. Once enqueued, we assume that prepare will</a>
<a name="ln1648">  //   always succeed on a replica operation (because the leader already prepared them</a>
<a name="ln1649">  //   successfully, and thus we know they are valid).</a>
<a name="ln1650">  // - The prepares corresponding to every operation that was logged must be in-flight</a>
<a name="ln1651">  //   first. This because should we need to abort certain operations (say a new leader</a>
<a name="ln1652">  //   says they are not committed) we need to have those prepares in-flight so that</a>
<a name="ln1653">  //   the operations can be continued (in the abort path).</a>
<a name="ln1654">  // - Failure to enqueue prepares is OK, we can continue and let the leader know that</a>
<a name="ln1655">  //   we only went so far. The leader will re-send the remaining messages.</a>
<a name="ln1656">  // - Prepares represent new operations, and operations consume memory. Thus, if the</a>
<a name="ln1657">  //   overall memory pressure on the server is too high, we will reject the prepares.</a>
<a name="ln1658">  //</a>
<a name="ln1659">  // 3 - We enqueue the writes to the WAL.</a>
<a name="ln1660">  //</a>
<a name="ln1661">  // We enqueue writes to the WAL, but only the operations that were successfully</a>
<a name="ln1662">  // enqueued for prepare (for the reasons introduced above). This means that even</a>
<a name="ln1663">  // if a prepare fails to enqueue, if any of the previous prepares were successfully</a>
<a name="ln1664">  // submitted they must be written to the WAL.</a>
<a name="ln1665">  // If writing to the WAL fails, we're in an inconsistent state and we crash. In this</a>
<a name="ln1666">  // case, no one will ever know of the operations we previously prepared so those are</a>
<a name="ln1667">  // inconsequential.</a>
<a name="ln1668">  //</a>
<a name="ln1669">  // 4 - We mark the operations as committed.</a>
<a name="ln1670">  //</a>
<a name="ln1671">  // For each operation which has been committed by the leader, we update the</a>
<a name="ln1672">  // operation state to reflect that. If the logging has already succeeded for that</a>
<a name="ln1673">  // operation, this will trigger the Apply phase. Otherwise, Apply will be triggered</a>
<a name="ln1674">  // when the logging completes. In both cases the Apply phase executes asynchronously.</a>
<a name="ln1675">  // This must, of course, happen after the prepares have been triggered as the same batch</a>
<a name="ln1676">  // can both replicate/prepare and commit/apply an operation.</a>
<a name="ln1677">  //</a>
<a name="ln1678">  // Currently, if a prepare failed to enqueue we still trigger all applies for operations</a>
<a name="ln1679">  // with an id lower than it (if we have them). This is important now as the leader will</a>
<a name="ln1680">  // not re-send those commit messages. This will be moot when we move to the commit</a>
<a name="ln1681">  // commitIndex way of doing things as we can simply ignore the applies as we know</a>
<a name="ln1682">  // they will be triggered with the next successful batch.</a>
<a name="ln1683">  //</a>
<a name="ln1684">  // 5 - We wait for the writes to be durable.</a>
<a name="ln1685">  //</a>
<a name="ln1686">  // Before replying to the leader we wait for the writes to be durable. We then</a>
<a name="ln1687">  // just update the last replicated watermark and respond.</a>
<a name="ln1688">  //</a>
<a name="ln1689">  // TODO - These failure scenarios need to be exercised in an unit</a>
<a name="ln1690">  //        test. Moreover we need to add more fault injection spots (well that</a>
<a name="ln1691">  //        and actually use them) for each of these steps.</a>
<a name="ln1692">  TRACE(&quot;Updating replica for $0 ops&quot;, request-&gt;ops_size());</a>
<a name="ln1693"> </a>
<a name="ln1694">  // The deduplicated request.</a>
<a name="ln1695">  LeaderRequest deduped_req;</a>
<a name="ln1696"> </a>
<a name="ln1697">  ReplicaState::UniqueLock lock;</a>
<a name="ln1698">  RETURN_NOT_OK(state_-&gt;LockForUpdate(&amp;lock));</a>
<a name="ln1699"> </a>
<a name="ln1700">  auto prev_committed_op_id = state_-&gt;GetCommittedOpIdUnlocked();</a>
<a name="ln1701"> </a>
<a name="ln1702">  deduped_req.leader_uuid = request-&gt;caller_uuid();</a>
<a name="ln1703"> </a>
<a name="ln1704">  RETURN_NOT_OK(CheckLeaderRequestUnlocked(request, response, &amp;deduped_req));</a>
<a name="ln1705"> </a>
<a name="ln1706">  if (response-&gt;status().has_error()) {</a>
<a name="ln1707">    // We had an error, like an invalid term, we still fill the response.</a>
<a name="ln1708">    FillConsensusResponseOKUnlocked(response);</a>
<a name="ln1709">    return UpdateReplicaResult();</a>
<a name="ln1710">  }</a>
<a name="ln1711"> </a>
<a name="ln1712">  TEST_PAUSE_IF_FLAG(TEST_pause_update_replica);</a>
<a name="ln1713"> </a>
<a name="ln1714">  // Snooze the failure detector as soon as we decide to accept the message.</a>
<a name="ln1715">  // We are guaranteed to be acting as a FOLLOWER at this point by the above</a>
<a name="ln1716">  // sanity check.</a>
<a name="ln1717">  SnoozeFailureDetector(DO_NOT_LOG);</a>
<a name="ln1718"> </a>
<a name="ln1719">  auto now = MonoTime::Now();</a>
<a name="ln1720"> </a>
<a name="ln1721">  // Update the expiration time of the current leader's lease, so that when this follower becomes</a>
<a name="ln1722">  // a leader, it can wait out the time interval while the old leader might still be active.</a>
<a name="ln1723">  if (request-&gt;has_leader_lease_duration_ms()) {</a>
<a name="ln1724">    state_-&gt;UpdateOldLeaderLeaseExpirationOnNonLeaderUnlocked(</a>
<a name="ln1725">        CoarseTimeLease(deduped_req.leader_uuid,</a>
<a name="ln1726">                        CoarseMonoClock::now() + request-&gt;leader_lease_duration_ms() * 1ms),</a>
<a name="ln1727">        PhysicalComponentLease(deduped_req.leader_uuid, request-&gt;ht_lease_expiration()));</a>
<a name="ln1728">  }</a>
<a name="ln1729"> </a>
<a name="ln1730">  // Also prohibit voting for anyone for the minimum election timeout.</a>
<a name="ln1731">  withhold_votes_until_.store(now + MinimumElectionTimeout(), std::memory_order_release);</a>
<a name="ln1732"> </a>
<a name="ln1733">  // 1 - Early commit pending (and committed) operations</a>
<a name="ln1734">  RETURN_NOT_OK(EarlyCommitUnlocked(*request, deduped_req));</a>
<a name="ln1735"> </a>
<a name="ln1736">  // 2 - Enqueue the prepares</a>
<a name="ln1737">  if (!VERIFY_RESULT(EnqueuePreparesUnlocked(*request, &amp;deduped_req, response))) {</a>
<a name="ln1738">    return UpdateReplicaResult();</a>
<a name="ln1739">  }</a>
<a name="ln1740"> </a>
<a name="ln1741">  // 3 - Enqueue the writes.</a>
<a name="ln1742">  auto last_from_leader = EnqueueWritesUnlocked(</a>
<a name="ln1743">      deduped_req, WriteEmpty(prev_committed_op_id != deduped_req.committed_op_id));</a>
<a name="ln1744"> </a>
<a name="ln1745">  // 4 - Mark operations as committed</a>
<a name="ln1746">  RETURN_NOT_OK(MarkOperationsAsCommittedUnlocked(*request, deduped_req, last_from_leader));</a>
<a name="ln1747"> </a>
<a name="ln1748">  // Fill the response with the current state. We will not mutate anymore state until</a>
<a name="ln1749">  // we actually reply to the leader, we'll just wait for the messages to be durable.</a>
<a name="ln1750">  FillConsensusResponseOKUnlocked(response);</a>
<a name="ln1751"> </a>
<a name="ln1752">  UpdateReplicaResult result;</a>
<a name="ln1753"> </a>
<a name="ln1754">  // Check if there is an election pending and the op id pending upon has just been committed.</a>
<a name="ln1755">  const auto&amp; pending_election_op_id = state_-&gt;GetPendingElectionOpIdUnlocked();</a>
<a name="ln1756">  result.start_election =</a>
<a name="ln1757">      pending_election_op_id.IsInitialized() &amp;&amp;</a>
<a name="ln1758">      pending_election_op_id.index() &lt;= state_-&gt;GetCommittedOpIdUnlocked().index;</a>
<a name="ln1759"> </a>
<a name="ln1760">  if (!deduped_req.messages.empty()) {</a>
<a name="ln1761">    result.wait_for_op_id = state_-&gt;GetLastReceivedOpIdUnlocked();</a>
<a name="ln1762">  }</a>
<a name="ln1763"> </a>
<a name="ln1764">  uint64_t update_time_ms = 0;</a>
<a name="ln1765">  if (request-&gt;has_propagated_hybrid_time()) {</a>
<a name="ln1766">    update_time_ms =  HybridTime::FromPB(</a>
<a name="ln1767">        request-&gt;propagated_hybrid_time()).GetPhysicalValueMicros() / 1000;</a>
<a name="ln1768">  } else if (!deduped_req.messages.empty()) {</a>
<a name="ln1769">    update_time_ms = HybridTime::FromPB(</a>
<a name="ln1770">        deduped_req.messages.back()-&gt;hybrid_time()).GetPhysicalValueMicros() / 1000;</a>
<a name="ln1771">  }</a>
<a name="ln1772">  follower_last_update_time_ms_metric_-&gt;UpdateTimestampInMilliseconds(</a>
<a name="ln1773">      (update_time_ms &gt; 0 ? update_time_ms : clock_-&gt;Now().GetPhysicalValueMicros() / 1000));</a>
<a name="ln1774">  TRACE(&quot;UpdateReplica() finished&quot;);</a>
<a name="ln1775">  return result;</a>
<a name="ln1776">}</a>
<a name="ln1777"> </a>
<a name="ln1778">Status RaftConsensus::EarlyCommitUnlocked(const ConsensusRequestPB&amp; request,</a>
<a name="ln1779">                                          const LeaderRequest&amp; deduped_req) {</a>
<a name="ln1780">  // What should we commit?</a>
<a name="ln1781">  // 1. As many pending operations as we can, except...</a>
<a name="ln1782">  // 2. ...if we commit beyond the preceding index, we'd regress KUDU-639</a>
<a name="ln1783">  //    (&quot;Leader doesn't overwrite demoted follower's log properly&quot;), and...</a>
<a name="ln1784">  // 3. ...the leader's committed index is always our upper bound.</a>
<a name="ln1785">  auto early_apply_up_to = yb::OpId::FromPB(state_-&gt;GetLastPendingOperationOpIdUnlocked());</a>
<a name="ln1786">  if (deduped_req.preceding_op_id.index &lt; early_apply_up_to.index) {</a>
<a name="ln1787">    early_apply_up_to = deduped_req.preceding_op_id;</a>
<a name="ln1788">  }</a>
<a name="ln1789">  if (request.committed_op_id().index() &lt; early_apply_up_to.index) {</a>
<a name="ln1790">    early_apply_up_to = yb::OpId::FromPB(request.committed_op_id());</a>
<a name="ln1791">  }</a>
<a name="ln1792"> </a>
<a name="ln1793">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Early marking committed up to &quot; &lt;&lt; early_apply_up_to;</a>
<a name="ln1794">  TRACE(&quot;Early marking committed up to $0.$1&quot;, early_apply_up_to.term, early_apply_up_to.index);</a>
<a name="ln1795">  return ResultToStatus(state_-&gt;AdvanceCommittedOpIdUnlocked(early_apply_up_to, CouldStop::kTrue));</a>
<a name="ln1796">}</a>
<a name="ln1797"> </a>
<a name="ln1798">Result&lt;bool&gt; RaftConsensus::EnqueuePreparesUnlocked(const ConsensusRequestPB&amp; request,</a>
<a name="ln1799">                                                    LeaderRequest* deduped_req_ptr,</a>
<a name="ln1800">                                                    ConsensusResponsePB* response) {</a>
<a name="ln1801">  LeaderRequest&amp; deduped_req = *deduped_req_ptr;</a>
<a name="ln1802">  TRACE(&quot;Triggering prepare for $0 ops&quot;, deduped_req.messages.size());</a>
<a name="ln1803"> </a>
<a name="ln1804">  Status prepare_status;</a>
<a name="ln1805">  auto iter = deduped_req.messages.begin();</a>
<a name="ln1806"> </a>
<a name="ln1807">  if (PREDICT_TRUE(deduped_req.messages.size() &gt; 0)) {</a>
<a name="ln1808">    // TODO Temporary until the leader explicitly propagates the safe hybrid_time.</a>
<a name="ln1809">    // TODO: what if there is a failure here because the updated time is too far in the future?</a>
<a name="ln1810">    clock_-&gt;Update(HybridTime(deduped_req.messages.back()-&gt;hybrid_time()));</a>
<a name="ln1811">  }</a>
<a name="ln1812"> </a>
<a name="ln1813">  HybridTime propagated_safe_time;</a>
<a name="ln1814">  if (request.has_propagated_safe_time()) {</a>
<a name="ln1815">    propagated_safe_time = HybridTime(request.propagated_safe_time());</a>
<a name="ln1816">    if (deduped_req.messages.empty()) {</a>
<a name="ln1817">      state_-&gt;context()-&gt;SetPropagatedSafeTime(propagated_safe_time);</a>
<a name="ln1818">    }</a>
<a name="ln1819">  }</a>
<a name="ln1820"> </a>
<a name="ln1821">  if (iter != deduped_req.messages.end()) {</a>
<a name="ln1822">    for (;;) {</a>
<a name="ln1823">      const ReplicateMsgPtr&amp; msg = *iter;</a>
<a name="ln1824">      ++iter;</a>
<a name="ln1825">      bool last = iter == deduped_req.messages.end();</a>
<a name="ln1826">      prepare_status = StartReplicaOperationUnlocked(</a>
<a name="ln1827">          msg, last ? propagated_safe_time : HybridTime::kInvalid);</a>
<a name="ln1828">      if (PREDICT_FALSE(!prepare_status.ok())) {</a>
<a name="ln1829">        --iter;</a>
<a name="ln1830">        LOG_WITH_PREFIX(WARNING) &lt;&lt; &quot;StartReplicaOperationUnlocked failed: &quot; &lt;&lt; prepare_status;</a>
<a name="ln1831">        break;</a>
<a name="ln1832">      }</a>
<a name="ln1833">      if (last) {</a>
<a name="ln1834">        break;</a>
<a name="ln1835">      }</a>
<a name="ln1836">    }</a>
<a name="ln1837">  }</a>
<a name="ln1838"> </a>
<a name="ln1839">  // If we stopped before reaching the end we failed to prepare some message(s) and need</a>
<a name="ln1840">  // to perform cleanup, namely trimming deduped_req.messages to only contain the messages</a>
<a name="ln1841">  // that were actually prepared, and deleting the other ones since we've taken ownership</a>
<a name="ln1842">  // when we first deduped.</a>
<a name="ln1843">  bool incomplete = iter != deduped_req.messages.end();</a>
<a name="ln1844">  if (incomplete) {</a>
<a name="ln1845">    {</a>
<a name="ln1846">      const ReplicateMsgPtr msg = *iter;</a>
<a name="ln1847">      LOG_WITH_PREFIX(WARNING)</a>
<a name="ln1848">          &lt;&lt; &quot;Could not prepare operation for op: &quot;</a>
<a name="ln1849">          &lt;&lt; msg-&gt;id() &lt;&lt; &quot;. Suppressed &quot; &lt;&lt; (deduped_req.messages.end() - iter - 1)</a>
<a name="ln1850">          &lt;&lt; &quot; other warnings. Status for this op: &quot; &lt;&lt; prepare_status;</a>
<a name="ln1851">      deduped_req.messages.erase(iter, deduped_req.messages.end());</a>
<a name="ln1852">    }</a>
<a name="ln1853"> </a>
<a name="ln1854">    // If this is empty, it means we couldn't prepare a single de-duped message. There is nothing</a>
<a name="ln1855">    // else we can do. The leader will detect this and retry later.</a>
<a name="ln1856">    if (deduped_req.messages.empty()) {</a>
<a name="ln1857">      auto msg = Format(&quot;Rejecting Update request from peer $0 for term $1. &quot;</a>
<a name="ln1858">                        &quot;Could not prepare a single operation due to: $2&quot;,</a>
<a name="ln1859">                        request.caller_uuid(),</a>
<a name="ln1860">                        request.caller_term(),</a>
<a name="ln1861">                        prepare_status);</a>
<a name="ln1862">      LOG_WITH_PREFIX(INFO) &lt;&lt; msg;</a>
<a name="ln1863">      FillConsensusResponseError(response, ConsensusErrorPB::CANNOT_PREPARE,</a>
<a name="ln1864">                                 STATUS(IllegalState, msg));</a>
<a name="ln1865">      FillConsensusResponseOKUnlocked(response);</a>
<a name="ln1866">      return false;</a>
<a name="ln1867">    }</a>
<a name="ln1868">  }</a>
<a name="ln1869"> </a>
<a name="ln1870">  deduped_req.committed_op_id = yb::OpId::FromPB(request.committed_op_id());</a>
<a name="ln1871">  if (!deduped_req.messages.empty()) {</a>
<a name="ln1872">    auto last_op_id = yb::OpId::FromPB(deduped_req.messages.back()-&gt;id());</a>
<a name="ln1873">    if (deduped_req.committed_op_id &gt; last_op_id) {</a>
<a name="ln1874">      LOG_IF_WITH_PREFIX(DFATAL, !incomplete)</a>
<a name="ln1875">          &lt;&lt; &quot;Received committed op id: &quot; &lt;&lt; deduped_req.committed_op_id</a>
<a name="ln1876">          &lt;&lt; &quot;, past last known op id: &quot; &lt;&lt; last_op_id;</a>
<a name="ln1877"> </a>
<a name="ln1878">      // It is possible that we failed to prepare of of messages,</a>
<a name="ln1879">      // so limit committed op id to avoid having committed op id past last known op it.</a>
<a name="ln1880">      deduped_req.committed_op_id = last_op_id;</a>
<a name="ln1881">    }</a>
<a name="ln1882">  }</a>
<a name="ln1883"> </a>
<a name="ln1884">  return true;</a>
<a name="ln1885">}</a>
<a name="ln1886"> </a>
<a name="ln1887">yb::OpId RaftConsensus::EnqueueWritesUnlocked(const LeaderRequest&amp; deduped_req,</a>
<a name="ln1888">                                              WriteEmpty write_empty) {</a>
<a name="ln1889">  // Now that we've triggered the prepares enqueue the operations to be written</a>
<a name="ln1890">  // to the WAL.</a>
<a name="ln1891">  if (PREDICT_TRUE(!deduped_req.messages.empty()) || write_empty) {</a>
<a name="ln1892">    // Trigger the log append asap, if fsync() is on this might take a while</a>
<a name="ln1893">    // and we can't reply until this is done.</a>
<a name="ln1894">    //</a>
<a name="ln1895">    // Since we've prepared, we need to be able to append (or we risk trying to apply</a>
<a name="ln1896">    // later something that wasn't logged). We crash if we can't.</a>
<a name="ln1897">    CHECK_OK(queue_-&gt;AppendOperations(</a>
<a name="ln1898">        deduped_req.messages, deduped_req.committed_op_id, state_-&gt;Clock().Now()));</a>
<a name="ln1899">  }</a>
<a name="ln1900"> </a>
<a name="ln1901">  return !deduped_req.messages.empty() ?</a>
<a name="ln1902">      yb::OpId::FromPB(deduped_req.messages.back()-&gt;id()) : deduped_req.preceding_op_id;</a>
<a name="ln1903">}</a>
<a name="ln1904"> </a>
<a name="ln1905">Status RaftConsensus::WaitForWrites(const yb::OpId&amp; wait_for_op_id) {</a>
<a name="ln1906">  // 5 - We wait for the writes to be durable.</a>
<a name="ln1907"> </a>
<a name="ln1908">  // Note that this is safe because dist consensus now only supports a single outstanding</a>
<a name="ln1909">  // request at a time and this way we can allow commits to proceed while we wait.</a>
<a name="ln1910">  TRACE(&quot;Waiting on the replicates to finish logging&quot;);</a>
<a name="ln1911">  TRACE_EVENT0(&quot;consensus&quot;, &quot;Wait for log&quot;);</a>
<a name="ln1912">  for (;;) {</a>
<a name="ln1913">    auto wait_result = log_-&gt;WaitForSafeOpIdToApply(</a>
<a name="ln1914">        wait_for_op_id, MonoDelta::FromMilliseconds(FLAGS_raft_heartbeat_interval_ms));</a>
<a name="ln1915">    // If just waiting for our log append to finish lets snooze the timer.</a>
<a name="ln1916">    // We don't want to fire leader election because we're waiting on our own log.</a>
<a name="ln1917">    if (wait_result) {</a>
<a name="ln1918">      break;</a>
<a name="ln1919">    }</a>
<a name="ln1920">    SnoozeFailureDetector(DO_NOT_LOG);</a>
<a name="ln1921"> </a>
<a name="ln1922">    const auto election_timeout_at = MonoTime::Now() + MinimumElectionTimeout();</a>
<a name="ln1923">    UpdateAtomicMax(&amp;withhold_votes_until_, election_timeout_at);</a>
<a name="ln1924">  }</a>
<a name="ln1925">  TRACE(&quot;Finished waiting on the replicates to finish logging&quot;);</a>
<a name="ln1926"> </a>
<a name="ln1927">  return Status::OK();</a>
<a name="ln1928">}</a>
<a name="ln1929"> </a>
<a name="ln1930">Status RaftConsensus::MarkOperationsAsCommittedUnlocked(const ConsensusRequestPB&amp; request,</a>
<a name="ln1931">                                                        const LeaderRequest&amp; deduped_req,</a>
<a name="ln1932">                                                        yb::OpId last_from_leader) {</a>
<a name="ln1933">  // Choose the last operation to be applied. This will either be 'committed_index', if</a>
<a name="ln1934">  // no prepare enqueuing failed, or the minimum between 'committed_index' and the id of</a>
<a name="ln1935">  // the last successfully enqueued prepare, if some prepare failed to enqueue.</a>
<a name="ln1936">  yb::OpId apply_up_to;</a>
<a name="ln1937">  if (last_from_leader.index &lt; request.committed_op_id().index()) {</a>
<a name="ln1938">    // we should never apply anything later than what we received in this request</a>
<a name="ln1939">    apply_up_to = last_from_leader;</a>
<a name="ln1940"> </a>
<a name="ln1941">    VLOG_WITH_PREFIX(2)</a>
<a name="ln1942">        &lt;&lt; &quot;Received commit index &quot; &lt;&lt; request.committed_op_id()</a>
<a name="ln1943">        &lt;&lt; &quot; from the leader but only marked up to &quot; &lt;&lt; apply_up_to &lt;&lt; &quot; as committed.&quot;;</a>
<a name="ln1944">  } else {</a>
<a name="ln1945">    apply_up_to = yb::OpId::FromPB(request.committed_op_id());</a>
<a name="ln1946">  }</a>
<a name="ln1947"> </a>
<a name="ln1948">  // We can now update the last received watermark.</a>
<a name="ln1949">  //</a>
<a name="ln1950">  // We do it here (and before we actually hear back from the wal whether things</a>
<a name="ln1951">  // are durable) so that, if we receive another, possible duplicate, message</a>
<a name="ln1952">  // that exercises this path we don't handle these messages twice.</a>
<a name="ln1953">  //</a>
<a name="ln1954">  // If any messages failed to be started locally, then we already have removed them</a>
<a name="ln1955">  // from 'deduped_req' at this point. So, we can simply update our last-received</a>
<a name="ln1956">  // watermark to the last message that remains in 'deduped_req'.</a>
<a name="ln1957">  //</a>
<a name="ln1958">  // It's possible that the leader didn't send us any new data -- it might be a completely</a>
<a name="ln1959">  // duplicate request. In that case, we don't need to update LastReceived at all.</a>
<a name="ln1960">  if (!deduped_req.messages.empty()) {</a>
<a name="ln1961">    OpIdPB last_appended = deduped_req.messages.back()-&gt;id();</a>
<a name="ln1962">    TRACE(Substitute(&quot;Updating last received op as $0&quot;, last_appended.ShortDebugString()));</a>
<a name="ln1963">    state_-&gt;UpdateLastReceivedOpIdUnlocked(last_appended);</a>
<a name="ln1964">  } else if (state_-&gt;GetLastReceivedOpIdUnlocked().index &lt; deduped_req.preceding_op_id.index) {</a>
<a name="ln1965">    return STATUS_FORMAT(InvalidArgument,</a>
<a name="ln1966">                         &quot;Bad preceding_opid: $0, last received: $1&quot;,</a>
<a name="ln1967">                         deduped_req.preceding_op_id,</a>
<a name="ln1968">                         state_-&gt;GetLastReceivedOpIdUnlocked());</a>
<a name="ln1969">  }</a>
<a name="ln1970"> </a>
<a name="ln1971">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Marking committed up to &quot; &lt;&lt; apply_up_to;</a>
<a name="ln1972">  TRACE(Format(&quot;Marking committed up to $0&quot;, apply_up_to));</a>
<a name="ln1973">  return ResultToStatus(state_-&gt;AdvanceCommittedOpIdUnlocked(apply_up_to, CouldStop::kTrue));</a>
<a name="ln1974">}</a>
<a name="ln1975"> </a>
<a name="ln1976">void RaftConsensus::FillConsensusResponseOKUnlocked(ConsensusResponsePB* response) {</a>
<a name="ln1977">  TRACE(&quot;Filling consensus response to leader.&quot;);</a>
<a name="ln1978">  response-&gt;set_responder_term(state_-&gt;GetCurrentTermUnlocked());</a>
<a name="ln1979">  state_-&gt;GetLastReceivedOpIdUnlocked().ToPB(response-&gt;mutable_status()-&gt;mutable_last_received());</a>
<a name="ln1980">  state_-&gt;GetLastReceivedOpIdCurLeaderUnlocked().ToPB(</a>
<a name="ln1981">      response-&gt;mutable_status()-&gt;mutable_last_received_current_leader());</a>
<a name="ln1982">  response-&gt;mutable_status()-&gt;set_last_committed_idx(state_-&gt;GetCommittedOpIdUnlocked().index);</a>
<a name="ln1983">}</a>
<a name="ln1984"> </a>
<a name="ln1985">void RaftConsensus::FillConsensusResponseError(ConsensusResponsePB* response,</a>
<a name="ln1986">                                               ConsensusErrorPB::Code error_code,</a>
<a name="ln1987">                                               const Status&amp; status) {</a>
<a name="ln1988">  ConsensusErrorPB* error = response-&gt;mutable_status()-&gt;mutable_error();</a>
<a name="ln1989">  error-&gt;set_code(error_code);</a>
<a name="ln1990">  StatusToPB(status, error-&gt;mutable_status());</a>
<a name="ln1991">}</a>
<a name="ln1992"> </a>
<a name="ln1993">Status RaftConsensus::RequestVote(const VoteRequestPB* request, VoteResponsePB* response) {</a>
<a name="ln1994">  TRACE_EVENT2(&quot;consensus&quot;, &quot;RaftConsensus::RequestVote&quot;,</a>
<a name="ln1995">               &quot;peer&quot;, peer_uuid(),</a>
<a name="ln1996">               &quot;tablet&quot;, tablet_id());</a>
<a name="ln1997">  bool preelection = request-&gt;preelection();</a>
<a name="ln1998"> </a>
<a name="ln1999">  response-&gt;set_responder_uuid(state_-&gt;GetPeerUuid());</a>
<a name="ln2000">  response-&gt;set_preelection(preelection);</a>
<a name="ln2001"> </a>
<a name="ln2002">  // We must acquire the update lock in order to ensure that this vote action</a>
<a name="ln2003">  // takes place between requests.</a>
<a name="ln2004">  // Lock ordering: The update lock must be acquired before the ReplicaState lock.</a>
<a name="ln2005">  std::unique_lock&lt;decltype(update_mutex_)&gt; update_guard(update_mutex_, std::defer_lock);</a>
<a name="ln2006">  if (FLAGS_enable_leader_failure_detection) {</a>
<a name="ln2007">    update_guard.try_lock();</a>
<a name="ln2008">  } else {</a>
<a name="ln2009">    // If failure detection is not enabled, then we can't just reject the vote,</a>
<a name="ln2010">    // because there will be no automatic retry later. So, block for the lock.</a>
<a name="ln2011">    update_guard.lock();</a>
<a name="ln2012">  }</a>
<a name="ln2013">  if (!update_guard.owns_lock()) {</a>
<a name="ln2014">    // There is another vote or update concurrent with the vote. In that case, that</a>
<a name="ln2015">    // other request is likely to reset the timer, and we'll end up just voting</a>
<a name="ln2016">    // &quot;NO&quot; after waiting. To avoid starving RPC handlers and causing cascading</a>
<a name="ln2017">    // timeouts, just vote a quick NO.</a>
<a name="ln2018">    //</a>
<a name="ln2019">    // We still need to take the state lock in order to respond with term info, etc.</a>
<a name="ln2020">    ReplicaState::UniqueLock state_guard;</a>
<a name="ln2021">    RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;state_guard));</a>
<a name="ln2022">    return RequestVoteRespondIsBusy(request, response);</a>
<a name="ln2023">  }</a>
<a name="ln2024"> </a>
<a name="ln2025">  // Acquire the replica state lock so we can read / modify the consensus state.</a>
<a name="ln2026">  ReplicaState::UniqueLock state_guard;</a>
<a name="ln2027">  RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;state_guard));</a>
<a name="ln2028"> </a>
<a name="ln2029">  // If the node is not in the configuration, allow the vote (this is required by Raft)</a>
<a name="ln2030">  // but log an informational message anyway.</a>
<a name="ln2031">  if (!IsRaftConfigMember(request-&gt;candidate_uuid(), state_-&gt;GetActiveConfigUnlocked())) {</a>
<a name="ln2032">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Handling vote request from an unknown peer &quot;</a>
<a name="ln2033">                          &lt;&lt; request-&gt;candidate_uuid();</a>
<a name="ln2034">  }</a>
<a name="ln2035"> </a>
<a name="ln2036">  // If we've heard recently from the leader, then we should ignore the request</a>
<a name="ln2037">  // (except if it is the leader itself requesting a vote -- something that might</a>
<a name="ln2038">  //  happen if the leader were to stepdown and call an election.). Otherwise,</a>
<a name="ln2039">  // it might be from a &quot;disruptive&quot; server. This could happen in a few cases:</a>
<a name="ln2040">  //</a>
<a name="ln2041">  // 1) Network partitions</a>
<a name="ln2042">  // If the leader can talk to a majority of the nodes, but is partitioned from a</a>
<a name="ln2043">  // bad node, the bad node's failure detector will trigger. If the bad node is</a>
<a name="ln2044">  // able to reach other nodes in the cluster, it will continuously trigger elections.</a>
<a name="ln2045">  //</a>
<a name="ln2046">  // 2) An abandoned node</a>
<a name="ln2047">  // It's possible that a node has fallen behind the log GC mark of the leader. In that</a>
<a name="ln2048">  // case, the leader will stop sending it requests. Eventually, the configuration</a>
<a name="ln2049">  // will change to eject the abandoned node, but until that point, we don't want the</a>
<a name="ln2050">  // abandoned follower to disturb the other nodes.</a>
<a name="ln2051">  //</a>
<a name="ln2052">  // See also https://ramcloud.stanford.edu/~ongaro/thesis.pdf</a>
<a name="ln2053">  // section 4.2.3.</a>
<a name="ln2054">  MonoTime now = MonoTime::Now();</a>
<a name="ln2055">  if (request-&gt;candidate_uuid() != state_-&gt;GetLeaderUuidUnlocked() &amp;&amp;</a>
<a name="ln2056">      !request-&gt;ignore_live_leader() &amp;&amp;</a>
<a name="ln2057">      now &lt; withhold_votes_until_.load(std::memory_order_acquire)) {</a>
<a name="ln2058">    return RequestVoteRespondLeaderIsAlive(request, response);</a>
<a name="ln2059">  }</a>
<a name="ln2060"> </a>
<a name="ln2061">  // Candidate is running behind.</a>
<a name="ln2062">  if (request-&gt;candidate_term() &lt; state_-&gt;GetCurrentTermUnlocked()) {</a>
<a name="ln2063">    return RequestVoteRespondInvalidTerm(request, response);</a>
<a name="ln2064">  }</a>
<a name="ln2065"> </a>
<a name="ln2066">  // We already voted this term.</a>
<a name="ln2067">  if (request-&gt;candidate_term() == state_-&gt;GetCurrentTermUnlocked() &amp;&amp;</a>
<a name="ln2068">      state_-&gt;HasVotedCurrentTermUnlocked()) {</a>
<a name="ln2069"> </a>
<a name="ln2070">    // Already voted for the same candidate in the current term.</a>
<a name="ln2071">    if (state_-&gt;GetVotedForCurrentTermUnlocked() == request-&gt;candidate_uuid()) {</a>
<a name="ln2072">      return RequestVoteRespondVoteAlreadyGranted(request, response);</a>
<a name="ln2073">    }</a>
<a name="ln2074"> </a>
<a name="ln2075">    // Voted for someone else in current term.</a>
<a name="ln2076">    return RequestVoteRespondAlreadyVotedForOther(request, response);</a>
<a name="ln2077">  }</a>
<a name="ln2078"> </a>
<a name="ln2079">  // The term advanced.</a>
<a name="ln2080">  if (request-&gt;candidate_term() &gt; state_-&gt;GetCurrentTermUnlocked() &amp;&amp; !preelection) {</a>
<a name="ln2081">    RETURN_NOT_OK_PREPEND(HandleTermAdvanceUnlocked(request-&gt;candidate_term()),</a>
<a name="ln2082">        Substitute(&quot;Could not step down in RequestVote. Current term: $0, candidate term: $1&quot;,</a>
<a name="ln2083">            state_-&gt;GetCurrentTermUnlocked(), request-&gt;candidate_term()));</a>
<a name="ln2084">  }</a>
<a name="ln2085"> </a>
<a name="ln2086">  // Candidate must have last-logged OpId at least as large as our own to get our vote.</a>
<a name="ln2087">  OpIdPB local_last_logged_opid;</a>
<a name="ln2088">  GetLatestOpIdFromLog().ToPB(&amp;local_last_logged_opid);</a>
<a name="ln2089">  if (OpIdLessThan(request-&gt;candidate_status().last_received(), local_last_logged_opid)) {</a>
<a name="ln2090">    return RequestVoteRespondLastOpIdTooOld(local_last_logged_opid, request, response);</a>
<a name="ln2091">  }</a>
<a name="ln2092"> </a>
<a name="ln2093">  if (!preelection) {</a>
<a name="ln2094">    // Clear the pending election op id if any before granting the vote. If another peer jumps in</a>
<a name="ln2095">    // before we can catch up and start the election, let's not disrupt the quorum with another</a>
<a name="ln2096">    // election.</a>
<a name="ln2097">    state_-&gt;ClearPendingElectionOpIdUnlocked();</a>
<a name="ln2098">  }</a>
<a name="ln2099"> </a>
<a name="ln2100">  auto remaining_old_leader_lease = state_-&gt;RemainingOldLeaderLeaseDuration();</a>
<a name="ln2101"> </a>
<a name="ln2102">  if (remaining_old_leader_lease.Initialized()) {</a>
<a name="ln2103">    response-&gt;set_remaining_leader_lease_duration_ms(</a>
<a name="ln2104">        remaining_old_leader_lease.ToMilliseconds());</a>
<a name="ln2105">    response-&gt;set_leader_lease_uuid(state_-&gt;old_leader_lease().holder_uuid);</a>
<a name="ln2106">  }</a>
<a name="ln2107"> </a>
<a name="ln2108">  const auto&amp; old_leader_ht_lease = state_-&gt;old_leader_ht_lease();</a>
<a name="ln2109">  if (old_leader_ht_lease) {</a>
<a name="ln2110">    response-&gt;set_leader_ht_lease_expiration(old_leader_ht_lease.expiration);</a>
<a name="ln2111">    response-&gt;set_leader_ht_lease_uuid(old_leader_ht_lease.holder_uuid);</a>
<a name="ln2112">  }</a>
<a name="ln2113"> </a>
<a name="ln2114">  // Passed all our checks. Vote granted.</a>
<a name="ln2115">  if (preelection) {</a>
<a name="ln2116">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Pre-election. Granting vote for candidate &quot;</a>
<a name="ln2117">                          &lt;&lt; request-&gt;candidate_uuid() &lt;&lt; &quot; in term &quot; &lt;&lt; request-&gt;candidate_term();</a>
<a name="ln2118">    FillVoteResponseVoteGranted(*request, response);</a>
<a name="ln2119">    return Status::OK();</a>
<a name="ln2120">  }</a>
<a name="ln2121"> </a>
<a name="ln2122">  return RequestVoteRespondVoteGranted(request, response);</a>
<a name="ln2123">}</a>
<a name="ln2124"> </a>
<a name="ln2125">Status RaftConsensus::IsLeaderReadyForChangeConfigUnlocked(ChangeConfigType type,</a>
<a name="ln2126">                                                           const string&amp; server_uuid) {</a>
<a name="ln2127">  const RaftConfigPB&amp; active_config = state_-&gt;GetActiveConfigUnlocked();</a>
<a name="ln2128">  int servers_in_transition = 0;</a>
<a name="ln2129">  if (type == ADD_SERVER) {</a>
<a name="ln2130">    servers_in_transition = CountServersInTransition(active_config);</a>
<a name="ln2131">  } else if (type == REMOVE_SERVER) {</a>
<a name="ln2132">    // If we are trying to remove the server in transition, then servers_in_transition shouldn't</a>
<a name="ln2133">    // count it so we can proceed with the operation.</a>
<a name="ln2134">    servers_in_transition = CountServersInTransition(active_config, server_uuid);</a>
<a name="ln2135">  }</a>
<a name="ln2136"> </a>
<a name="ln2137">  // Check that all the following requirements are met:</a>
<a name="ln2138">  // 1. We are required by Raft to reject config change operations until we have</a>
<a name="ln2139">  //    committed at least one operation in our current term as leader.</a>
<a name="ln2140">  //    See https://groups.google.com/forum/#!topic/raft-dev/t4xj6dJTP6E</a>
<a name="ln2141">  // 2. Ensure there is no other pending change config.</a>
<a name="ln2142">  // 3. There are no peers that are in the process of becoming VOTERs or OBSERVERs.</a>
<a name="ln2143">  if (!state_-&gt;AreCommittedAndCurrentTermsSameUnlocked() ||</a>
<a name="ln2144">      state_-&gt;IsConfigChangePendingUnlocked() ||</a>
<a name="ln2145">      servers_in_transition != 0) {</a>
<a name="ln2146">    return STATUS_FORMAT(IllegalState,</a>
<a name="ln2147">                         &quot;Leader is not ready for Config Change, can try again. &quot;</a>
<a name="ln2148">                         &quot;Num peers in transit: $0. Type: $1. Has opid: $2. Committed config: $3. &quot;</a>
<a name="ln2149">                         &quot;Pending config: $4. Current term: $5. Committed op id: $6.&quot;,</a>
<a name="ln2150">                         servers_in_transition, ChangeConfigType_Name(type),</a>
<a name="ln2151">                         active_config.has_opid_index(),</a>
<a name="ln2152">                         state_-&gt;GetCommittedConfigUnlocked().ShortDebugString(),</a>
<a name="ln2153">                         state_-&gt;IsConfigChangePendingUnlocked() ?</a>
<a name="ln2154">                             state_-&gt;GetPendingConfigUnlocked().ShortDebugString() : &quot;&quot;,</a>
<a name="ln2155">                         state_-&gt;GetCurrentTermUnlocked(), state_-&gt;GetCommittedOpIdUnlocked());</a>
<a name="ln2156">  }</a>
<a name="ln2157"> </a>
<a name="ln2158">  return Status::OK();</a>
<a name="ln2159">}</a>
<a name="ln2160"> </a>
<a name="ln2161">Status RaftConsensus::ChangeConfig(const ChangeConfigRequestPB&amp; req,</a>
<a name="ln2162">                                   const StdStatusCallback&amp; client_cb,</a>
<a name="ln2163">                                   boost::optional&lt;TabletServerErrorPB::Code&gt;* error_code) {</a>
<a name="ln2164">  if (PREDICT_FALSE(!req.has_type())) {</a>
<a name="ln2165">    return STATUS(InvalidArgument, &quot;Must specify 'type' argument to ChangeConfig()&quot;,</a>
<a name="ln2166">                                   req.ShortDebugString());</a>
<a name="ln2167">  }</a>
<a name="ln2168">  if (PREDICT_FALSE(!req.has_server())) {</a>
<a name="ln2169">    *error_code = TabletServerErrorPB::INVALID_CONFIG;</a>
<a name="ln2170">    return STATUS(InvalidArgument, &quot;Must specify 'server' argument to ChangeConfig()&quot;,</a>
<a name="ln2171">                                   req.ShortDebugString());</a>
<a name="ln2172">  }</a>
<a name="ln2173">  YB_LOG_EVERY_N(INFO, FLAGS_TEST_log_change_config_every_n)</a>
<a name="ln2174">      &lt;&lt; &quot;Received ChangeConfig request &quot; &lt;&lt; req.ShortDebugString();</a>
<a name="ln2175">  ChangeConfigType type = req.type();</a>
<a name="ln2176">  bool use_hostport = req.has_use_host() &amp;&amp; req.use_host();</a>
<a name="ln2177"> </a>
<a name="ln2178">  if (type != REMOVE_SERVER &amp;&amp; use_hostport) {</a>
<a name="ln2179">    return STATUS_SUBSTITUTE(InvalidArgument, &quot;Cannot set use_host for change config type $0, &quot;</a>
<a name="ln2180">                             &quot;only allowed with REMOVE_SERVER.&quot;, type);</a>
<a name="ln2181">  }</a>
<a name="ln2182"> </a>
<a name="ln2183">  if (PREDICT_FALSE(FLAGS_TEST_return_error_on_change_config != 0.0 &amp;&amp; type == CHANGE_ROLE)) {</a>
<a name="ln2184">    DCHECK(FLAGS_TEST_return_error_on_change_config &gt;= 0.0 &amp;&amp;</a>
<a name="ln2185">           FLAGS_TEST_return_error_on_change_config &lt;= 1.0);</a>
<a name="ln2186">    if (clock_-&gt;Now().ToUint64() % 100 &lt; 100 * FLAGS_TEST_return_error_on_change_config) {</a>
<a name="ln2187">      return STATUS(IllegalState, &quot;Returning error for unit test&quot;);</a>
<a name="ln2188">    }</a>
<a name="ln2189">  }</a>
<a name="ln2190">  RaftPeerPB* new_peer = nullptr;</a>
<a name="ln2191">  const RaftPeerPB&amp; server = req.server();</a>
<a name="ln2192">  if (!use_hostport &amp;&amp; !server.has_permanent_uuid()) {</a>
<a name="ln2193">    return STATUS(InvalidArgument,</a>
<a name="ln2194">                  Substitute(&quot;server must have permanent_uuid or use_host specified: $0&quot;,</a>
<a name="ln2195">                             req.ShortDebugString()));</a>
<a name="ln2196">  }</a>
<a name="ln2197">  {</a>
<a name="ln2198">    ReplicaState::UniqueLock lock;</a>
<a name="ln2199">    RETURN_NOT_OK(state_-&gt;LockForConfigChange(&amp;lock));</a>
<a name="ln2200">    Status s = state_-&gt;CheckActiveLeaderUnlocked(LeaderLeaseCheckMode::DONT_NEED_LEASE);</a>
<a name="ln2201">    if (!s.ok()) {</a>
<a name="ln2202">      *error_code = TabletServerErrorPB::NOT_THE_LEADER;</a>
<a name="ln2203">      return s;</a>
<a name="ln2204">    }</a>
<a name="ln2205"> </a>
<a name="ln2206">    const string&amp; server_uuid = server.has_permanent_uuid() ? server.permanent_uuid() : &quot;&quot;;</a>
<a name="ln2207">    s = IsLeaderReadyForChangeConfigUnlocked(type, server_uuid);</a>
<a name="ln2208">    if (!s.ok()) {</a>
<a name="ln2209">      YB_LOG_EVERY_N(INFO, FLAGS_TEST_log_change_config_every_n)</a>
<a name="ln2210">          &lt;&lt; &quot;Returning not ready for &quot; &lt;&lt; ChangeConfigType_Name(type)</a>
<a name="ln2211">          &lt;&lt; &quot; due to error &quot; &lt;&lt; s.ToString();</a>
<a name="ln2212">      *error_code = TabletServerErrorPB::LEADER_NOT_READY_CHANGE_CONFIG;</a>
<a name="ln2213">      return s;</a>
<a name="ln2214">    }</a>
<a name="ln2215"> </a>
<a name="ln2216">    const RaftConfigPB&amp; committed_config = state_-&gt;GetCommittedConfigUnlocked();</a>
<a name="ln2217"> </a>
<a name="ln2218">    // Support atomic ChangeConfig requests.</a>
<a name="ln2219">    if (req.has_cas_config_opid_index()) {</a>
<a name="ln2220">      if (committed_config.opid_index() != req.cas_config_opid_index()) {</a>
<a name="ln2221">        *error_code = TabletServerErrorPB::CAS_FAILED;</a>
<a name="ln2222">        return STATUS(IllegalState, Substitute(&quot;Request specified cas_config_opid_index &quot;</a>
<a name="ln2223">                                               &quot;of $0 but the committed config has opid_index &quot;</a>
<a name="ln2224">                                               &quot;of $1&quot;,</a>
<a name="ln2225">                                               req.cas_config_opid_index(),</a>
<a name="ln2226">                                               committed_config.opid_index()));</a>
<a name="ln2227">      }</a>
<a name="ln2228">    }</a>
<a name="ln2229"> </a>
<a name="ln2230">    RaftConfigPB new_config = committed_config;</a>
<a name="ln2231">    new_config.clear_opid_index();</a>
<a name="ln2232">    switch (type) {</a>
<a name="ln2233">      case ADD_SERVER:</a>
<a name="ln2234">        // Ensure the server we are adding is not already a member of the configuration.</a>
<a name="ln2235">        if (IsRaftConfigMember(server_uuid, committed_config)) {</a>
<a name="ln2236">          *error_code = TabletServerErrorPB::ADD_CHANGE_CONFIG_ALREADY_PRESENT;</a>
<a name="ln2237">          return STATUS(IllegalState,</a>
<a name="ln2238">              Substitute(&quot;Server with UUID $0 is already a member of the config. RaftConfig: $1&quot;,</a>
<a name="ln2239">                        server_uuid, committed_config.ShortDebugString()));</a>
<a name="ln2240">        }</a>
<a name="ln2241">        if (!server.has_member_type()) {</a>
<a name="ln2242">          return STATUS(InvalidArgument,</a>
<a name="ln2243">                        Substitute(&quot;Server must have member_type specified. Request: $0&quot;,</a>
<a name="ln2244">                                   req.ShortDebugString()));</a>
<a name="ln2245">        }</a>
<a name="ln2246">        if (server.member_type() != RaftPeerPB::PRE_VOTER &amp;&amp;</a>
<a name="ln2247">            server.member_type() != RaftPeerPB::PRE_OBSERVER) {</a>
<a name="ln2248">          return STATUS(InvalidArgument,</a>
<a name="ln2249">              Substitute(&quot;Server with UUID $0 must be of member_type PRE_VOTER or PRE_OBSERVER. &quot;</a>
<a name="ln2250">                         &quot;member_type received: $1&quot;, server_uuid,</a>
<a name="ln2251">                         RaftPeerPB::MemberType_Name(server.member_type())));</a>
<a name="ln2252">        }</a>
<a name="ln2253">        if (server.last_known_private_addr().empty()) {</a>
<a name="ln2254">          return STATUS(InvalidArgument, &quot;server must have last_known_addr specified&quot;,</a>
<a name="ln2255">                                         req.ShortDebugString());</a>
<a name="ln2256">        }</a>
<a name="ln2257">        new_peer = new_config.add_peers();</a>
<a name="ln2258">        *new_peer = server;</a>
<a name="ln2259">        break;</a>
<a name="ln2260"> </a>
<a name="ln2261">      case REMOVE_SERVER:</a>
<a name="ln2262">        if (use_hostport) {</a>
<a name="ln2263">          if (server.last_known_private_addr().empty()) {</a>
<a name="ln2264">            return STATUS(InvalidArgument, &quot;Must have last_known_addr specified.&quot;,</a>
<a name="ln2265">                          req.ShortDebugString());</a>
<a name="ln2266">          }</a>
<a name="ln2267">          HostPort leader_hp;</a>
<a name="ln2268">          RETURN_NOT_OK(GetHostPortFromConfig(</a>
<a name="ln2269">              new_config, peer_uuid(), queue_-&gt;local_cloud_info(), &amp;leader_hp));</a>
<a name="ln2270">          for (const auto&amp; host_port : server.last_known_private_addr()) {</a>
<a name="ln2271">            if (leader_hp.port() == host_port.port() &amp;&amp; leader_hp.host() == host_port.host()) {</a>
<a name="ln2272">              return STATUS(InvalidArgument, &quot;Cannot remove live leader using hostport.&quot;,</a>
<a name="ln2273">                            req.ShortDebugString());</a>
<a name="ln2274">            }</a>
<a name="ln2275">          }</a>
<a name="ln2276">        }</a>
<a name="ln2277">        if (server_uuid == peer_uuid()) {</a>
<a name="ln2278">          *error_code = TabletServerErrorPB::LEADER_NEEDS_STEP_DOWN;</a>
<a name="ln2279">          return STATUS(InvalidArgument,</a>
<a name="ln2280">              Substitute(&quot;Cannot remove peer $0 from the config because it is the leader. &quot;</a>
<a name="ln2281">                         &quot;Force another leader to be elected to remove this server. &quot;</a>
<a name="ln2282">                         &quot;Active consensus state: $1&quot;, server_uuid,</a>
<a name="ln2283">                         state_-&gt;ConsensusStateUnlocked(CONSENSUS_CONFIG_ACTIVE)</a>
<a name="ln2284">                            .ShortDebugString()));</a>
<a name="ln2285">        }</a>
<a name="ln2286">        if (!RemoveFromRaftConfig(&amp;new_config, req)) {</a>
<a name="ln2287">          *error_code = TabletServerErrorPB::REMOVE_CHANGE_CONFIG_NOT_PRESENT;</a>
<a name="ln2288">          return STATUS(NotFound,</a>
<a name="ln2289">              Substitute(&quot;Server with UUID $0 not a member of the config. RaftConfig: $1&quot;,</a>
<a name="ln2290">                        server_uuid, committed_config.ShortDebugString()));</a>
<a name="ln2291">        }</a>
<a name="ln2292">        break;</a>
<a name="ln2293"> </a>
<a name="ln2294">      case CHANGE_ROLE:</a>
<a name="ln2295">        if (server_uuid == peer_uuid()) {</a>
<a name="ln2296">          return STATUS(InvalidArgument,</a>
<a name="ln2297">              Substitute(&quot;Cannot change role of peer $0 because it is the leader. Force &quot;</a>
<a name="ln2298">                         &quot;another leader to be elected. Active consensus state: $1&quot;, server_uuid,</a>
<a name="ln2299">                         state_-&gt;ConsensusStateUnlocked(CONSENSUS_CONFIG_ACTIVE)</a>
<a name="ln2300">                             .ShortDebugString()));</a>
<a name="ln2301">        }</a>
<a name="ln2302">        VLOG(3) &lt;&lt; &quot;config before CHANGE_ROLE: &quot; &lt;&lt; new_config.DebugString();</a>
<a name="ln2303"> </a>
<a name="ln2304">        if (!GetMutableRaftConfigMember(&amp;new_config, server_uuid, &amp;new_peer).ok()) {</a>
<a name="ln2305">          return STATUS(NotFound,</a>
<a name="ln2306">            Substitute(&quot;Server with UUID $0 not a member of the config. RaftConfig: $1&quot;,</a>
<a name="ln2307">                       server_uuid, new_config.ShortDebugString()));</a>
<a name="ln2308">        }</a>
<a name="ln2309">        if (new_peer-&gt;member_type() != RaftPeerPB::PRE_OBSERVER &amp;&amp;</a>
<a name="ln2310">            new_peer-&gt;member_type() != RaftPeerPB::PRE_VOTER) {</a>
<a name="ln2311">          return STATUS(IllegalState, Substitute(&quot;Cannot change role of server with UUID $0 &quot;</a>
<a name="ln2312">                                                 &quot;because its member type is $1&quot;,</a>
<a name="ln2313">                                                 server_uuid, new_peer-&gt;member_type()));</a>
<a name="ln2314">        }</a>
<a name="ln2315">        if (new_peer-&gt;member_type() == RaftPeerPB::PRE_OBSERVER) {</a>
<a name="ln2316">          new_peer-&gt;set_member_type(RaftPeerPB::OBSERVER);</a>
<a name="ln2317">        } else {</a>
<a name="ln2318">          new_peer-&gt;set_member_type(RaftPeerPB::VOTER);</a>
<a name="ln2319">        }</a>
<a name="ln2320"> </a>
<a name="ln2321">        VLOG(3) &lt;&lt; &quot;config after CHANGE_ROLE: &quot; &lt;&lt; new_config.DebugString();</a>
<a name="ln2322">        break;</a>
<a name="ln2323">      default:</a>
<a name="ln2324">        return STATUS(InvalidArgument, Substitute(&quot;Unsupported type $0&quot;,</a>
<a name="ln2325">                                                  ChangeConfigType_Name(type)));</a>
<a name="ln2326">    }</a>
<a name="ln2327"> </a>
<a name="ln2328">    auto cc_replicate = std::make_shared&lt;ReplicateMsg&gt;();</a>
<a name="ln2329">    cc_replicate-&gt;set_op_type(CHANGE_CONFIG_OP);</a>
<a name="ln2330">    ChangeConfigRecordPB* cc_req = cc_replicate-&gt;mutable_change_config_record();</a>
<a name="ln2331">    cc_req-&gt;set_tablet_id(tablet_id());</a>
<a name="ln2332">    *cc_req-&gt;mutable_old_config() = committed_config;</a>
<a name="ln2333">    *cc_req-&gt;mutable_new_config() = new_config;</a>
<a name="ln2334">    // Note: This hybrid_time has no meaning from a serialization perspective</a>
<a name="ln2335">    // because this method is not executed on the TabletPeer's prepare thread.</a>
<a name="ln2336">    cc_replicate-&gt;set_hybrid_time(clock_-&gt;Now().ToUint64());</a>
<a name="ln2337">    state_-&gt;GetCommittedOpIdUnlocked().ToPB(cc_replicate-&gt;mutable_committed_op_id());</a>
<a name="ln2338"> </a>
<a name="ln2339">    auto context = std::make_shared&lt;StateChangeContext&gt;(</a>
<a name="ln2340">        StateChangeReason::LEADER_CONFIG_CHANGE_COMPLETE, *cc_req,</a>
<a name="ln2341">        type == REMOVE_SERVER ? server_uuid : &quot;&quot;);</a>
<a name="ln2342"> </a>
<a name="ln2343">    RETURN_NOT_OK(</a>
<a name="ln2344">        ReplicateConfigChangeUnlocked(cc_replicate,</a>
<a name="ln2345">                                      new_config,</a>
<a name="ln2346">                                      type,</a>
<a name="ln2347">                                      std::bind(&amp;RaftConsensus::MarkDirtyOnSuccess,</a>
<a name="ln2348">                                           this,</a>
<a name="ln2349">                                           std::move(context),</a>
<a name="ln2350">                                           std::move(client_cb), std::placeholders::_1)));</a>
<a name="ln2351">  }</a>
<a name="ln2352"> </a>
<a name="ln2353">  peer_manager_-&gt;SignalRequest(RequestTriggerMode::kNonEmptyOnly);</a>
<a name="ln2354"> </a>
<a name="ln2355">  return Status::OK();</a>
<a name="ln2356">}</a>
<a name="ln2357"> </a>
<a name="ln2358">void RaftConsensus::Shutdown() {</a>
<a name="ln2359">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Shutdown.&quot;;</a>
<a name="ln2360"> </a>
<a name="ln2361">  // Avoid taking locks if already shut down so we don't violate</a>
<a name="ln2362">  // ThreadRestrictions assertions in the case where the RaftConsensus</a>
<a name="ln2363">  // destructor runs on the reactor thread due to an election callback being</a>
<a name="ln2364">  // the last outstanding reference.</a>
<a name="ln2365">  if (shutdown_.Load(kMemOrderAcquire)) return;</a>
<a name="ln2366"> </a>
<a name="ln2367">  CHECK_OK(ExecuteHook(PRE_SHUTDOWN));</a>
<a name="ln2368"> </a>
<a name="ln2369">  {</a>
<a name="ln2370">    ReplicaState::UniqueLock lock;</a>
<a name="ln2371">    // Transition to kShuttingDown state.</a>
<a name="ln2372">    CHECK_OK(state_-&gt;LockForShutdown(&amp;lock));</a>
<a name="ln2373">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Raft consensus shutting down.&quot;;</a>
<a name="ln2374">  }</a>
<a name="ln2375"> </a>
<a name="ln2376">  // Close the peer manager.</a>
<a name="ln2377">  peer_manager_-&gt;Close();</a>
<a name="ln2378"> </a>
<a name="ln2379">  // We must close the queue after we close the peers.</a>
<a name="ln2380">  queue_-&gt;Close();</a>
<a name="ln2381"> </a>
<a name="ln2382">  CHECK_OK(state_-&gt;CancelPendingOperations());</a>
<a name="ln2383"> </a>
<a name="ln2384">  {</a>
<a name="ln2385">    ReplicaState::UniqueLock lock;</a>
<a name="ln2386">    CHECK_OK(state_-&gt;LockForShutdown(&amp;lock));</a>
<a name="ln2387">    CHECK_EQ(ReplicaState::kShuttingDown, state_-&gt;state());</a>
<a name="ln2388">    CHECK_OK(state_-&gt;ShutdownUnlocked());</a>
<a name="ln2389">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Raft consensus is shut down!&quot;;</a>
<a name="ln2390">  }</a>
<a name="ln2391"> </a>
<a name="ln2392">  // Shut down things that might acquire locks during destruction.</a>
<a name="ln2393">  raft_pool_token_-&gt;Shutdown();</a>
<a name="ln2394">  // We might not have run Start yet, so make sure we have a FD.</a>
<a name="ln2395">  if (failure_detector_) {</a>
<a name="ln2396">    DisableFailureDetector();</a>
<a name="ln2397">  }</a>
<a name="ln2398"> </a>
<a name="ln2399">  CHECK_OK(ExecuteHook(POST_SHUTDOWN));</a>
<a name="ln2400"> </a>
<a name="ln2401">  shutdown_.Store(true, kMemOrderRelease);</a>
<a name="ln2402">}</a>
<a name="ln2403"> </a>
<a name="ln2404">RaftPeerPB::Role RaftConsensus::GetActiveRole() const {</a>
<a name="ln2405">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln2406">  return state_-&gt;GetActiveRoleUnlocked();</a>
<a name="ln2407">}</a>
<a name="ln2408"> </a>
<a name="ln2409">yb::OpId RaftConsensus::GetLatestOpIdFromLog() {</a>
<a name="ln2410">  return log_-&gt;GetLatestEntryOpId();</a>
<a name="ln2411">}</a>
<a name="ln2412"> </a>
<a name="ln2413">Status RaftConsensus::StartConsensusOnlyRoundUnlocked(const ReplicateMsgPtr&amp; msg) {</a>
<a name="ln2414">  OperationType op_type = msg-&gt;op_type();</a>
<a name="ln2415">  if (!IsConsensusOnlyOperation(op_type)) {</a>
<a name="ln2416">    return STATUS_FORMAT(InvalidArgument,</a>
<a name="ln2417">                         &quot;Expected a consensus-only op type, got $0: $1&quot;,</a>
<a name="ln2418">                         OperationType_Name(op_type),</a>
<a name="ln2419">                         *msg);</a>
<a name="ln2420">  }</a>
<a name="ln2421">  VLOG_WITH_PREFIX(1) &lt;&lt; &quot;Starting consensus round: &quot;</a>
<a name="ln2422">                      &lt;&lt; msg-&gt;id().ShortDebugString();</a>
<a name="ln2423">  scoped_refptr&lt;ConsensusRound&gt; round(new ConsensusRound(this, msg));</a>
<a name="ln2424">  std::shared_ptr&lt;StateChangeContext&gt; context = nullptr;</a>
<a name="ln2425"> </a>
<a name="ln2426">  // We are here for NO_OP or CHANGE_CONFIG_OP type ops. We need to set the change record for an</a>
<a name="ln2427">  // actual config change operation. The NO_OP does not update the config, as it is used for a new</a>
<a name="ln2428">  // leader election term change replicate message, which keeps the same config.</a>
<a name="ln2429">  if (IsChangeConfigOperation(op_type)) {</a>
<a name="ln2430">    context =</a>
<a name="ln2431">      std::make_shared&lt;StateChangeContext&gt;(StateChangeReason::FOLLOWER_CONFIG_CHANGE_COMPLETE,</a>
<a name="ln2432">                                           msg-&gt;change_config_record());</a>
<a name="ln2433">  } else {</a>
<a name="ln2434">    context = std::make_shared&lt;StateChangeContext&gt;(StateChangeReason::FOLLOWER_NO_OP_COMPLETE);</a>
<a name="ln2435">  }</a>
<a name="ln2436"> </a>
<a name="ln2437">  StdStatusCallback client_cb =</a>
<a name="ln2438">      std::bind(&amp;RaftConsensus::MarkDirtyOnSuccess,</a>
<a name="ln2439">                this,</a>
<a name="ln2440">                context,</a>
<a name="ln2441">                &amp;DoNothingStatusCB,</a>
<a name="ln2442">                std::placeholders::_1);</a>
<a name="ln2443">  round-&gt;SetConsensusReplicatedCallback(std::bind(&amp;RaftConsensus::NonTxRoundReplicationFinished,</a>
<a name="ln2444">                                                  this,</a>
<a name="ln2445">                                                  round.get(),</a>
<a name="ln2446">                                                  std::move(client_cb),</a>
<a name="ln2447">                                                  std::placeholders::_1));</a>
<a name="ln2448">  return state_-&gt;AddPendingOperation(round);</a>
<a name="ln2449">}</a>
<a name="ln2450"> </a>
<a name="ln2451">Status RaftConsensus::WaitForLeaderLeaseImprecise(CoarseTimePoint deadline) {</a>
<a name="ln2452">  CoarseTimePoint now;</a>
<a name="ln2453">  while ((now = CoarseMonoClock::Now()) &lt; deadline) {</a>
<a name="ln2454">    MonoDelta remaining_old_leader_lease;</a>
<a name="ln2455">    LeaderLeaseStatus leader_lease_status;</a>
<a name="ln2456">    {</a>
<a name="ln2457">      auto lock = state_-&gt;LockForRead();</a>
<a name="ln2458">      if (state_-&gt;GetActiveRoleUnlocked() != RaftPeerPB::LEADER) {</a>
<a name="ln2459">        return STATUS_FORMAT(IllegalState, &quot;Not the leader: $0&quot;, state_-&gt;GetActiveRoleUnlocked());</a>
<a name="ln2460">      }</a>
<a name="ln2461">      leader_lease_status = state_-&gt;GetLeaderLeaseStatusUnlocked(&amp;remaining_old_leader_lease);</a>
<a name="ln2462">    }</a>
<a name="ln2463">    switch (leader_lease_status) {</a>
<a name="ln2464">      case LeaderLeaseStatus::HAS_LEASE:</a>
<a name="ln2465">        return Status::OK();</a>
<a name="ln2466">      case LeaderLeaseStatus::NO_MAJORITY_REPLICATED_LEASE:</a>
<a name="ln2467">        {</a>
<a name="ln2468">          std::unique_lock&lt;decltype(leader_lease_wait_mtx_)&gt; lock(leader_lease_wait_mtx_);</a>
<a name="ln2469">          // Because we're not taking the same lock (leader_lease_wait_mtx_) when we check the</a>
<a name="ln2470">          // leader lease status, there is a possibility of a race condition when we miss the</a>
<a name="ln2471">          // notification and by this point we already have a lease. Rather than re-taking the</a>
<a name="ln2472">          // ReplicaState lock and re-checking, here we simply block for up to 100ms in that case,</a>
<a name="ln2473">          // because this function is currently (08/14/2017) only used in a context when it is OK,</a>
<a name="ln2474">          // such as catalog manager initialization.</a>
<a name="ln2475">          leader_lease_wait_cond_.wait_for(</a>
<a name="ln2476">              lock, std::max&lt;MonoDelta&gt;(100ms, deadline - now).ToSteadyDuration());</a>
<a name="ln2477">        }</a>
<a name="ln2478">        continue;</a>
<a name="ln2479">      case LeaderLeaseStatus::OLD_LEADER_MAY_HAVE_LEASE: {</a>
<a name="ln2480">        auto wait_deadline = std::min({deadline, now + 100ms, now + remaining_old_leader_lease});</a>
<a name="ln2481">        std::this_thread::sleep_until(wait_deadline);</a>
<a name="ln2482">      } continue;</a>
<a name="ln2483">    }</a>
<a name="ln2484">    FATAL_INVALID_ENUM_VALUE(LeaderLeaseStatus, leader_lease_status);</a>
<a name="ln2485">  }</a>
<a name="ln2486">  return STATUS_FORMAT(TimedOut, &quot;Waited for $0 to acquire a leader lease&quot;, deadline);</a>
<a name="ln2487">}</a>
<a name="ln2488"> </a>
<a name="ln2489">Status RaftConsensus::CheckIsActiveLeaderAndHasLease() const {</a>
<a name="ln2490">  return state_-&gt;CheckIsActiveLeaderAndHasLease();</a>
<a name="ln2491">}</a>
<a name="ln2492"> </a>
<a name="ln2493">MicrosTime RaftConsensus::MajorityReplicatedHtLeaseExpiration(</a>
<a name="ln2494">    MicrosTime min_allowed, CoarseTimePoint deadline) const {</a>
<a name="ln2495">  return state_-&gt;MajorityReplicatedHtLeaseExpiration(min_allowed, deadline);</a>
<a name="ln2496">}</a>
<a name="ln2497"> </a>
<a name="ln2498">std::string RaftConsensus::GetRequestVoteLogPrefix(const VoteRequestPB&amp; request) const {</a>
<a name="ln2499">  return Format(&quot;$0 Leader $1election vote request&quot;,</a>
<a name="ln2500">                state_-&gt;LogPrefix(), request.preelection() ? &quot;pre-&quot; : &quot;&quot;);</a>
<a name="ln2501">}</a>
<a name="ln2502"> </a>
<a name="ln2503">void RaftConsensus::FillVoteResponseVoteGranted(</a>
<a name="ln2504">    const VoteRequestPB&amp; request, VoteResponsePB* response) {</a>
<a name="ln2505">  response-&gt;set_responder_term(request.candidate_term());</a>
<a name="ln2506">  response-&gt;set_vote_granted(true);</a>
<a name="ln2507">}</a>
<a name="ln2508"> </a>
<a name="ln2509">void RaftConsensus::FillVoteResponseVoteDenied(ConsensusErrorPB::Code error_code,</a>
<a name="ln2510">                                               VoteResponsePB* response) {</a>
<a name="ln2511">  response-&gt;set_responder_term(state_-&gt;GetCurrentTermUnlocked());</a>
<a name="ln2512">  response-&gt;set_vote_granted(false);</a>
<a name="ln2513">  response-&gt;mutable_consensus_error()-&gt;set_code(error_code);</a>
<a name="ln2514">}</a>
<a name="ln2515"> </a>
<a name="ln2516">void RaftConsensus::RequestVoteRespondVoteDenied(</a>
<a name="ln2517">    ConsensusErrorPB::Code error_code, const std::string&amp; message_suffix,</a>
<a name="ln2518">    const VoteRequestPB&amp; request, VoteResponsePB* response) {</a>
<a name="ln2519">  auto status = STATUS_FORMAT(</a>
<a name="ln2520">      InvalidArgument, &quot;$0: Denying vote to candidate $1 $2&quot;,</a>
<a name="ln2521">      GetRequestVoteLogPrefix(request), request.candidate_uuid(), message_suffix);</a>
<a name="ln2522">  FillVoteResponseVoteDenied(error_code, response);</a>
<a name="ln2523">  LOG(INFO) &lt;&lt; status.message().ToBuffer();</a>
<a name="ln2524">  StatusToPB(status, response-&gt;mutable_consensus_error()-&gt;mutable_status());</a>
<a name="ln2525">}</a>
<a name="ln2526"> </a>
<a name="ln2527">Status RaftConsensus::RequestVoteRespondInvalidTerm(const VoteRequestPB* request,</a>
<a name="ln2528">                                                    VoteResponsePB* response) {</a>
<a name="ln2529">  auto message_suffix = Format(</a>
<a name="ln2530">      &quot;for earlier term $0. Current term is $1.&quot;,</a>
<a name="ln2531">      request-&gt;candidate_term(), state_-&gt;GetCurrentTermUnlocked());</a>
<a name="ln2532">  RequestVoteRespondVoteDenied(ConsensusErrorPB::INVALID_TERM, message_suffix, *request, response);</a>
<a name="ln2533">  return Status::OK();</a>
<a name="ln2534">}</a>
<a name="ln2535"> </a>
<a name="ln2536">Status RaftConsensus::RequestVoteRespondVoteAlreadyGranted(const VoteRequestPB* request,</a>
<a name="ln2537">                                                           VoteResponsePB* response) {</a>
<a name="ln2538">  FillVoteResponseVoteGranted(*request, response);</a>
<a name="ln2539">  LOG(INFO) &lt;&lt; Substitute(&quot;$0: Already granted yes vote for candidate $1 in term $2. &quot;</a>
<a name="ln2540">                          &quot;Re-sending same reply.&quot;,</a>
<a name="ln2541">                          GetRequestVoteLogPrefix(*request),</a>
<a name="ln2542">                          request-&gt;candidate_uuid(),</a>
<a name="ln2543">                          request-&gt;candidate_term());</a>
<a name="ln2544">  return Status::OK();</a>
<a name="ln2545">}</a>
<a name="ln2546"> </a>
<a name="ln2547">Status RaftConsensus::RequestVoteRespondAlreadyVotedForOther(const VoteRequestPB* request,</a>
<a name="ln2548">                                                             VoteResponsePB* response) {</a>
<a name="ln2549">  auto message_suffix = Format(</a>
<a name="ln2550">      &quot;in current term $0: Already voted for candidate $1 in this term.&quot;,</a>
<a name="ln2551">      state_-&gt;GetCurrentTermUnlocked(), state_-&gt;GetVotedForCurrentTermUnlocked());</a>
<a name="ln2552">  RequestVoteRespondVoteDenied(ConsensusErrorPB::ALREADY_VOTED, message_suffix, *request, response);</a>
<a name="ln2553">  return Status::OK();</a>
<a name="ln2554">}</a>
<a name="ln2555"> </a>
<a name="ln2556">Status RaftConsensus::RequestVoteRespondLastOpIdTooOld(const OpIdPB&amp; local_last_logged_opid,</a>
<a name="ln2557">                                                       const VoteRequestPB* request,</a>
<a name="ln2558">                                                       VoteResponsePB* response) {</a>
<a name="ln2559">  auto message_suffix = Format(</a>
<a name="ln2560">      &quot;for term $0 because replica has last-logged OpId of $1, which is greater than that of the &quot;</a>
<a name="ln2561">          &quot;candidate, which has last-logged OpId of $2.&quot;,</a>
<a name="ln2562">      request-&gt;candidate_term(), local_last_logged_opid,</a>
<a name="ln2563">      request-&gt;candidate_status().last_received());</a>
<a name="ln2564">  RequestVoteRespondVoteDenied(</a>
<a name="ln2565">      ConsensusErrorPB::LAST_OPID_TOO_OLD, message_suffix, *request, response);</a>
<a name="ln2566">  return Status::OK();</a>
<a name="ln2567">}</a>
<a name="ln2568"> </a>
<a name="ln2569">Status RaftConsensus::RequestVoteRespondLeaderIsAlive(const VoteRequestPB* request,</a>
<a name="ln2570">                                                      VoteResponsePB* response) {</a>
<a name="ln2571">  FillVoteResponseVoteDenied(ConsensusErrorPB::LEADER_IS_ALIVE, response);</a>
<a name="ln2572">  std::string msg = Format(</a>
<a name="ln2573">      &quot;$0: Denying vote to candidate $1 for term $2 because replica is either leader or believes a &quot;</a>
<a name="ln2574">      &quot;valid leader to be alive. Time left: $3&quot;,</a>
<a name="ln2575">      GetRequestVoteLogPrefix(*request), request-&gt;candidate_uuid(), request-&gt;candidate_term(),</a>
<a name="ln2576">      withhold_votes_until_.load(std::memory_order_acquire) - MonoTime::Now());</a>
<a name="ln2577">  LOG(INFO) &lt;&lt; msg;</a>
<a name="ln2578">  StatusToPB(STATUS(InvalidArgument, msg), response-&gt;mutable_consensus_error()-&gt;mutable_status());</a>
<a name="ln2579">  return Status::OK();</a>
<a name="ln2580">}</a>
<a name="ln2581"> </a>
<a name="ln2582">Status RaftConsensus::RequestVoteRespondIsBusy(const VoteRequestPB* request,</a>
<a name="ln2583">                                               VoteResponsePB* response) {</a>
<a name="ln2584">  FillVoteResponseVoteDenied(ConsensusErrorPB::CONSENSUS_BUSY, response);</a>
<a name="ln2585">  string msg = Substitute(&quot;$0: Denying vote to candidate $1 for term $2 because &quot;</a>
<a name="ln2586">                          &quot;replica is already servicing an update from a current leader &quot;</a>
<a name="ln2587">                          &quot;or another vote.&quot;,</a>
<a name="ln2588">                          GetRequestVoteLogPrefix(*request),</a>
<a name="ln2589">                          request-&gt;candidate_uuid(),</a>
<a name="ln2590">                          request-&gt;candidate_term());</a>
<a name="ln2591">  LOG(INFO) &lt;&lt; msg;</a>
<a name="ln2592">  StatusToPB(STATUS(ServiceUnavailable, msg),</a>
<a name="ln2593">             response-&gt;mutable_consensus_error()-&gt;mutable_status());</a>
<a name="ln2594">  return Status::OK();</a>
<a name="ln2595">}</a>
<a name="ln2596"> </a>
<a name="ln2597">Status RaftConsensus::RequestVoteRespondVoteGranted(const VoteRequestPB* request,</a>
<a name="ln2598">                                                    VoteResponsePB* response) {</a>
<a name="ln2599">  // We know our vote will be &quot;yes&quot;, so avoid triggering an election while we</a>
<a name="ln2600">  // persist our vote to disk. We use an exponential backoff to avoid too much</a>
<a name="ln2601">  // split-vote contention when nodes display high latencies.</a>
<a name="ln2602">  MonoDelta additional_backoff = LeaderElectionExpBackoffDeltaUnlocked();</a>
<a name="ln2603">  SnoozeFailureDetector(ALLOW_LOGGING, additional_backoff);</a>
<a name="ln2604"> </a>
<a name="ln2605">  // Persist our vote to disk.</a>
<a name="ln2606">  RETURN_NOT_OK(state_-&gt;SetVotedForCurrentTermUnlocked(request-&gt;candidate_uuid()));</a>
<a name="ln2607"> </a>
<a name="ln2608">  FillVoteResponseVoteGranted(*request, response);</a>
<a name="ln2609"> </a>
<a name="ln2610">  // Give peer time to become leader. Snooze one more time after persisting our</a>
<a name="ln2611">  // vote. When disk latency is high, this should help reduce churn.</a>
<a name="ln2612">  SnoozeFailureDetector(DO_NOT_LOG, additional_backoff);</a>
<a name="ln2613"> </a>
<a name="ln2614">  LOG(INFO) &lt;&lt; Substitute(&quot;$0: Granting yes vote for candidate $1 in term $2.&quot;,</a>
<a name="ln2615">                          GetRequestVoteLogPrefix(*request),</a>
<a name="ln2616">                          request-&gt;candidate_uuid(),</a>
<a name="ln2617">                          state_-&gt;GetCurrentTermUnlocked());</a>
<a name="ln2618">  return Status::OK();</a>
<a name="ln2619">}</a>
<a name="ln2620"> </a>
<a name="ln2621">RaftPeerPB::Role RaftConsensus::GetRoleUnlocked() const {</a>
<a name="ln2622">  DCHECK(state_-&gt;IsLocked());</a>
<a name="ln2623">  return state_-&gt;GetActiveRoleUnlocked();</a>
<a name="ln2624">}</a>
<a name="ln2625"> </a>
<a name="ln2626">RaftPeerPB::Role RaftConsensus::role() const {</a>
<a name="ln2627">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln2628">  return GetRoleUnlocked();</a>
<a name="ln2629">}</a>
<a name="ln2630"> </a>
<a name="ln2631">LeaderState RaftConsensus::GetLeaderState(bool allow_stale) const {</a>
<a name="ln2632">  return state_-&gt;GetLeaderState(allow_stale);</a>
<a name="ln2633">}</a>
<a name="ln2634"> </a>
<a name="ln2635">std::string RaftConsensus::LogPrefix() {</a>
<a name="ln2636">  return state_-&gt;LogPrefix();</a>
<a name="ln2637">}</a>
<a name="ln2638"> </a>
<a name="ln2639">void RaftConsensus::SetLeaderUuidUnlocked(const string&amp; uuid) {</a>
<a name="ln2640">  failed_elections_since_stable_leader_.store(0, std::memory_order_release);</a>
<a name="ln2641">  state_-&gt;SetLeaderUuidUnlocked(uuid);</a>
<a name="ln2642">  auto context = std::make_shared&lt;StateChangeContext&gt;(StateChangeReason::NEW_LEADER_ELECTED, uuid);</a>
<a name="ln2643">  MarkDirty(context);</a>
<a name="ln2644">}</a>
<a name="ln2645"> </a>
<a name="ln2646">Status RaftConsensus::ReplicateConfigChangeUnlocked(const ReplicateMsgPtr&amp; replicate_ref,</a>
<a name="ln2647">                                                    const RaftConfigPB&amp; new_config,</a>
<a name="ln2648">                                                    ChangeConfigType type,</a>
<a name="ln2649">                                                    StdStatusCallback client_cb) {</a>
<a name="ln2650">  scoped_refptr&lt;ConsensusRound&gt; round(new ConsensusRound(this, replicate_ref));</a>
<a name="ln2651">  round-&gt;SetConsensusReplicatedCallback(std::bind(&amp;RaftConsensus::NonTxRoundReplicationFinished,</a>
<a name="ln2652">                                                  this,</a>
<a name="ln2653">                                                  round.get(),</a>
<a name="ln2654">                                                  std::move(client_cb), std::placeholders::_1));</a>
<a name="ln2655">  LOG(INFO) &lt;&lt; &quot;Setting replicate pending config &quot; &lt;&lt; new_config.ShortDebugString()</a>
<a name="ln2656">            &lt;&lt; &quot;, type = &quot; &lt;&lt; ChangeConfigType_Name(type);</a>
<a name="ln2657"> </a>
<a name="ln2658">  RETURN_NOT_OK(state_-&gt;SetPendingConfigUnlocked(new_config));</a>
<a name="ln2659"> </a>
<a name="ln2660">  if (type == CHANGE_ROLE &amp;&amp;</a>
<a name="ln2661">      PREDICT_FALSE(FLAGS_TEST_inject_delay_leader_change_role_append_secs)) {</a>
<a name="ln2662">    LOG(INFO) &lt;&lt; &quot;Adding change role sleep for &quot;</a>
<a name="ln2663">              &lt;&lt; FLAGS_TEST_inject_delay_leader_change_role_append_secs &lt;&lt; &quot; secs.&quot;;</a>
<a name="ln2664">    SleepFor(MonoDelta::FromSeconds(FLAGS_TEST_inject_delay_leader_change_role_append_secs));</a>
<a name="ln2665">  }</a>
<a name="ln2666"> </a>
<a name="ln2667">  // Set as pending.</a>
<a name="ln2668">  RefreshConsensusQueueAndPeersUnlocked();</a>
<a name="ln2669">  auto status = AppendNewRoundToQueueUnlocked(round);</a>
<a name="ln2670">  if (!status.ok()) {</a>
<a name="ln2671">    // We could just cancel pending config, because there is could be only one pending config.</a>
<a name="ln2672">    auto clear_status = state_-&gt;ClearPendingConfigUnlocked();</a>
<a name="ln2673">    if (!clear_status.ok()) {</a>
<a name="ln2674">      LOG(WARNING) &lt;&lt; &quot;Could not clear pending config: &quot; &lt;&lt; clear_status;</a>
<a name="ln2675">    }</a>
<a name="ln2676">  }</a>
<a name="ln2677">  return status;</a>
<a name="ln2678">}</a>
<a name="ln2679"> </a>
<a name="ln2680">void RaftConsensus::RefreshConsensusQueueAndPeersUnlocked() {</a>
<a name="ln2681">  DCHECK_EQ(RaftPeerPB::LEADER, state_-&gt;GetActiveRoleUnlocked());</a>
<a name="ln2682">  const RaftConfigPB&amp; active_config = state_-&gt;GetActiveConfigUnlocked();</a>
<a name="ln2683"> </a>
<a name="ln2684">  // Change the peers so that we're able to replicate messages remotely and</a>
<a name="ln2685">  // locally. Peer manager connections are updated using the active config. Connections to peers</a>
<a name="ln2686">  // that are not part of active_config are closed. New connections are created for those peers</a>
<a name="ln2687">  // that are present in active_config but have no connections. When the queue is in LEADER</a>
<a name="ln2688">  // mode, it checks that all registered peers are a part of the active config.</a>
<a name="ln2689">  peer_manager_-&gt;ClosePeersNotInConfig(active_config);</a>
<a name="ln2690">  queue_-&gt;SetLeaderMode(state_-&gt;GetCommittedOpIdUnlocked().ToPB&lt;OpIdPB&gt;(),</a>
<a name="ln2691">                        state_-&gt;GetCurrentTermUnlocked(),</a>
<a name="ln2692">                        active_config);</a>
<a name="ln2693"> </a>
<a name="ln2694">  ScopedDnsTracker dns_tracker(update_raft_config_dns_latency_.get());</a>
<a name="ln2695">  peer_manager_-&gt;UpdateRaftConfig(active_config);</a>
<a name="ln2696">}</a>
<a name="ln2697"> </a>
<a name="ln2698">string RaftConsensus::peer_uuid() const {</a>
<a name="ln2699">  return state_-&gt;GetPeerUuid();</a>
<a name="ln2700">}</a>
<a name="ln2701"> </a>
<a name="ln2702">string RaftConsensus::tablet_id() const {</a>
<a name="ln2703">  return state_-&gt;GetOptions().tablet_id;</a>
<a name="ln2704">}</a>
<a name="ln2705"> </a>
<a name="ln2706">ConsensusStatePB RaftConsensus::ConsensusState(</a>
<a name="ln2707">    ConsensusConfigType type,</a>
<a name="ln2708">    LeaderLeaseStatus* leader_lease_status) const {</a>
<a name="ln2709">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln2710">  return ConsensusStateUnlocked(type, leader_lease_status);</a>
<a name="ln2711">}</a>
<a name="ln2712"> </a>
<a name="ln2713">ConsensusStatePB RaftConsensus::ConsensusStateUnlocked(</a>
<a name="ln2714">    ConsensusConfigType type,</a>
<a name="ln2715">    LeaderLeaseStatus* leader_lease_status) const {</a>
<a name="ln2716">  CHECK(state_-&gt;IsLocked());</a>
<a name="ln2717">  if (leader_lease_status) {</a>
<a name="ln2718">    if (GetRoleUnlocked() == RaftPeerPB_Role_LEADER) {</a>
<a name="ln2719">      *leader_lease_status = state_-&gt;GetLeaderLeaseStatusUnlocked();</a>
<a name="ln2720">    } else {</a>
<a name="ln2721">      // We'll still return a valid value if we're not a leader.</a>
<a name="ln2722">      *leader_lease_status = LeaderLeaseStatus::NO_MAJORITY_REPLICATED_LEASE;</a>
<a name="ln2723">    }</a>
<a name="ln2724">  }</a>
<a name="ln2725">  return state_-&gt;ConsensusStateUnlocked(type);</a>
<a name="ln2726">}</a>
<a name="ln2727"> </a>
<a name="ln2728">RaftConfigPB RaftConsensus::CommittedConfig() const {</a>
<a name="ln2729">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln2730">  return state_-&gt;GetCommittedConfigUnlocked();</a>
<a name="ln2731">}</a>
<a name="ln2732"> </a>
<a name="ln2733">void RaftConsensus::DumpStatusHtml(std::ostream&amp; out) const {</a>
<a name="ln2734">  out &lt;&lt; &quot;&lt;h1&gt;Raft Consensus State&lt;/h1&gt;&quot; &lt;&lt; std::endl;</a>
<a name="ln2735"> </a>
<a name="ln2736">  out &lt;&lt; &quot;&lt;h2&gt;State&lt;/h2&gt;&quot; &lt;&lt; std::endl;</a>
<a name="ln2737">  out &lt;&lt; &quot;&lt;pre&gt;&quot; &lt;&lt; EscapeForHtmlToString(queue_-&gt;ToString()) &lt;&lt; &quot;&lt;/pre&gt;&quot; &lt;&lt; std::endl;</a>
<a name="ln2738"> </a>
<a name="ln2739">  // Dump the queues on a leader.</a>
<a name="ln2740">  RaftPeerPB::Role role;</a>
<a name="ln2741">  {</a>
<a name="ln2742">    auto lock = state_-&gt;LockForRead();</a>
<a name="ln2743">    role = state_-&gt;GetActiveRoleUnlocked();</a>
<a name="ln2744">  }</a>
<a name="ln2745">  if (role == RaftPeerPB::LEADER) {</a>
<a name="ln2746">    out &lt;&lt; &quot;&lt;h2&gt;Queue overview&lt;/h2&gt;&quot; &lt;&lt; std::endl;</a>
<a name="ln2747">    out &lt;&lt; &quot;&lt;pre&gt;&quot; &lt;&lt; EscapeForHtmlToString(queue_-&gt;ToString()) &lt;&lt; &quot;&lt;/pre&gt;&quot; &lt;&lt; std::endl;</a>
<a name="ln2748">    out &lt;&lt; &quot;&lt;hr/&gt;&quot; &lt;&lt; std::endl;</a>
<a name="ln2749">    out &lt;&lt; &quot;&lt;h2&gt;Queue details&lt;/h2&gt;&quot; &lt;&lt; std::endl;</a>
<a name="ln2750">    queue_-&gt;DumpToHtml(out);</a>
<a name="ln2751">  }</a>
<a name="ln2752">}</a>
<a name="ln2753"> </a>
<a name="ln2754">ReplicaState* RaftConsensus::GetReplicaStateForTests() {</a>
<a name="ln2755">  return state_.get();</a>
<a name="ln2756">}</a>
<a name="ln2757"> </a>
<a name="ln2758">void RaftConsensus::ElectionCallback(const LeaderElectionData&amp; data,</a>
<a name="ln2759">                                     const ElectionResult&amp; result) {</a>
<a name="ln2760">  // The election callback runs on a reactor thread, so we need to defer to our</a>
<a name="ln2761">  // threadpool. If the threadpool is already shut down for some reason, it's OK --</a>
<a name="ln2762">  // we're OK with the callback never running.</a>
<a name="ln2763">  WARN_NOT_OK(raft_pool_token_-&gt;SubmitFunc(</a>
<a name="ln2764">              std::bind(&amp;RaftConsensus::DoElectionCallback, shared_from_this(), data, result)),</a>
<a name="ln2765">              state_-&gt;LogPrefix() + &quot;Unable to run election callback&quot;);</a>
<a name="ln2766">}</a>
<a name="ln2767"> </a>
<a name="ln2768">void RaftConsensus::NotifyOriginatorAboutLostElection(const std::string&amp; originator_uuid) {</a>
<a name="ln2769">  if (originator_uuid.empty()) {</a>
<a name="ln2770">    return;</a>
<a name="ln2771">  }</a>
<a name="ln2772"> </a>
<a name="ln2773">  ReplicaState::UniqueLock lock;</a>
<a name="ln2774">  Status s = state_-&gt;LockForConfigChange(&amp;lock);</a>
<a name="ln2775">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln2776">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Unable to notify originator about lost election, lock failed: &quot;</a>
<a name="ln2777">                          &lt;&lt; s.ToString();</a>
<a name="ln2778">    return;</a>
<a name="ln2779">  }</a>
<a name="ln2780"> </a>
<a name="ln2781">  const RaftConfigPB&amp; active_config = state_-&gt;GetActiveConfigUnlocked();</a>
<a name="ln2782">  for (const RaftPeerPB&amp; peer : active_config.peers()) {</a>
<a name="ln2783">    if (peer.permanent_uuid() == originator_uuid) {</a>
<a name="ln2784">      // TODO(sergei) Currently we preserved synchronous DNS resolution in this case.</a>
<a name="ln2785">      // It is possible that it should be changed so async in future.</a>
<a name="ln2786">      // But look like it is not problem to leave synchronous variant here.</a>
<a name="ln2787">      auto proxy = peer_proxy_factory_-&gt;NewProxy(peer);</a>
<a name="ln2788">      LeaderElectionLostRequestPB req;</a>
<a name="ln2789">      req.set_dest_uuid(originator_uuid);</a>
<a name="ln2790">      req.set_election_lost_by_uuid(state_-&gt;GetPeerUuid());</a>
<a name="ln2791">      req.set_tablet_id(state_-&gt;GetOptions().tablet_id);</a>
<a name="ln2792">      auto resp = std::make_shared&lt;LeaderElectionLostResponsePB&gt;();</a>
<a name="ln2793">      auto rpc = std::make_shared&lt;rpc::RpcController&gt;();</a>
<a name="ln2794">      rpc-&gt;set_invoke_callback_mode(rpc::InvokeCallbackMode::kThreadPoolHigh);</a>
<a name="ln2795">      auto log_prefix = state_-&gt;LogPrefix();</a>
<a name="ln2796">      proxy-&gt;LeaderElectionLostAsync(&amp;req, resp.get(), rpc.get(), [log_prefix, resp, rpc] {</a>
<a name="ln2797">        if (!rpc-&gt;status().ok()) {</a>
<a name="ln2798">          LOG(WARNING) &lt;&lt; log_prefix &lt;&lt; &quot;Notify about lost election RPC failure: &quot;</a>
<a name="ln2799">                       &lt;&lt; rpc-&gt;status().ToString();</a>
<a name="ln2800">        } else if (resp-&gt;has_error()) {</a>
<a name="ln2801">          LOG(WARNING) &lt;&lt; log_prefix &lt;&lt; &quot;Notify about lost election failed: &quot;</a>
<a name="ln2802">                       &lt;&lt; StatusFromPB(resp-&gt;error().status()).ToString();</a>
<a name="ln2803">        }</a>
<a name="ln2804">      });</a>
<a name="ln2805">      return;</a>
<a name="ln2806">    }</a>
<a name="ln2807">  }</a>
<a name="ln2808">  LOG_WITH_PREFIX(WARNING) &lt;&lt; &quot;Failed to find originators peer: &quot; &lt;&lt; originator_uuid</a>
<a name="ln2809">                           &lt;&lt; &quot;, config: &quot; &lt;&lt; active_config.ShortDebugString();</a>
<a name="ln2810">}</a>
<a name="ln2811"> </a>
<a name="ln2812">void RaftConsensus::DoElectionCallback(const LeaderElectionData&amp; data,</a>
<a name="ln2813">                                       const ElectionResult&amp; result) {</a>
<a name="ln2814">  const char* election_name = result.preelection ? &quot;Pre-election&quot; : &quot;election&quot;;</a>
<a name="ln2815">  const char* decision_name = result.decision == ElectionVote::kGranted ? &quot;won&quot; : &quot;lost&quot;;</a>
<a name="ln2816">  // Snooze to avoid the election timer firing again as much as possible.</a>
<a name="ln2817">  {</a>
<a name="ln2818">    auto lock = state_-&gt;LockForRead();</a>
<a name="ln2819">    // We need to snooze when we win and when we lose:</a>
<a name="ln2820">    // - When we win because we're about to disable the timer and become leader.</a>
<a name="ln2821">    // - When we loose or otherwise we can fall into a cycle, where everyone keeps</a>
<a name="ln2822">    //   triggering elections but no election ever completes because by the time they</a>
<a name="ln2823">    //   finish another one is triggered already.</a>
<a name="ln2824">    // We ignore the status as we don't want to fail if we the timer is</a>
<a name="ln2825">    // disabled.</a>
<a name="ln2826">    SnoozeFailureDetector(ALLOW_LOGGING, LeaderElectionExpBackoffDeltaUnlocked());</a>
<a name="ln2827"> </a>
<a name="ln2828">    if (!result.preelections_not_supported_by_uuid.empty()) {</a>
<a name="ln2829">      disable_pre_elections_until_ =</a>
<a name="ln2830">          CoarseMonoClock::now() + FLAGS_temporary_disable_preelections_timeout_ms * 1ms;</a>
<a name="ln2831">      LOG_WITH_PREFIX(WARNING)</a>
<a name="ln2832">          &lt;&lt; &quot;Disable pre-elections until &quot; &lt;&lt; ToString(disable_pre_elections_until_)</a>
<a name="ln2833">          &lt;&lt; &quot;, because &quot; &lt;&lt; result.preelections_not_supported_by_uuid &lt;&lt; &quot; does not support them.&quot;;</a>
<a name="ln2834">    }</a>
<a name="ln2835">  }</a>
<a name="ln2836">  if (result.decision == ElectionVote::kDenied) {</a>
<a name="ln2837">    failed_elections_since_stable_leader_.fetch_add(1, std::memory_order_acq_rel);</a>
<a name="ln2838">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Leader &quot; &lt;&lt; election_name &lt;&lt; &quot; lost for term &quot;</a>
<a name="ln2839">                          &lt;&lt; result.election_term &lt;&lt; &quot;. Reason: &quot;</a>
<a name="ln2840">                          &lt;&lt; (!result.message.empty() ? result.message : &quot;None given&quot;)</a>
<a name="ln2841">                          &lt;&lt; &quot;. Originator: &quot; &lt;&lt; data.originator_uuid;</a>
<a name="ln2842">    NotifyOriginatorAboutLostElection(data.originator_uuid);</a>
<a name="ln2843"> </a>
<a name="ln2844">    if (result.higher_term) {</a>
<a name="ln2845">      ReplicaState::UniqueLock lock;</a>
<a name="ln2846">      Status s = state_-&gt;LockForConfigChange(&amp;lock);</a>
<a name="ln2847">      if (s.ok()) {</a>
<a name="ln2848">        s = HandleTermAdvanceUnlocked(*result.higher_term);</a>
<a name="ln2849">      }</a>
<a name="ln2850">      if (!s.ok()) {</a>
<a name="ln2851">        LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Unable to advance term as &quot; &lt;&lt; election_name &lt;&lt; &quot; result: &quot; &lt;&lt; s;</a>
<a name="ln2852">      }</a>
<a name="ln2853">    }</a>
<a name="ln2854"> </a>
<a name="ln2855">    return;</a>
<a name="ln2856">  }</a>
<a name="ln2857"> </a>
<a name="ln2858">  ReplicaState::UniqueLock lock;</a>
<a name="ln2859">  Status s = state_-&gt;LockForConfigChange(&amp;lock);</a>
<a name="ln2860">  if (PREDICT_FALSE(!s.ok())) {</a>
<a name="ln2861">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Received &quot; &lt;&lt; election_name &lt;&lt; &quot; callback for term &quot;</a>
<a name="ln2862">                          &lt;&lt; result.election_term &lt;&lt; &quot; while not running: &quot;</a>
<a name="ln2863">                          &lt;&lt; s.ToString();</a>
<a name="ln2864">    return;</a>
<a name="ln2865">  }</a>
<a name="ln2866"> </a>
<a name="ln2867">  auto desired_term = state_-&gt;GetCurrentTermUnlocked() + (result.preelection ? 1 : 0);</a>
<a name="ln2868">  if (result.election_term != desired_term) {</a>
<a name="ln2869">    LOG_WITH_PREFIX(INFO)</a>
<a name="ln2870">        &lt;&lt; &quot;Leader &quot; &lt;&lt; election_name &lt;&lt; &quot; decision for defunct term &quot;</a>
<a name="ln2871">        &lt;&lt; result.election_term &lt;&lt; &quot;: &quot; &lt;&lt; decision_name;</a>
<a name="ln2872">    return;</a>
<a name="ln2873">  }</a>
<a name="ln2874"> </a>
<a name="ln2875">  const RaftConfigPB&amp; active_config = state_-&gt;GetActiveConfigUnlocked();</a>
<a name="ln2876">  if (!IsRaftConfigVoter(state_-&gt;GetPeerUuid(), active_config)) {</a>
<a name="ln2877">    LOG_WITH_PREFIX(WARNING)</a>
<a name="ln2878">        &lt;&lt; &quot;Leader &quot; &lt;&lt; election_name &lt;&lt; &quot; decision while not in active config. &quot;</a>
<a name="ln2879">        &lt;&lt; &quot;Result: Term &quot; &lt;&lt; result.election_term &lt;&lt; &quot;: &quot; &lt;&lt; decision_name</a>
<a name="ln2880">        &lt;&lt; &quot;. RaftConfig: &quot; &lt;&lt; active_config.ShortDebugString();</a>
<a name="ln2881">    return;</a>
<a name="ln2882">  }</a>
<a name="ln2883"> </a>
<a name="ln2884">  if (result.preelection) {</a>
<a name="ln2885">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Leader pre-election won for term &quot; &lt;&lt; result.election_term;</a>
<a name="ln2886">    lock.unlock();</a>
<a name="ln2887">    WARN_NOT_OK(DoStartElection(data, PreElected::kTrue), &quot;Start election failed: &quot;);</a>
<a name="ln2888">    return;</a>
<a name="ln2889">  }</a>
<a name="ln2890"> </a>
<a name="ln2891">  if (state_-&gt;GetActiveRoleUnlocked() == RaftPeerPB::LEADER) {</a>
<a name="ln2892">    LOG_WITH_PREFIX(DFATAL)</a>
<a name="ln2893">        &lt;&lt; &quot;Leader &quot; &lt;&lt; election_name &lt;&lt; &quot; callback while already leader! Result: Term &quot;</a>
<a name="ln2894">        &lt;&lt; result.election_term &lt;&lt; &quot;: &quot;</a>
<a name="ln2895">        &lt;&lt; decision_name;</a>
<a name="ln2896">    return;</a>
<a name="ln2897">  }</a>
<a name="ln2898"> </a>
<a name="ln2899">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Leader &quot; &lt;&lt; election_name &lt;&lt; &quot; won for term &quot; &lt;&lt; result.election_term;</a>
<a name="ln2900"> </a>
<a name="ln2901">  // Apply lease updates that were possible received from voters.</a>
<a name="ln2902">  state_-&gt;UpdateOldLeaderLeaseExpirationOnNonLeaderUnlocked(</a>
<a name="ln2903">      result.old_leader_lease, result.old_leader_ht_lease);</a>
<a name="ln2904"> </a>
<a name="ln2905">  state_-&gt;SetLeaderNoOpCommittedUnlocked(false);</a>
<a name="ln2906">  // Convert role to LEADER.</a>
<a name="ln2907">  SetLeaderUuidUnlocked(state_-&gt;GetPeerUuid());</a>
<a name="ln2908"> </a>
<a name="ln2909">  // TODO: BecomeLeaderUnlocked() can fail due to state checks during shutdown.</a>
<a name="ln2910">  // It races with the above state check.</a>
<a name="ln2911">  // This could be a problem during tablet deletion.</a>
<a name="ln2912">  auto status = BecomeLeaderUnlocked();</a>
<a name="ln2913">  if (!status.ok()) {</a>
<a name="ln2914">    LOG_WITH_PREFIX(DFATAL) &lt;&lt; &quot;Failed to become leader: &quot; &lt;&lt; status.ToString();</a>
<a name="ln2915">  }</a>
<a name="ln2916">}</a>
<a name="ln2917"> </a>
<a name="ln2918">yb::OpId RaftConsensus::GetLastReceivedOpId() {</a>
<a name="ln2919">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln2920">  return state_-&gt;GetLastReceivedOpIdUnlocked();</a>
<a name="ln2921">}</a>
<a name="ln2922"> </a>
<a name="ln2923">yb::OpId RaftConsensus::GetLastCommittedOpId() {</a>
<a name="ln2924">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln2925">  return state_-&gt;GetCommittedOpIdUnlocked();</a>
<a name="ln2926">}</a>
<a name="ln2927"> </a>
<a name="ln2928">yb::OpId RaftConsensus::GetSplitOpId() {</a>
<a name="ln2929">  auto lock = state_-&gt;LockForRead();</a>
<a name="ln2930">  return state_-&gt;GetSplitOpIdUnlocked();</a>
<a name="ln2931">}</a>
<a name="ln2932"> </a>
<a name="ln2933">Status RaftConsensus::ResetSplitOpId() {</a>
<a name="ln2934">  ReplicaState::UniqueLock lock;</a>
<a name="ln2935">  RETURN_NOT_OK(state_-&gt;LockForUpdate(&amp;lock));</a>
<a name="ln2936">  state_-&gt;ResetSplitOpIdUnlocked();</a>
<a name="ln2937">  return Status::OK();</a>
<a name="ln2938">}</a>
<a name="ln2939"> </a>
<a name="ln2940">void RaftConsensus::MarkDirty(std::shared_ptr&lt;StateChangeContext&gt; context) {</a>
<a name="ln2941">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Calling mark dirty synchronously for reason code &quot; &lt;&lt; context-&gt;reason;</a>
<a name="ln2942">  mark_dirty_clbk_.Run(context);</a>
<a name="ln2943">}</a>
<a name="ln2944"> </a>
<a name="ln2945">void RaftConsensus::MarkDirtyOnSuccess(std::shared_ptr&lt;StateChangeContext&gt; context,</a>
<a name="ln2946">                                       const StdStatusCallback&amp; client_cb,</a>
<a name="ln2947">                                       const Status&amp; status) {</a>
<a name="ln2948">  if (PREDICT_TRUE(status.ok())) {</a>
<a name="ln2949">    MarkDirty(context);</a>
<a name="ln2950">  }</a>
<a name="ln2951">  client_cb(status);</a>
<a name="ln2952">}</a>
<a name="ln2953"> </a>
<a name="ln2954">void RaftConsensus::NonTxRoundReplicationFinished(ConsensusRound* round,</a>
<a name="ln2955">                                                  const StdStatusCallback&amp; client_cb,</a>
<a name="ln2956">                                                  const Status&amp; status) {</a>
<a name="ln2957">  DCHECK(state_-&gt;IsLocked());</a>
<a name="ln2958">  OperationType op_type = round-&gt;replicate_msg()-&gt;op_type();</a>
<a name="ln2959">  string op_type_str = OperationType_Name(op_type);</a>
<a name="ln2960">  if (!IsConsensusOnlyOperation(op_type)) {</a>
<a name="ln2961">    LOG(ERROR) &lt;&lt; &quot;Unexpected op type: &quot; &lt;&lt; op_type_str;</a>
<a name="ln2962">    return;</a>
<a name="ln2963">  }</a>
<a name="ln2964">  if (!status.ok()) {</a>
<a name="ln2965">    // TODO: Do something with the status on failure?</a>
<a name="ln2966">    LOG_WITH_PREFIX(INFO) &lt;&lt; op_type_str &lt;&lt; &quot; replication failed: &quot; &lt;&lt; status;</a>
<a name="ln2967"> </a>
<a name="ln2968">    // Clear out the pending state (ENG-590).</a>
<a name="ln2969">    if (IsChangeConfigOperation(op_type)) {</a>
<a name="ln2970">      Status s = state_-&gt;ClearPendingConfigUnlocked();</a>
<a name="ln2971">      if (!s.ok()) {</a>
<a name="ln2972">        LOG(WARNING) &lt;&lt; &quot;Could not clear pending state : &quot; &lt;&lt; s.ToString();</a>
<a name="ln2973">      }</a>
<a name="ln2974">    }</a>
<a name="ln2975">  } else if (IsChangeConfigOperation(op_type)) {</a>
<a name="ln2976">    // Notify the TabletPeer owner object.</a>
<a name="ln2977">    state_-&gt;context()-&gt;ChangeConfigReplicated(state_-&gt;GetCommittedConfigUnlocked());</a>
<a name="ln2978">  }</a>
<a name="ln2979"> </a>
<a name="ln2980">  client_cb(status);</a>
<a name="ln2981"> </a>
<a name="ln2982">  // Set 'Leader is ready to serve' flag only for commited NoOp operation</a>
<a name="ln2983">  // and only if the term is up-to-date.</a>
<a name="ln2984">  if (op_type == NO_OP &amp;&amp; round-&gt;id().has_term() &amp;&amp;</a>
<a name="ln2985">      round-&gt;id().term() == state_-&gt;GetCurrentTermUnlocked()) {</a>
<a name="ln2986">    state_-&gt;SetLeaderNoOpCommittedUnlocked(true);</a>
<a name="ln2987">  }</a>
<a name="ln2988">}</a>
<a name="ln2989"> </a>
<a name="ln2990">void RaftConsensus::EnableFailureDetector(MonoDelta delta) {</a>
<a name="ln2991">  if (PREDICT_TRUE(FLAGS_enable_leader_failure_detection)) {</a>
<a name="ln2992">    failure_detector_-&gt;Start(delta);</a>
<a name="ln2993">  }</a>
<a name="ln2994">}</a>
<a name="ln2995"> </a>
<a name="ln2996">void RaftConsensus::DisableFailureDetector() {</a>
<a name="ln2997">  if (PREDICT_TRUE(FLAGS_enable_leader_failure_detection)) {</a>
<a name="ln2998">    failure_detector_-&gt;Stop();</a>
<a name="ln2999">  }</a>
<a name="ln3000">}</a>
<a name="ln3001"> </a>
<a name="ln3002">void RaftConsensus::SnoozeFailureDetector(AllowLogging allow_logging, MonoDelta delta) {</a>
<a name="ln3003">  if (PREDICT_TRUE(GetAtomicFlag(&amp;FLAGS_enable_leader_failure_detection))) {</a>
<a name="ln3004">    if (allow_logging == ALLOW_LOGGING) {</a>
<a name="ln3005">      LOG_WITH_PREFIX(INFO) &lt;&lt; Format(&quot;Snoozing failure detection for $0&quot;,</a>
<a name="ln3006">                                      delta.Initialized() ? delta.ToString() : &quot;election timeout&quot;);</a>
<a name="ln3007">    }</a>
<a name="ln3008"> </a>
<a name="ln3009">    if (!delta.Initialized()) {</a>
<a name="ln3010">      delta = MinimumElectionTimeout();</a>
<a name="ln3011">    }</a>
<a name="ln3012">    failure_detector_-&gt;Snooze(delta);</a>
<a name="ln3013">  }</a>
<a name="ln3014">}</a>
<a name="ln3015"> </a>
<a name="ln3016">MonoDelta RaftConsensus::MinimumElectionTimeout() const {</a>
<a name="ln3017">  int32_t failure_timeout = FLAGS_leader_failure_max_missed_heartbeat_periods *</a>
<a name="ln3018">      FLAGS_raft_heartbeat_interval_ms;</a>
<a name="ln3019"> </a>
<a name="ln3020">  return MonoDelta::FromMilliseconds(failure_timeout);</a>
<a name="ln3021">}</a>
<a name="ln3022"> </a>
<a name="ln3023">MonoDelta RaftConsensus::LeaderElectionExpBackoffDeltaUnlocked() {</a>
<a name="ln3024">  // Compute a backoff factor based on how many leader elections have</a>
<a name="ln3025">  // taken place since a stable leader was last seen.</a>
<a name="ln3026">  double backoff_factor = pow(</a>
<a name="ln3027">      1.1,</a>
<a name="ln3028">      failed_elections_since_stable_leader_.load(std::memory_order_acquire) + 1);</a>
<a name="ln3029">  double min_timeout = MinimumElectionTimeout().ToMilliseconds();</a>
<a name="ln3030">  double max_timeout = std::min&lt;double&gt;(</a>
<a name="ln3031">      min_timeout * backoff_factor,</a>
<a name="ln3032">      FLAGS_leader_failure_exp_backoff_max_delta_ms);</a>
<a name="ln3033">  if (max_timeout &lt; min_timeout) {</a>
<a name="ln3034">    LOG(INFO) &lt;&lt; &quot;Resetting max_timeout from &quot; &lt;&lt;  max_timeout &lt;&lt; &quot; to &quot; &lt;&lt; min_timeout</a>
<a name="ln3035">              &lt;&lt; &quot;, max_delta_flag=&quot; &lt;&lt; FLAGS_leader_failure_exp_backoff_max_delta_ms;</a>
<a name="ln3036">    max_timeout = min_timeout;</a>
<a name="ln3037">  }</a>
<a name="ln3038">  // Randomize the timeout between the minimum and the calculated value.</a>
<a name="ln3039">  // We do this after the above capping to the max. Otherwise, after a</a>
<a name="ln3040">  // churny period, we'd end up highly likely to backoff exactly the max</a>
<a name="ln3041">  // amount.</a>
<a name="ln3042">  double timeout = min_timeout + (max_timeout - min_timeout) * rng_.NextDoubleFraction();</a>
<a name="ln3043">  DCHECK_GE(timeout, min_timeout);</a>
<a name="ln3044"> </a>
<a name="ln3045">  return MonoDelta::FromMilliseconds(timeout);</a>
<a name="ln3046">}</a>
<a name="ln3047"> </a>
<a name="ln3048">Status RaftConsensus::IncrementTermUnlocked() {</a>
<a name="ln3049">  return HandleTermAdvanceUnlocked(state_-&gt;GetCurrentTermUnlocked() + 1);</a>
<a name="ln3050">}</a>
<a name="ln3051"> </a>
<a name="ln3052">Status RaftConsensus::HandleTermAdvanceUnlocked(ConsensusTerm new_term) {</a>
<a name="ln3053">  if (new_term &lt;= state_-&gt;GetCurrentTermUnlocked()) {</a>
<a name="ln3054">    return STATUS(IllegalState, Substitute(&quot;Can't advance term to: $0 current term: $1 is higher.&quot;,</a>
<a name="ln3055">                                           new_term, state_-&gt;GetCurrentTermUnlocked()));</a>
<a name="ln3056">  }</a>
<a name="ln3057"> </a>
<a name="ln3058">  if (state_-&gt;GetActiveRoleUnlocked() == RaftPeerPB::LEADER) {</a>
<a name="ln3059">    LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Stepping down as leader of term &quot;</a>
<a name="ln3060">                          &lt;&lt; state_-&gt;GetCurrentTermUnlocked()</a>
<a name="ln3061">                          &lt;&lt; &quot; since new term is &quot; &lt;&lt; new_term;</a>
<a name="ln3062"> </a>
<a name="ln3063">    RETURN_NOT_OK(BecomeReplicaUnlocked(std::string()));</a>
<a name="ln3064">  }</a>
<a name="ln3065"> </a>
<a name="ln3066">  LOG_WITH_PREFIX(INFO) &lt;&lt; &quot;Advancing to term &quot; &lt;&lt; new_term;</a>
<a name="ln3067">  RETURN_NOT_OK(state_-&gt;SetCurrentTermUnlocked(new_term));</a>
<a name="ln3068">  term_metric_-&gt;set_value(new_term);</a>
<a name="ln3069">  return Status::OK();</a>
<a name="ln3070">}</a>
<a name="ln3071"> </a>
<a name="ln3072">Result&lt;ReadOpsResult&gt; RaftConsensus::ReadReplicatedMessagesForCDC(const yb::OpId&amp; from,</a>
<a name="ln3073">  int64_t* last_replicated_opid_index) {</a>
<a name="ln3074">  return queue_-&gt;ReadReplicatedMessagesForCDC(from, last_replicated_opid_index);</a>
<a name="ln3075">}</a>
<a name="ln3076"> </a>
<a name="ln3077">void RaftConsensus::UpdateCDCConsumerOpId(const yb::OpId&amp; op_id) {</a>
<a name="ln3078">  return queue_-&gt;UpdateCDCConsumerOpId(op_id);</a>
<a name="ln3079">}</a>
<a name="ln3080"> </a>
<a name="ln3081">void RaftConsensus::RollbackIdAndDeleteOpId(const ReplicateMsgPtr&amp; replicate_msg,</a>
<a name="ln3082">                                            bool should_exists) {</a>
<a name="ln3083">  std::unique_ptr&lt;OpIdPB&gt; op_id(replicate_msg-&gt;release_id());</a>
<a name="ln3084">  state_-&gt;CancelPendingOperation(*op_id, should_exists);</a>
<a name="ln3085">}</a>
<a name="ln3086"> </a>
<a name="ln3087">uint64_t RaftConsensus::OnDiskSize() const {</a>
<a name="ln3088">  return state_-&gt;OnDiskSize();</a>
<a name="ln3089">}</a>
<a name="ln3090"> </a>
<a name="ln3091">yb::OpId RaftConsensus::WaitForSafeOpIdToApply(const yb::OpId&amp; op_id) {</a>
<a name="ln3092">  return log_-&gt;WaitForSafeOpIdToApply(op_id);</a>
<a name="ln3093">}</a>
<a name="ln3094"> </a>
<a name="ln3095">yb::OpId RaftConsensus::MinRetryableRequestOpId() {</a>
<a name="ln3096">  return state_-&gt;MinRetryableRequestOpId();</a>
<a name="ln3097">}</a>
<a name="ln3098"> </a>
<a name="ln3099">size_t RaftConsensus::LogCacheSize() {</a>
<a name="ln3100">  return queue_-&gt;LogCacheSize();</a>
<a name="ln3101">}</a>
<a name="ln3102"> </a>
<a name="ln3103">size_t RaftConsensus::EvictLogCache(size_t bytes_to_evict) {</a>
<a name="ln3104">  return queue_-&gt;EvictLogCache(bytes_to_evict);</a>
<a name="ln3105">}</a>
<a name="ln3106"> </a>
<a name="ln3107">Status RaftConsensus::CopyLogTo(const std::string&amp; dest_dir) {</a>
<a name="ln3108">  return queue_-&gt;CopyLogTo(dest_dir);</a>
<a name="ln3109">}</a>
<a name="ln3110"> </a>
<a name="ln3111">Status RaftConsensus::FlushLogIndex() {</a>
<a name="ln3112">  return queue_-&gt;FlushLogIndex();</a>
<a name="ln3113">}</a>
<a name="ln3114"> </a>
<a name="ln3115">RetryableRequestsCounts RaftConsensus::TEST_CountRetryableRequests() {</a>
<a name="ln3116">  return state_-&gt;TEST_CountRetryableRequests();</a>
<a name="ln3117">}</a>
<a name="ln3118"> </a>
<a name="ln3119">void RaftConsensus::TrackOperationMemory(const yb::OpId&amp; op_id) {</a>
<a name="ln3120">  queue_-&gt;TrackOperationsMemory({op_id});</a>
<a name="ln3121">}</a>
<a name="ln3122"> </a>
<a name="ln3123">}  // namespace consensus</a>
<a name="ln3124">}  // namespace yb</a>

</code></pre>
<div class="balloon" rel="275"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="475"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="595"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="836"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="850"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="885"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="913"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1015"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1140"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1169"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1184"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1288"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1316"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1353"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1392"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1412"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1793"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1874"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1897"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1941"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="1971"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2184"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2302"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2321"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2367"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2372"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2382"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2386"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2388"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2399"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2421"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2622"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2716"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>
<div class="balloon" rel="2957"><p><span style="font-size:18px">&uarr;</span> <a href="https://www.viva64.com/en/w/v521/" target="_blank">V521</a> Such expressions using the ',' operator are dangerous. Make sure the expression is correct.</p></div>

<link rel="stylesheet" href="highlight.css">
<script src="highlight.pack.js"></script>
<script src="highlightjs-line-numbers.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
  $(document).ready(function() {
      $('.balloon').each(function () {
          var bl = $(this);
          var line = bl.attr('rel');
          var text = $('a[name="ln'+line+'"]').text();

          var space_count = 0;
          for(var i = 0; i<text.length; i++){
              var char = text[i];
              if((char !== ' ')&&(char !== '\t'))break;
              if(char === '\t')space_count++;
              space_count++;
          }

          bl.css('margin-left', space_count*8);
          $('a[name="ln'+line+'"]').after(bl);
      });

      window.location = window.location;
  });
</script>
</body>
</html>
